[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hydroinformatics",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#how-to-use-these-materials",
    "href": "index.html#how-to-use-these-materials",
    "title": "Hydroinformatics",
    "section": "How to use these materials",
    "text": "How to use these materials\nAt the top of each chapter there is a link to a github repository. In each repository is the code that produces each chapter and a version where the code chunks within it are blank. These repositories are all template repositories, so you can easily copy them to your own github space by clicking Use This Template on the repo page.\nIn my class, I work through the each document, live coding with students following along.Typically I ask students to watch as I code and explain the chunk and then replicate it on their computer. Depending on the lesson, I will ask students to try some of the chunks before I show them the code as an in-class activity. Some chunks are explicitly designed for this purpose and are typically labeled a “challenge”.\nChapters called ACTIVITY are either homework or class-period-long in-class activities. The code chunks in these are therefore blank. If you would like a key for any of these, please just send me an email."
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#download-and-install-tidyverse-library",
    "href": "01-Plotting_Demo_COMPLETE.html#download-and-install-tidyverse-library",
    "title": "1  Intro to Plotting",
    "section": "1.1 Download and install tidyverse library",
    "text": "1.1 Download and install tidyverse library\nWe will use the tidyverse a lot this semester. It is a suite of packages that handles plotting and data wrangling efficiently.\nYou only have to install the library once. You have to load it using the library() function each time you start an R session.\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#reading-data",
    "href": "01-Plotting_Demo_COMPLETE.html#reading-data",
    "title": "1  Intro to Plotting",
    "section": "1.2 Reading data",
    "text": "1.2 Reading data\nThe following lines will read in the data we will use for this exercise. Don’t worry about this right now beyond running it, we will talk more about it later.\n\nPine &lt;- read_csv(\"PINE_Jan-Mar_2010.csv\") \n\nRows: 2160 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): StationID, surrogate\ndbl  (5): cfs, year, quarter, month, day\ndttm (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nSNP &lt;- read_csv(\"PINE_NFDR_Jan-Mar_2010.csv\")\n\nRows: 4320 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): StationID, surrogate\ndbl  (5): cfs, year, quarter, month, day\ndttm (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nRBI &lt;- read_csv(\"Flashy_Dat_Subset.csv\")\n\nRows: 49 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): STANAME, STATE, CLASS, AGGECOREGION\ndbl (22): site_no, RBI, RBIrank, DRAIN_SQKM, HUC02, LAT_GAGE, LNG_GAGE, PPTA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nBasic ggplot syntax"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#our-first-ggplot",
    "href": "01-Plotting_Demo_COMPLETE.html#our-first-ggplot",
    "title": "1  Intro to Plotting",
    "section": "1.3 Our first ggplot",
    "text": "1.3 Our first ggplot\nLet’s look at the Pine data, plotting streamflow (the cfs column) by the date (datetime column). We will show the time series as a line.\n\nggplot(data = Pine, aes(x = datetime, y = cfs))+\n  geom_line()"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#change-point-type",
    "href": "01-Plotting_Demo_COMPLETE.html#change-point-type",
    "title": "1  Intro to Plotting",
    "section": "1.4 Change point type",
    "text": "1.4 Change point type\nNow let’s make the same plot but show the data as points, using the pch parameter in geom_point() we can change the point type to any of the following:\n\n\n\npch options from R help file\n\n\n\nggplot(data = Pine, aes(x = datetime, y = cfs))+\n  geom_point(pch = 8)"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#set-colors",
    "href": "01-Plotting_Demo_COMPLETE.html#set-colors",
    "title": "1  Intro to Plotting",
    "section": "1.5 Set colors",
    "text": "1.5 Set colors\nWe can also “easily” change the color. Easily is in quotes because this often trips people up. If you put color = “blue” in the aesthetic function, think about what that is telling ggplot. It says “control the color using”blue”“. That doesn’t make a whole lot of sense, so neither does the output… Try it.\nWhat happens is that if color = “blue” is in the aesthetic, you are telling R that the color used in the geom represents “blue”. This is very useful if you have multiple geoms in your plot, are coloring them differently, and are building a legend. But if you are just trying to color the points, it kind of feels like R is trolling you… doesn’t it?\nTake the color = “blue” out of the aesthetic and you’re golden.\n\nggplot(data = Pine, aes(datetime, y = cfs, color = \"blue\"))+\n  geom_point()\n\n\n\nggplot(data = Pine, aes(x = datetime, y = cfs))+\n  geom_point(color = \"blue\")"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#controlling-color-with-a-third-variable-and-other-functions",
    "href": "01-Plotting_Demo_COMPLETE.html#controlling-color-with-a-third-variable-and-other-functions",
    "title": "1  Intro to Plotting",
    "section": "1.6 Controlling color with a third variable and other functions",
    "text": "1.6 Controlling color with a third variable and other functions\nLet’s plot the data as a line again, but play with it a bit.\nFirst: make the line blue\nSecond: change the theme\nThird: change the axis labels\nFourth: color by discharge\n\nggplot(data = Pine, aes(x = datetime, y = cfs, color = cfs))+\n  geom_line()+\n  ylab(\"Discharge (cfs)\")+\n  xlab(element_blank())+\n  theme_classic()"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#plotting-multiple-groups",
    "href": "01-Plotting_Demo_COMPLETE.html#plotting-multiple-groups",
    "title": "1  Intro to Plotting",
    "section": "1.7 Plotting multiple groups",
    "text": "1.7 Plotting multiple groups\nThe SNP dataset has two different streams: Pine and NFDR\nWe can look at the two of those a couple of different ways\nFirst, make two lines, colored by the stream by adding color = to your aesthetic.\n\nggplot(data = SNP, aes(x = datetime,y = cfs, color = StationID)) +\n  geom_line()"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#facets",
    "href": "01-Plotting_Demo_COMPLETE.html#facets",
    "title": "1  Intro to Plotting",
    "section": "1.8 Facets",
    "text": "1.8 Facets\nWe can also use facets.\nYou must tell the facet_wrap what variable to use to make the separate panels (facet =). It’ll decide how to orient them or you can tell it how. We want them to be on top of each other so we are going to tell it we want 2 rows by setting nrow = 2. Note that we have to put the column used to make the facets in quotes after facets =\n\nggplot(data = SNP, aes(x = datetime,y = cfs)) +\n  geom_line() +\n  facet_wrap(facets = \"StationID\", nrow = 2)"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#two-variable-faceting",
    "href": "01-Plotting_Demo_COMPLETE.html#two-variable-faceting",
    "title": "1  Intro to Plotting",
    "section": "1.9 Two variable faceting",
    "text": "1.9 Two variable faceting\nYou can also use facet_grid() to break your plots up into panels based on two variables. Below we will create a panel for each month in each watershed. Adding scales = “free” allows facet_grid to change the axes. By default, all axes will be the same. This is often what we want, so we can more easily compare magnitudes, but sometimes we are looking for patterns more, so we may want to let the axes have whatever range works for the individual plots.\n\nggplot(data = SNP, aes(x = datetime,y = cfs)) +\n  geom_line() +\n  facet_grid(StationID ~ month, scales = \"free\")"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#boxplots",
    "href": "01-Plotting_Demo_COMPLETE.html#boxplots",
    "title": "1  Intro to Plotting",
    "section": "1.10 Boxplots",
    "text": "1.10 Boxplots\nWe can look at these data in other ways as well. A very useful way to look at the variation of two groups is to use a boxplot.\nBecause the data span several orders of magnitude, we will have to log the y axis to see the differences between the two streams. We do that by adding scale_y_log10()\n\nggplot(data = SNP, aes(x = StationID, y = cfs)) + \n  stat_boxplot()+\n  scale_y_log10()"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#more-about-color-size-etc",
    "href": "01-Plotting_Demo_COMPLETE.html#more-about-color-size-etc",
    "title": "1  Intro to Plotting",
    "section": "1.11 More about color, size, etc",
    "text": "1.11 More about color, size, etc\nLet’s play around a bit with controlling color, point size, etc with other data.\nWe can control the size of points by putting size = in the aes() and color by putting color =\nIf you use a point type that has a background, like #21, you can also set the background color using bg =\nIf points are too close together to see them all you can use a hollow point type or set the alpha lower so the points are transparent (alpha = )\n\nggplot(RBI, aes(RBI, DRAIN_SQKM, size = T_AVG_SITE, bg = STATE))+\n  geom_point(pch = 21, alpha = 0.3)"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#multiple-geoms",
    "href": "01-Plotting_Demo_COMPLETE.html#multiple-geoms",
    "title": "1  Intro to Plotting",
    "section": "1.12 Multiple geoms",
    "text": "1.12 Multiple geoms\nFinally: You can add multiple geoms to the same plot. Examples of when you might want to do this are when you are showing a line fit and want to show the points as well, or maybe showing a boxplot and want to show the data behind it. You simply add additional geom_… lines to add additional geoms.\n\nggplot(RBI, aes(RBI, DRAIN_SQKM, color = AGGECOREGION))+\n  stat_smooth(method = \"lm\", linetype = 2)+\n  geom_point()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "01-Plotting_Demo_COMPLETE.html#test-your-knowledge",
    "href": "01-Plotting_Demo_COMPLETE.html#test-your-knowledge",
    "title": "1  Intro to Plotting",
    "section": "1.13 Test your knowledge!",
    "text": "1.13 Test your knowledge!\n\n1.13.1 Which is a valid ggplot statement?\n\n ggplot(mydata, c(x = date, y = flow)) + geom_line() ggplot(mydata, c(x = date, y = flow)), geom_line() ggplot(mydata c(x = date, y = flow)) + geom_line() ggplot(mydata, c(x = date y = flow)) + geom_line()"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#introduction",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#introduction",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nWe have messed around with plotting a bit and you’ve seen a little of what R can do. So now let’s review or introduce you to some basics. Even if you have worked in R before, it is good to be remind of/practice with this stuff, so stay tuned in!\nThis exercise covers most of the same principles as two chapters in R for Data Science\nWorkflow: basics (https://r4ds.hadley.nz/workflow-basics)\nData transformation (https://r4ds.hadley.nz/data-transform)"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#you-can-use-r-as-a-calculator",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#you-can-use-r-as-a-calculator",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.2 You can use R as a calculator",
    "text": "2.2 You can use R as a calculator\nIf you just type numbers and operators in, R will spit out the results\n\n1 + 2\n\n[1] 3"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#you-can-create-new-objects-using--",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#you-can-create-new-objects-using--",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.3 You can create new objects using <-",
    "text": "2.3 You can create new objects using &lt;-\nYea yea, = does the same thing. But use &lt;-. We will call &lt;- assignment or assignment operator. When we are coding in R we use &lt;- to assign values to objects and = to set values for parameters in functions. Using &lt;- helps us differentiate between the two. Norms for formatting are important because they help us understand what code is doing, especially when stuff gets complex.\nOh, one more thing: Surround operators with spaces. Don’t code like a gorilla.\nx &lt;- 1 looks better than x&lt;-1 and if you disagree you are wrong. :)\nYou can assign single numbers or entire chunks of data using &lt;-\nSo if you had an object called my_data and wanted to copy it into my_new_data you could do:\nmy_new_data &lt;- my_data\nYou can then recall/print the values in an object by just typing the name by itself.\nIn the code chunk below, assign a 3 to the object “y” and then print it out.\n\ny &lt;- 3\ny\n\n[1] 3\n\n\nIf you want to assign multiple values, you have to put them in the function c() c means combine. R doesn’t know what to do if you just give it a bunch of values with space or commas, but if you put them as arguments in the combine function, it’ll make them into a vector.\nAny time you need to use several values, even passing as an argument to a function, you have to put them in c() or it won’t work.\n\na &lt;- c(1,2,3,4)\na\n\n[1] 1 2 3 4\n\n\nWhen you are creating objects, try to give them meaningful names so you can remember what they are. You can’t have spaces or operators that mean something else as part of a name. And remember, everything is case sensitive.\nAssign the value 5.4 to water_pH and then try to recall it by typing “water_ph”\n\nwater_pH &lt;- 5.4\n\n#water_ph\n\nYou can also set objects equal to strings, or values that have letters in them. To do this you just have to put the value in quotes, otherwise R will think it is an object name and tell you it doesn’t exist.\nTry: name &lt;- “JP” and then name &lt;- JP\nWhat happens if you forget the ending parenthesis?\nTry: name &lt;- “JP\nR can be cryptic with it’s error messages or other responses, but once you get used to them, you know exactly what is wrong when they pop up.\n\nname &lt;- \"JP\"\n#name &lt;- JP"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#using-functions",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#using-functions",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.4 Using functions",
    "text": "2.4 Using functions\n\nAs an example, let’s try the seq() function, which creates a sequence of numbers.\n\nseq(from = 1, to = 10, by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n#or\n\nseq(1, 10, 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n#or\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n#what does this do\nseq(10,1)\n\n [1] 10  9  8  7  6  5  4  3  2  1"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#read-in-some-data.",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#read-in-some-data.",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.5 Read in some data.",
    "text": "2.5 Read in some data.\nFor the following demonstration we will use the RBI data from a sample of USGS gages we used last class. First we will load the tidyverse library, everything we have done so far is in base R.\nImportant: read_csv() is the tidyverse csv reading function, the base R function is read.csv(). read.csv() will not read your data in as a tibble, which is the format used by tidyverse functions.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nrbi &lt;- read_csv(\"Flashy_Dat_Subset.csv\")\n\nRows: 49 Columns: 26\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): STANAME, STATE, CLASS, AGGECOREGION\ndbl (22): site_no, RBI, RBIrank, DRAIN_SQKM, HUC02, LAT_GAGE, LNG_GAGE, PPTA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#wait-hold-up.-what-is-a-tibble",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#wait-hold-up.-what-is-a-tibble",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.6 Wait, hold up. What is a tibble?",
    "text": "2.6 Wait, hold up. What is a tibble?\nGood question. It’s a fancy way to store data that works well with tidyverse functions. Let’s look at the rbi tibble.\n\nhead(rbi)\n\n# A tibble: 6 × 26\n  site_no    RBI RBIrank STANAME  DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE STATE CLASS\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 1013500 0.0584      35 Fish Ri…     2253.      1     47.2    -68.6 ME    Ref  \n2 1021480 0.208      300 Old Str…       76.7     1     44.9    -67.7 ME    Ref  \n3 1022500 0.198      286 Narragu…      574.      1     44.6    -67.9 ME    Ref  \n4 1029200 0.132      183 Seboeis…      445.      1     46.1    -68.6 ME    Ref  \n5 1030500 0.114      147 Mattawa…     3676.      1     45.5    -68.3 ME    Ref  \n6 1031300 0.297      489 Piscata…      304.      1     45.3    -69.6 ME    Ref  \n# ℹ 16 more variables: AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;,\n#   PPTAVG_SITE &lt;dbl&gt;, T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;,\n#   T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;,\n#   T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, SNOW_PCT_PRECIP &lt;dbl&gt;,\n#   PRECIP_SEAS_IND &lt;dbl&gt;, FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt;\n\n\nNow read in the same data with read.csv() which will NOT read the data as a tibble. How is it different? Output each one in the Console.\nKnowing the data type for each column is super helpful for a few reasons…. let’s talk about them.\nTypes: int, dbl, fctr, char, logical\n\nrbi_NT &lt;- read.csv(\"Flashy_Dat_Subset.csv\")\n\nhead(rbi_NT)\n\n  site_no        RBI RBIrank                                     STANAME\n1 1013500 0.05837454      35            Fish River near Fort Kent, Maine\n2 1021480 0.20797008     300               Old Stream near Wesley, Maine\n3 1022500 0.19805382     286     Narraguagus River at Cherryfield, Maine\n4 1029200 0.13151299     183         Seboeis River near Shin Pond, Maine\n5 1030500 0.11350485     147 Mattawamkeag River near Mattawamkeag, Maine\n6 1031300 0.29718786     489       Piscataquis River at Blanchard, Maine\n  DRAIN_SQKM HUC02 LAT_GAGE  LNG_GAGE STATE CLASS AGGECOREGION PPTAVG_BASIN\n1     2252.7     1 47.23739 -68.58264    ME   Ref    NorthEast        97.42\n2       76.7     1 44.93694 -67.73611    ME   Ref    NorthEast       115.39\n3      573.6     1 44.60797 -67.93524    ME   Ref    NorthEast       120.07\n4      444.9     1 46.14306 -68.63361    ME   Ref    NorthEast       102.19\n5     3676.2     1 45.50097 -68.30596    ME   Ref    NorthEast       108.19\n6      304.4     1 45.26722 -69.58389    ME   Ref    NorthEast       119.83\n  PPTAVG_SITE T_AVG_BASIN T_AVG_SITE T_MAX_BASIN T_MAXSTD_BASIN T_MAX_SITE\n1       93.53        3.00        3.0        9.67          0.202       10.0\n2      117.13        5.71        5.8       11.70          0.131       11.9\n3      129.56        5.95        6.3       11.90          0.344       12.2\n4      103.24        3.61        4.0        9.88          0.231       10.4\n5      113.13        4.82        5.4       10.75          0.554       11.7\n6      120.93        3.60        4.2        9.57          0.431       11.0\n  T_MIN_BASIN T_MINSTD_BASIN T_MIN_SITE   PET SNOW_PCT_PRECIP PRECIP_SEAS_IND\n1       -2.49          0.269       -2.7 504.7            36.9           0.102\n2       -0.85          0.123       -0.6 554.2            39.5           0.046\n3        0.06          0.873        1.4 553.1            38.2           0.047\n4       -2.13          0.216       -1.5 513.0            36.4           0.070\n5       -1.49          0.251       -1.2 540.8            37.2           0.033\n6       -2.46          0.268       -1.7 495.8            40.2           0.030\n  FLOWYRS_1990_2009 wy00_09\n1                20      10\n2                11      10\n3                20      10\n4                11      10\n5                20      10\n6                13      10"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#data-wrangling-in-dplyr",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#data-wrangling-in-dplyr",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.7 Data wrangling in dplyr",
    "text": "2.7 Data wrangling in dplyr\nIf you forget syntax or what the following functions do, here is an excellent cheat sheet: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf\nWe will demo five functions below:\n\nfilter() - returns rows that meet specified conditions\narrange() - reorders rows\nselect() - pull out variables (columns)\nmutate() - create new variables (columns) or reformat existing ones\nsummarize() - collapse groups of values into summary stats\n\nWith all of these, the first argument is the data and then the arguments after that specify what you want the function to do."
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#filter",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#filter",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.8 Filter",
    "text": "2.8 Filter\nWrite an expression that returns data in rbi for the state of Maine (ME)\nOperators:\n== equal\n!= not equal\n&gt;= , &lt;= greater than or equal to, less than or equal to\n&gt;, &lt; greater than or less then\n%in% included in a list of values\n& and\n| or\n\nfilter(rbi, STATE == \"ME\")\n\n# A tibble: 13 × 26\n   site_no    RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE STATE CLASS\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1013500 0.0584      35 Fish R…     2253.      1     47.2    -68.6 ME    Ref  \n 2 1021480 0.208      300 Old St…       76.7     1     44.9    -67.7 ME    Ref  \n 3 1022500 0.198      286 Narrag…      574.      1     44.6    -67.9 ME    Ref  \n 4 1029200 0.132      183 Seboei…      445.      1     46.1    -68.6 ME    Ref  \n 5 1030500 0.114      147 Mattaw…     3676.      1     45.5    -68.3 ME    Ref  \n 6 1031300 0.297      489 Piscat…      304.      1     45.3    -69.6 ME    Ref  \n 7 1031500 0.320      545 Piscat…      769       1     45.2    -69.3 ME    Ref  \n 8 1037380 0.318      537 Ducktr…       39       1     44.3    -69.1 ME    Ref  \n 9 1044550 0.242      360 Spence…      500.      1     45.3    -70.2 ME    Ref  \n10 1047000 0.344      608 Carrab…      909.      1     44.9    -70.0 ME    Ref  \n11 1054200 0.492      805 Wild R…      181       1     44.4    -71.0 ME    Ref  \n12 1055000 0.450      762 Swift …      251.      1     44.6    -70.6 ME    Ref  \n13 1057000 0.326      561 Little…      191.      1     44.3    -70.5 ME    Ref  \n# ℹ 16 more variables: AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;,\n#   PPTAVG_SITE &lt;dbl&gt;, T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;,\n#   T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;,\n#   T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, SNOW_PCT_PRECIP &lt;dbl&gt;,\n#   PRECIP_SEAS_IND &lt;dbl&gt;, FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt;\n\n\n\n2.8.1 Multiple conditions\nHow many gages are there in Maine with an rbi greater than 0.25\n\nfilter(rbi, STATE == \"ME\" & RBI &gt; 0.25)\n\n# A tibble: 7 × 26\n  site_no   RBI RBIrank STANAME   DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE STATE CLASS\n    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 1031300 0.297     489 Piscataq…       304.     1     45.3    -69.6 ME    Ref  \n2 1031500 0.320     545 Piscataq…       769      1     45.2    -69.3 ME    Ref  \n3 1037380 0.318     537 Ducktrap…        39      1     44.3    -69.1 ME    Ref  \n4 1047000 0.344     608 Carrabas…       909.     1     44.9    -70.0 ME    Ref  \n5 1054200 0.492     805 Wild Riv…       181      1     44.4    -71.0 ME    Ref  \n6 1055000 0.450     762 Swift Ri…       251.     1     44.6    -70.6 ME    Ref  \n7 1057000 0.326     561 Little A…       191.     1     44.3    -70.5 ME    Ref  \n# ℹ 16 more variables: AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;,\n#   PPTAVG_SITE &lt;dbl&gt;, T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;,\n#   T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;,\n#   T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, SNOW_PCT_PRECIP &lt;dbl&gt;,\n#   PRECIP_SEAS_IND &lt;dbl&gt;, FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt;"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#arrange",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#arrange",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.9 Arrange",
    "text": "2.9 Arrange\nArrange sorts by a column in your dataset.\nSort the rbi data by the RBI column in ascending and then descending order\n\narrange(rbi, RBI)\n\n# A tibble: 49 × 26\n   site_no    RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE STATE CLASS\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1305500 0.0464      18 SWAN R…       21.3     2     40.8    -73.0 NY    Non-…\n 2 1013500 0.0584      35 Fish R…     2253.      1     47.2    -68.6 ME    Ref  \n 3 1306460 0.0587      37 CONNET…       55.7     2     40.8    -73.2 NY    Non-…\n 4 1030500 0.114      147 Mattaw…     3676.      1     45.5    -68.3 ME    Ref  \n 5 1029200 0.132      183 Seboei…      445.      1     46.1    -68.6 ME    Ref  \n 6 1117468 0.172      244 BEAVER…       25.3     1     41.5    -71.6 RI    Ref  \n 7 1022500 0.198      286 Narrag…      574.      1     44.6    -67.9 ME    Ref  \n 8 1021480 0.208      300 Old St…       76.7     1     44.9    -67.7 ME    Ref  \n 9 1162500 0.213      311 PRIEST…       49.7     1     42.7    -72.1 MA    Ref  \n10 1117370 0.230      338 QUEEN …       50.5     1     41.5    -71.6 RI    Ref  \n# ℹ 39 more rows\n# ℹ 16 more variables: AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;,\n#   PPTAVG_SITE &lt;dbl&gt;, T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;,\n#   T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;,\n#   T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, SNOW_PCT_PRECIP &lt;dbl&gt;,\n#   PRECIP_SEAS_IND &lt;dbl&gt;, FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt;\n\narrange(rbi, desc(RBI))\n\n# A tibble: 49 × 26\n   site_no   RBI RBIrank STANAME  DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE STATE CLASS\n     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1311500 0.856    1017 VALLEY …       18.1     2     40.7    -73.7 NY    Non-…\n 2 1054200 0.492     805 Wild Ri…      181       1     44.4    -71.0 ME    Ref  \n 3 1187300 0.487     800 HUBBARD…       53.9     1     42.0    -72.9 MA    Ref  \n 4 1105600 0.484     797 OLD SWA…       12.7     1     42.2    -70.9 MA    Non-…\n 5 1055000 0.450     762 Swift R…      251.      1     44.6    -70.6 ME    Ref  \n 6 1195100 0.430     744 INDIAN …       14.8     1     41.3    -72.5 CT    Ref  \n 7 1181000 0.420     732 WEST BR…      244.      1     42.2    -72.9 MA    Ref  \n 8 1350000 0.414     721 SCHOHAR…      612.      2     42.3    -74.4 NY    Ref  \n 9 1121000 0.404     710 MOUNT H…       70.3     1     41.8    -72.2 CT    Ref  \n10 1169000 0.395     688 NORTH R…      231.      1     42.6    -72.7 MA    Ref  \n# ℹ 39 more rows\n# ℹ 16 more variables: AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;,\n#   PPTAVG_SITE &lt;dbl&gt;, T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;,\n#   T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;,\n#   T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, SNOW_PCT_PRECIP &lt;dbl&gt;,\n#   PRECIP_SEAS_IND &lt;dbl&gt;, FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt;"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#select",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#select",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.10 Select",
    "text": "2.10 Select\nThere are too many columns! You will often want to do this when you are manipulating the structure of your data and need to trim it down to only include what you will use.\nSelect Site name, state, and RBI from the rbi data\nNote they come back in the order you put them in in the function, not the order they were in in the original data.\nYou can do a lot more with select, especially when you need to select a bunch of columns but don’t want to type them all out. But we don’t need to cover all that today. For a taste though, if you want to select a group of columns you can specify the first and last with a colon in between (first:last) and it’ll return all of them. Select the rbi columns from site_no to DRAIN_SQKM.\n\nselect(rbi, STANAME, STATE, RBI)\n\n# A tibble: 49 × 3\n   STANAME                                      STATE    RBI\n   &lt;chr&gt;                                        &lt;chr&gt;  &lt;dbl&gt;\n 1 Fish River near Fort Kent, Maine             ME    0.0584\n 2 Old Stream near Wesley, Maine                ME    0.208 \n 3 Narraguagus River at Cherryfield, Maine      ME    0.198 \n 4 Seboeis River near Shin Pond, Maine          ME    0.132 \n 5 Mattawamkeag River near Mattawamkeag, Maine  ME    0.114 \n 6 Piscataquis River at Blanchard, Maine        ME    0.297 \n 7 Piscataquis River near Dover-Foxcroft, Maine ME    0.320 \n 8 Ducktrap River near Lincolnville, Maine      ME    0.318 \n 9 Spencer Stream near Grand Falls, Maine       ME    0.242 \n10 Carrabassett River near North Anson, Maine   ME    0.344 \n# ℹ 39 more rows\n\nselect(rbi, site_no:DRAIN_SQKM)\n\n# A tibble: 49 × 5\n   site_no    RBI RBIrank STANAME                                     DRAIN_SQKM\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                                            &lt;dbl&gt;\n 1 1013500 0.0584      35 Fish River near Fort Kent, Maine                2253. \n 2 1021480 0.208      300 Old Stream near Wesley, Maine                     76.7\n 3 1022500 0.198      286 Narraguagus River at Cherryfield, Maine          574. \n 4 1029200 0.132      183 Seboeis River near Shin Pond, Maine              445. \n 5 1030500 0.114      147 Mattawamkeag River near Mattawamkeag, Maine     3676. \n 6 1031300 0.297      489 Piscataquis River at Blanchard, Maine            304. \n 7 1031500 0.320      545 Piscataquis River near Dover-Foxcroft, Mai…      769  \n 8 1037380 0.318      537 Ducktrap River near Lincolnville, Maine           39  \n 9 1044550 0.242      360 Spencer Stream near Grand Falls, Maine           500. \n10 1047000 0.344      608 Carrabassett River near North Anson, Maine       909. \n# ℹ 39 more rows"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#mutate",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#mutate",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.11 Mutate",
    "text": "2.11 Mutate\nUse mutate to add new columns based on additional ones. Common uses are to create a column of data in different units, or to calculate something based on two columns. You can also use it to just update a column, by naming the new column the same as the original one (but be careful because you’ll lose the original one!). I commonly use this when I am changing the datatype of a column, say from a character to a factor or a string to a date.\nCreate a new column in rbi called T_RANGE by subtracting T_MIN_SITE from T_MAX_SITE\n\nmutate(rbi, T_RANGE = T_MAX_SITE - T_MIN_SITE)\n\n# A tibble: 49 × 27\n   site_no    RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE STATE CLASS\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1013500 0.0584      35 Fish R…     2253.      1     47.2    -68.6 ME    Ref  \n 2 1021480 0.208      300 Old St…       76.7     1     44.9    -67.7 ME    Ref  \n 3 1022500 0.198      286 Narrag…      574.      1     44.6    -67.9 ME    Ref  \n 4 1029200 0.132      183 Seboei…      445.      1     46.1    -68.6 ME    Ref  \n 5 1030500 0.114      147 Mattaw…     3676.      1     45.5    -68.3 ME    Ref  \n 6 1031300 0.297      489 Piscat…      304.      1     45.3    -69.6 ME    Ref  \n 7 1031500 0.320      545 Piscat…      769       1     45.2    -69.3 ME    Ref  \n 8 1037380 0.318      537 Ducktr…       39       1     44.3    -69.1 ME    Ref  \n 9 1044550 0.242      360 Spence…      500.      1     45.3    -70.2 ME    Ref  \n10 1047000 0.344      608 Carrab…      909.      1     44.9    -70.0 ME    Ref  \n# ℹ 39 more rows\n# ℹ 17 more variables: AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;,\n#   PPTAVG_SITE &lt;dbl&gt;, T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;,\n#   T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;,\n#   T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, SNOW_PCT_PRECIP &lt;dbl&gt;,\n#   PRECIP_SEAS_IND &lt;dbl&gt;, FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt;,\n#   T_RANGE &lt;dbl&gt;\n\n\nWhen downloading data from the USGS through R, you have to enter the gage ID as a character, even though they are all made up of numbers. So to practice doing this, update the site_no column to be a character datatype\n\nmutate(rbi, site_no = as.character(site_no))\n\n# A tibble: 49 × 26\n   site_no    RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE STATE CLASS\n   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n 1 1013500 0.0584      35 Fish R…     2253.      1     47.2    -68.6 ME    Ref  \n 2 1021480 0.208      300 Old St…       76.7     1     44.9    -67.7 ME    Ref  \n 3 1022500 0.198      286 Narrag…      574.      1     44.6    -67.9 ME    Ref  \n 4 1029200 0.132      183 Seboei…      445.      1     46.1    -68.6 ME    Ref  \n 5 1030500 0.114      147 Mattaw…     3676.      1     45.5    -68.3 ME    Ref  \n 6 1031300 0.297      489 Piscat…      304.      1     45.3    -69.6 ME    Ref  \n 7 1031500 0.320      545 Piscat…      769       1     45.2    -69.3 ME    Ref  \n 8 1037380 0.318      537 Ducktr…       39       1     44.3    -69.1 ME    Ref  \n 9 1044550 0.242      360 Spence…      500.      1     45.3    -70.2 ME    Ref  \n10 1047000 0.344      608 Carrab…      909.      1     44.9    -70.0 ME    Ref  \n# ℹ 39 more rows\n# ℹ 16 more variables: AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;,\n#   PPTAVG_SITE &lt;dbl&gt;, T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;,\n#   T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;,\n#   T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, SNOW_PCT_PRECIP &lt;dbl&gt;,\n#   PRECIP_SEAS_IND &lt;dbl&gt;, FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt;"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#summarize",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#summarize",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.12 Summarize",
    "text": "2.12 Summarize\nSummarize will perform an operation on all of your data, or groups if you assign groups.\nUse summarize to compute the mean, min, and max rbi\n\nsummarize(rbi, meanrbi = mean(RBI), maxrbi = max(RBI), minrbi = min(RBI))\n\n# A tibble: 1 × 3\n  meanrbi maxrbi minrbi\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   0.316  0.856 0.0464\n\n\nNow use the group function to group by state and then summarize in the same way as above\n\nrbistate &lt;- group_by(rbi, STATE)\nsummarize(rbistate, meanrbi = mean(RBI), maxrbi = max(RBI), minrbi = min(RBI))\n\n# A tibble: 7 × 4\n  STATE meanrbi maxrbi minrbi\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 CT      0.366  0.430 0.295 \n2 MA      0.367  0.487 0.213 \n3 ME      0.269  0.492 0.0584\n4 NH      0.336  0.368 0.265 \n5 NY      0.342  0.856 0.0464\n6 RI      0.201  0.230 0.172 \n7 VT      0.299  0.365 0.231"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#multiple-operations-with-pipes",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#multiple-operations-with-pipes",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.13 Multiple operations with pipes",
    "text": "2.13 Multiple operations with pipes\nThe pipe operator |&gt; allows you to perform multiple operations in a sequence without saving intermediate steps. Not only is this more efficient, but structuring operations with pipes is also more intuitive than nesting functions within functions (the other way you can do multiple operations). The |&gt; pipe is included in base R, if you see code elsewhere that has a %&gt;% pipe, that is the original pipe, from the magrittr package. It was incorporated into base R and is now |&gt; but works the same!\n\n2.13.1 Let’s say we want to tell R to make a PB&J sandwich by using the pbbread(), jbread(), and joinslices() functions and the data “ingredients”. If we do this saving each step if would look like this:\n\nsando &lt;- pbbread(ingredients)\n\n\nsando &lt;- jbread(sando)\n\n\nsando &lt;- joinslices(sando)\n\n\n\n2.13.2 If we nest the functions together we get this\n\njoinslice(jbread(pbbread(ingredients)))\n\nEfficient… but tough to read/interpret\n\n\n2.13.3 Using the pipe it would look like this\n\ningredients|&gt;\npbbread() |&gt;\njbread() |&gt;\njoinslices()\n\nMuch easier to follow!\n\n\n2.13.4 When you use the pipe, it basically takes whatever came out of the first function and puts it into the data argument for the next one\nso rbi |&gt; group_by(STATE) is the same as group_by(rbi, STATE)\nTake the groupby and summarize code from above and perform the operation using the pipe\n\nrbi |&gt;\n  group_by(STATE) |&gt;\n  summarize(meanrbi = mean(RBI), maxrbi = max(RBI), minrbi = min(RBI))\n\n# A tibble: 7 × 4\n  STATE meanrbi maxrbi minrbi\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 CT      0.366  0.430 0.295 \n2 MA      0.367  0.487 0.213 \n3 ME      0.269  0.492 0.0584\n4 NH      0.336  0.368 0.265 \n5 NY      0.342  0.856 0.0464\n6 RI      0.201  0.230 0.172 \n7 VT      0.299  0.365 0.231"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#save-your-results-to-a-new-tibble",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#save-your-results-to-a-new-tibble",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.14 Save your results to a new tibble",
    "text": "2.14 Save your results to a new tibble\nWe have just been writing everything to the screen so we can see what we are doing… In order to save anything we do with these functions to work with it later, we just have to use the assignment operator (&lt;-) to store the data.\nOne kind of awesome thing about the assignment operator is that it works both ways…\nx &lt;- 3 and 3 -&gt; x do the same thing (WHAT?!)\nSo you can do the assignment at the beginning of the end of your dplyr workings, whatever you like best.\nUse the assignment operator to save the summary table you just made.\n\nstateRBIs &lt;- rbi |&gt;\n  group_by(STATE) |&gt;\n  summarize(meanrbi = mean(RBI), maxrbi = max(RBI), minrbi = min(RBI))\n\n# Notice when you do this it doesn't output the result... \n# You can see what you did by clickon in stateRBIs in your environment panel\n# or just type stateRBIs\n\nstateRBIs\n\n# A tibble: 7 × 4\n  STATE meanrbi maxrbi minrbi\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 CT      0.366  0.430 0.295 \n2 MA      0.367  0.487 0.213 \n3 ME      0.269  0.492 0.0584\n4 NH      0.336  0.368 0.265 \n5 NY      0.342  0.856 0.0464\n6 RI      0.201  0.230 0.172 \n7 VT      0.299  0.365 0.231"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#what-about-nas",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#what-about-nas",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.15 What about NAs?",
    "text": "2.15 What about NAs?\nWe will talk more about this when we discuss stats, but some operations will fail if there are NA’s in the data. If appropriate, you can tell functions like mean() to ignore NAs. You can also use drop_na() if you’re working with a tibble. But be aware if you use that and save the result, drop_na() gets rid of the whole row, not just the NA. Because what would you replace it with…. an NA?\n\nx &lt;- c(1,2,3,4,NA)\nmean(x, na.rm = TRUE)\n\n[1] 2.5"
  },
  {
    "objectID": "02-Programming_Basics_Demo_COMPLETE.html#what-are-some-things-you-think-ill-ask-you-to-do-for-the-activity-next-class",
    "href": "02-Programming_Basics_Demo_COMPLETE.html#what-are-some-things-you-think-ill-ask-you-to-do-for-the-activity-next-class",
    "title": "2  R Tidyverse Programming Basics",
    "section": "2.16 What are some things you think I’ll ask you to do for the activity next class?",
    "text": "2.16 What are some things you think I’ll ask you to do for the activity next class?"
  },
  {
    "objectID": "03-Activity-Intro-Skills.html#problem-1",
    "href": "03-Activity-Intro-Skills.html#problem-1",
    "title": "3  ACTIVITY Intro Skills",
    "section": "3.1 Problem 1",
    "text": "3.1 Problem 1\nLoad the tidyverse and lubridate libraries.\nRead in the PINE_NFDR_Jan-Mar_2010 csv using read_csv()\nMake a plot with the date on the x axis, discharge on the y axis. Show the discharge of the two watersheds as a line, coloring by watershed (StationID)"
  },
  {
    "objectID": "03-Activity-Intro-Skills.html#problem-2",
    "href": "03-Activity-Intro-Skills.html#problem-2",
    "title": "3  ACTIVITY Intro Skills",
    "section": "3.2 Problem 2",
    "text": "3.2 Problem 2\nMake a boxplot to compare the discharge of Pine to NFDR for February 2010.\nHint: use the pipe operator and the filter() function.\nHint2: when you filter dates, you have to let R know you’re giving it a date. You can do this by using the mdy() function from lubridate."
  },
  {
    "objectID": "03-Activity-Intro-Skills.html#problem-3",
    "href": "03-Activity-Intro-Skills.html#problem-3",
    "title": "3  ACTIVITY Intro Skills",
    "section": "3.3 Problem 3",
    "text": "3.3 Problem 3\nRead in the Flashy Dat Subset file.\nFor only sites in ME, NH, and VT: Plot PET (Potential Evapotranspiration) on the X axis and RBI (flashiness index) on the Y axis. Color the points based on what state they are in. Use the classic ggplot theme."
  },
  {
    "objectID": "03-Activity-Intro-Skills.html#problem-4",
    "href": "03-Activity-Intro-Skills.html#problem-4",
    "title": "3  ACTIVITY Intro Skills",
    "section": "3.4 Problem 4",
    "text": "3.4 Problem 4\nWe want to look at the amount of snow for each site in the flashy dataset. Problem is, we are only given the average amount of total precip (PPTAVG_BASIN) and the percentage of snow (SNOW_PCT_PRECIP).\nCreate a new column in the dataset called SNOW_AVG_BASIN and make it equal to the average total precip times the percentage of snow (careful with the percentage number).\nMake a barplot showing the amount of snow for each site in Maine. Put station name on the x axis and snow amount on the y. You have to add something to geom_bar() to use it for a 2 variable plot… check out the ggplot cheatsheet or do a quick internet search.\nThe x axis of the resulting plot looks terrible! Can you figure out how to rotate the X axis labels so we can read them?"
  },
  {
    "objectID": "03-Activity-Intro-Skills.html#problem-5",
    "href": "03-Activity-Intro-Skills.html#problem-5",
    "title": "3  ACTIVITY Intro Skills",
    "section": "3.5 Problem 5",
    "text": "3.5 Problem 5\nCreate a new tibble that contains the min, max, and mean PET for each state. Sort the tibble by mean PET from high to low. Give your columns meaningful names within the summarize function or using rename().\nBe sure your code outputs the tibble."
  },
  {
    "objectID": "03-Activity-Intro-Skills.html#problem-6",
    "href": "03-Activity-Intro-Skills.html#problem-6",
    "title": "3  ACTIVITY Intro Skills",
    "section": "3.6 Problem 6",
    "text": "3.6 Problem 6\nTake the tibble from problem 5. Create a new column that is the Range of the PET (max PET - min PET). Then get rid of the max PET and min PET columns so the tibble just has columns for State, mean PET, and PET range.\nBe sure your code outputs the tibble."
  },
  {
    "objectID": "04-Intro-Stats-Completed.html#reading-for-this-section-statistical-methods-in-water-resources-chapter-1",
    "href": "04-Intro-Stats-Completed.html#reading-for-this-section-statistical-methods-in-water-resources-chapter-1",
    "title": "4  Introduction to Basic Statistics",
    "section": "4.1 Reading for this section: Statistical Methods in Water Resources: Chapter 1",
    "text": "4.1 Reading for this section: Statistical Methods in Water Resources: Chapter 1\nhttps://pubs.usgs.gov/tm/04/a03/tm4a3.pdf"
  },
  {
    "objectID": "04-Intro-Stats-Completed.html#questions-for-today",
    "href": "04-Intro-Stats-Completed.html#questions-for-today",
    "title": "4  Introduction to Basic Statistics",
    "section": "4.2 Questions for today:",
    "text": "4.2 Questions for today:\n\nWhat is the difference between a sample and a population?\nHow do we look at the distribution of data in a sample\nHow do we measure aspects of a distribution\nWhat is a normal distribution?\n\nFirst let’s generate some synthetic data and talk about how to visualize it.\n\n#generate a normal distribution\nExNorm &lt;- rnorm(1000, mean = 5) |&gt; \n  as_tibble()\n\n#look at distributions\n#histogram\nExNorm |&gt;\n  ggplot(aes(value)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n#pdf\nExNorm |&gt;\n  ggplot(aes(value)) +\n  stat_density()\n\n\n\n#Let's generate a plot that makes comparing these two easier\n\n\n4.2.1 Stack plots to compare histogram and pdf\nWe will save each plot as ggplot object and then output them using the patchwork package (loaded in the setup chunk).\nWhat is the difference between a histogram and a pdf?\nWhat features of the histogram are preserved? Which are lost?\n\n#histogram\nexhist &lt;- ExNorm |&gt;\n  ggplot(aes(value)) +\n  geom_histogram()\n\n#pdf\nexpdf &lt;- ExNorm |&gt;\n  ggplot(aes(value)) +\n  stat_density()\n\n#put the plots side by side with + or on top of each other with /\nexhist/expdf\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "04-Intro-Stats-Completed.html#what-is-the-difference-between-a-sample-and-a-population.",
    "href": "04-Intro-Stats-Completed.html#what-is-the-difference-between-a-sample-and-a-population.",
    "title": "4  Introduction to Basic Statistics",
    "section": "4.3 What is the difference between a sample and a population.",
    "text": "4.3 What is the difference between a sample and a population.\nSimply put: a population is the thing you are trying to measure. A sample is the data you measure in an effort to measure the population. A sample is a subset of a population.\nLet’s write some code for an example:\nWe will create a POPULATION that is a large set of numbers. Think of this is as the concentration of Calcium in every bit of water in a lake. Then we will create a SAMPLE by randomly grabbing values from the POPULATION. This simulates us going around in a boat and taking grab samples in an effort to figure out the concentration of calcium in the lake.\nWe can then run this code a bunch of times, you’ll get a different sample each time. You can also take a smaller or larger number of samples by changing “size” in the sample() function.\nHow does your sample distribution look similar or different from the population?\nWhy does the sample change every time you run it?\nWhat happens as you increase or decrease the number of samples?\nWhat happens if you set the number of samples to the size of the population?\n\nall_the_water &lt;- rnorm(10000, mean = 6) |&gt; as_tibble()\n\nsample_of_water &lt;- sample(all_the_water$value, size = 100, replace = FALSE) |&gt; as_tibble()\n\npopulation_hist &lt;- all_the_water |&gt;\n  ggplot(aes(value))+\n  geom_histogram()+\n  ggtitle(\"Population: All the water in the lake\")\n\nsample_hist &lt;- sample_of_water |&gt;\n  ggplot(aes(value))+\n  geom_histogram()+\n  ggtitle(\"Your sample of the lake\")\n\npopulation_hist + sample_hist\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "04-Intro-Stats-Completed.html#measuring-our-sample-distribution-central-tendency.",
    "href": "04-Intro-Stats-Completed.html#measuring-our-sample-distribution-central-tendency.",
    "title": "4  Introduction to Basic Statistics",
    "section": "4.4 Measuring our sample distribution: central tendency.",
    "text": "4.4 Measuring our sample distribution: central tendency.\nWhen we take a sample of a population, there are a few things we will want to measure about the distribution of values: where is the middle, how variable is it, and is it skewed to one side or another?\nThe first of these, “where is the middle?” is addressed with measures of central tendency. We will discuss three possible ways to measure this. The mean, median, and weighted mean.\nTo explain the importance of choosing between the mean and median, we will first import some discharge data. Read in the PINE discharge data.\n\npineQ &lt;- read_csv(\"PINE_Jan-Mar_2010.csv\")\n\nRows: 2160 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): StationID, surrogate\ndbl  (5): cfs, year, quarter, month, day\ndttm (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTo find the mean (average), you just sum up all the values in your sample and divide by the number of values.\nTo find the median, you put the values IN ORDER, and choose the middle value. The middle value is the one where there are the same number of values higher than that value as there are values lower than it.\nBecause it uses the order of the values rather than just the values themselves, the median is resistant to skewed distributions. This means it is less effected by very large or very small values compared to most values in the sample data.\nLet’s look at our normal distribution from earlier (ExNorm) compared to the Pine watershed discharge (pineQ)\nNote that distributions like pineQ, that are positively skewed, are very common in environmental data.\n\n#Calculate mean and median for cfs in pineQ and values in ExNorm\npineMean &lt;- mean(pineQ$cfs)\npineMedian &lt;- median(pineQ$cfs)\n\nxmean &lt;- mean(ExNorm$value)\nxmedian &lt;- median(ExNorm$value)\n\n#plot mean and median on the ExNorm distribution\nEx &lt;- ExNorm |&gt; ggplot(aes(value)) +\n  geom_histogram()+\n  geom_vline(xintercept = xmean, color = \"red\")+\n  geom_vline(xintercept = xmedian, color = \"blue\")\n\n#plot mean and median on the pineQ discharge histogram\nPineP &lt;- pineQ |&gt; ggplot(aes(cfs)) +\n  geom_histogram()+\n  geom_vline(xintercept = pineMean, color = \"red\")+\n  geom_vline(xintercept = pineMedian, color = \"blue\")\n\nEx / PineP  \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n4.4.1 So what’s a weighted average?\nWhen you compute a standard mean or median, you are giving equal weight to each measurement. Adding up all the values in a sample and dividing by the number of samples is the same as multiplying each value by 1/# of samples. For instance if you had ten samples, to calculate the mean you would add them up and divide by 10. This is the same as multiplying each value by 1/10 and then adding them up. Each value is equally weighted at 1/10.\nThere are certain situations in which this is not the ideal way to calculate an average. A common one in hydrology is that you have samples that are supposed to represent different portions of an area. One sample may be taken to measure a forest type that takes up 100 ha of a watershed while another sample represents a forest type that only takes up 4 ha. You may not want to simply average those values!\nAnother example is precipitation gages. In the image below, you see there are 5 rain gages. To get a precipitation number for the watershed, we could just average them, or we could assume they represent an area of the watershed and then weight their values by the area they represent. One method of designating the areas is by using Theissen polygons (the middle watershed). Another method of weighting is isohyetal contours, but we won’t worry about that for now!\nIn the weighted situation, we find the average by multiplying each precipitation values by the proportion of the watershed it represents, shown by the Thiessen polygons, and then add them all together. Let’s do an example.\nsource: https://edx.hydrolearn.org/assets/courseware/v1/e5dc65098f1e8c5faacae0e171e28ccf/asset-v1:HydroLearn+HydroLearn401+2019_S2+type@asset+block/l2_image004.png\nThe precip values for the watershed above are 4.5, 5.5, 5.8, 4.7, and 3.0\nWe will assume the proportions of the watershed that each gauge represents are 0.20, 0.15, 0.40, 0.15, 0.10, respectively (or 20%, 15%, 40%, 15%, 10%)\nWrite some code to compute the regular mean precip from the values, and then the weighted mean.\n\nprecip &lt;- c(4.5, 5.5, 5.8, 4.7, 3.0)\nweights &lt;- c(0.2, 0.15, 0.4, 0.15, 0.1)\n\nmean(precip)\n\n[1] 4.7\n\nsum(precip * weights)\n\n[1] 5.05"
  },
  {
    "objectID": "04-Intro-Stats-Completed.html#measures-of-variability",
    "href": "04-Intro-Stats-Completed.html#measures-of-variability",
    "title": "4  Introduction to Basic Statistics",
    "section": "4.5 Measures of variability",
    "text": "4.5 Measures of variability\nMeasures of variability allow us to measure the width of our sample data histogram or pdf. If all the values in our sample are close together, we would have small measures of variability, and a pointy pdf/histogram. If they vary more, we would have larger measures of variability and a broad pdf/histogram.\nWe will explore four measures of variability:\n\n4.5.0.1 Variance:\nSum of the squared difference of each value from the mean divided by the number of samples minus 1. var()\n(https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf) source: https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf\n\n\n4.5.0.2 Standard deviation:\nThe square root of the variance sd()\n**Both variance and standard deviation are sensitive to outliers.\n\n\n4.5.0.3 CV: Coefficient of Variation\nCV is simply the standard deviation divided by the mean of the data. Because you divide by the mean, CV is dimensionless. This allows you to use it to compare the variation in samples with very different magnitudes.\n\n\n4.5.0.4 IQR: Interquartile Range\nIQR is resistant to outliers because it works like a median. It measures the range of the middle 50% of the data in your distribution. So the IQR is the difference between the value between the 75th and 25th percentiles of your data, where the 75th percentile means 75% of the data is BELOW that value and the 25th percentile means 25% is below that value. Using the same vocabulary, the median is the same as the 50th percentile of the data.\nIf you ask R for the QUANTILES of your sample data, it will give you the values at which 0%, 25%, 50%, 75%, and 100% of the data are below. These are the 1,2,3,4, and 5th quantiles. Therefore, the IQR is the difference between the 4th and 2nd quantile.\nOkay, code time.\nFirst, let’s explore how changing the variability of a distribution changes the shape of it’s distribution. Create a plot a random normal distribution using rnorm() and set sd to different numbers. Make the mean of the distribution 0, the sample size 300, and the standard deviation 1 to start. Then increase the standard deviation incrementally to 10 and see what happens. Make the limits of the x axis on the plot -30 to 30.\n\nrnorm(300, mean = 0, sd = 1) |&gt; as_tibble() |&gt;\n  ggplot(aes(value))+\n  stat_density()+\n  xlim(c(-30,30))\n\n\n\n\nNow let’s calculate the standard deviation, variance, coefficient of variation, and IQR of the Pine discharge data.\n\n#standard deviation\nsd(pineQ$cfs)\n\n[1] 84.47625\n\n#variance\nvar(pineQ$cfs)\n\n[1] 7136.237\n\n#coefficient of variation\nsd(pineQ$cfs)/mean(pineQ$cfs)\n\n[1] 2.800221\n\n#IQR using the IQR funciton\nIQR(pineQ$cfs)\n\n[1] 8.1325\n\n#IQR using the quantile function\nquants &lt;- quantile(pineQ$cfs)\nquants[4] - quants[2]\n\n   75% \n8.1325 \n\n\n\n\n4.5.0.5 What about how lopsided the distribution is?\nThere are several ways to measure this as well, but we are just going to look at one: The Quartile skew. The quartile skew is the difference between the upper quartiles (50th-75th) and the lower quartiles (25th-50th) divided by the IQR (75th-25th).\n source: https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf\nLet’s look at the quartile skew of the two distributions we’ve been measuring. Calculate it for the pineQ discharge data and the random normal distribution we generated.\nWhich one is more skewed?\n\nquantsP &lt;- quantile(pineQ$cfs)\n\n((quantsP[3]-quantsP[2]) - (quantsP[2] - quantsP[1])) / quantsP[3] - quantsP[1]\n\n      50% \n-4.837233 \n\nquantsX &lt;- quantile(ExNorm$value)\n\n((quantsX[3]-quantsX[2]) - (quantsX[2] - quantsX[1])) / quantsX[3] - quantsX[1]\n\n      50% \n-1.961801"
  },
  {
    "objectID": "04-Intro-Stats-Completed.html#what-is-a-normal-distribution-and-how-can-we-determine-if-we-have-one",
    "href": "04-Intro-Stats-Completed.html#what-is-a-normal-distribution-and-how-can-we-determine-if-we-have-one",
    "title": "4  Introduction to Basic Statistics",
    "section": "4.6 What is a normal distribution and how can we determine if we have one?",
    "text": "4.6 What is a normal distribution and how can we determine if we have one?\nThe distribution we generated with rnorm() is a normal distribution. The distribution of pineQ discharge is not normal. Now that we’ve looked at different ways to characterize distributions, we have the vocabulary to describe why.\nNormal distributions:\n\nmean = median, half values to the right, half to the left\nsymmetric (not skewed)\nsingle peak\n\nMany statistical tests require that the distribution of the data you put into them is normally distributed. BE CAREFUL! There are also tests that use ranked data. Similar to how the median is resistant to outliers, these rank-based tests are resistant to non-normal data. Two popular ones are Kruskal-Wallis and Wilcoxon rank-sum.\nBut how far off can you be before you don’t consider a distribution normal? Seems like a judgement call!\nR to the rescue! There is a built in test for normality called shapiro.test(), which performs the Shapiro-Wilk test of normality. The hypothesis this test tests is “The distribution is normal.” So if this function returns a p-value less than 0.05, you reject that hypothesis and your function is NOT normal.\nYou can also make a quantile-quantile plot. A straight line on this plot indicates a normal distribution, a non-straight line indicates it is not normal.\n\nshapiro.test(pineQ$cfs)\n\n\n    Shapiro-Wilk normality test\n\ndata:  pineQ$cfs\nW = 0.27155, p-value &lt; 2.2e-16\n\nqqnorm(pineQ$cfs)"
  },
  {
    "objectID": "05-Intro-Stats-Activity.html#problem-1",
    "href": "05-Intro-Stats-Activity.html#problem-1",
    "title": "5  Intro Stats Activity",
    "section": "5.1 Problem 1",
    "text": "5.1 Problem 1\nLoad the tidyverse and patchwork libraries and read in the Flashy and Pine datasets."
  },
  {
    "objectID": "05-Intro-Stats-Activity.html#problem-2",
    "href": "05-Intro-Stats-Activity.html#problem-2",
    "title": "5  Intro Stats Activity",
    "section": "5.2 Problem 2",
    "text": "5.2 Problem 2\nUsing the flashy dataset, make a pdf of the average basin rainfall (PPTAVG_BASIN) for the NorthEast AGGECOREGION. On that pdf, add vertical lines showing the mean, median, standard deviation, and IQR. Make each a different color and note which is which in a typed answer below this question. (or if you want an extra challenged, make a custom legend that shows this)"
  },
  {
    "objectID": "05-Intro-Stats-Activity.html#problem-3",
    "href": "05-Intro-Stats-Activity.html#problem-3",
    "title": "5  Intro Stats Activity",
    "section": "5.3 Problem 3",
    "text": "5.3 Problem 3\nPerform a Shapiro-Wilk test for normality on the data from question 2. Using the results from that test and the plot and stats from question 2, discuss whether or not the distribution is normal."
  },
  {
    "objectID": "05-Intro-Stats-Activity.html#problem-4",
    "href": "05-Intro-Stats-Activity.html#problem-4",
    "title": "5  Intro Stats Activity",
    "section": "5.4 Problem 4",
    "text": "5.4 Problem 4\nMake a plot that shows the distribution of the data from the PINE watershed and the NFDR watershed (two pdfs on the same plot). Log the x axis."
  },
  {
    "objectID": "05-Intro-Stats-Activity.html#problem-5",
    "href": "05-Intro-Stats-Activity.html#problem-5",
    "title": "5  Intro Stats Activity",
    "section": "5.5 Problem 5",
    "text": "5.5 Problem 5\nYou want to compare how variable the discharge is in each of the watersheds in question 4. Which measure of spread would you use and why? If you wanted to measure the central tendency which measure would you use and why?"
  },
  {
    "objectID": "05-Intro-Stats-Activity.html#problem-6",
    "href": "05-Intro-Stats-Activity.html#problem-6",
    "title": "5  Intro Stats Activity",
    "section": "5.6 Problem 6",
    "text": "5.6 Problem 6\nCompute 3 measures of spread and 2 measures of central tendency for the PINE and NFDR watershed. (hint: use group_by() and summarize()) Be sure your code outputs the result. Which watershed has higher flow? Which one has more variable flow? How do you know?"
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#goals-for-today",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#goals-for-today",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.1 Goals for today",
    "text": "6.1 Goals for today\n\nGet familiar with the dataRetrieval package\nIntro to joins\nLearn about long vs. wide data and how to change between them\n\nPrep question: How would you get data from the USGS NWIS (non-R)?\nInstall the dataRetrieval package. Load it and the tidyverse.\n\n#install.packages(\"dataRetrieval\")\nlibrary(dataRetrieval)\nlibrary(tidyverse)\nlibrary(lubridate)"
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#exploring-what-dataretrieval-can-do.",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#exploring-what-dataretrieval-can-do.",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.2 Exploring what dataRetrieval can do.",
    "text": "6.2 Exploring what dataRetrieval can do.\nThink about the dataRetrieval as a way to interact with same public data you can access through waterdata.usgs.gov but without having to click on buttons and search around. It makes getting data or doing analyses with USGS data much more reproducible and fast!\nTo explore a few of the capabilities (NOT ALL!!) we will start with the USGS gage on the New River at Radford. The gage number is 03171000.\nThe documentation for the package is extremely helpful: https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html\nI always have to look up how to do things because the package is very specialized! This is the case with most website APIs, in my experience. It’s a good argument for getting good at navigating package documentation! Basically you just look through and try to piece together the recipe for what you want to do using the examples they give in the document.\nFirst, let’s get information about the site using the readNWISsite() and whatNWISdata() functions. Try each out and see what they tell you.\nRemember, all the parameter codes and site names get passed to dataRetrieval functions as characters, so they must be in quotes.\n\n#important: note the site number gets input as a character\nsite &lt;- \"03171000\"\n\n#Information about the site\nsiteinfo &lt;- readNWISsite(site)\n\n#What data is available for the site?\n#Daily values, mean values\ndataAvailable &lt;- whatNWISdata(siteNumber = site, service = \"dv\", statCd = \"00003\")\n\ndataAvailable\n\n  agency_cd  site_no               station_nm site_tp_cd dec_lat_va dec_long_va\n2      USGS 03171000 NEW RIVER AT RADFORD, VA         ST   37.14179   -80.56922\n3      USGS 03171000 NEW RIVER AT RADFORD, VA         ST   37.14179   -80.56922\n4      USGS 03171000 NEW RIVER AT RADFORD, VA         ST   37.14179   -80.56922\n  coord_acy_cd dec_coord_datum_cd  alt_va alt_acy_va alt_datum_cd   huc_cd\n2            U              NAD83 1711.99       0.13       NAVD88 05050001\n3            U              NAD83 1711.99       0.13       NAVD88 05050001\n4            U              NAD83 1711.99       0.13       NAVD88 05050001\n  data_type_cd parm_cd stat_cd  ts_id loc_web_ds medium_grp_cd parm_grp_cd\n2           dv   00010   00003 241564         NA           wat        &lt;NA&gt;\n3           dv   00060   00003 145684         NA           wat        &lt;NA&gt;\n4           dv   00095   00003 145685         NA           wat        &lt;NA&gt;\n   srs_id access_cd begin_date   end_date count_nu\n2 1645597         0 2006-12-20 2009-03-18      704\n3 1645423         0 1907-10-01 2024-02-19    33744\n4 1646694         0 2006-12-20 2008-09-29      534"
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#joins",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#joins",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.3 Joins",
    "text": "6.3 Joins\nWhen we look at what whatNWISdata returns, we see it gives us parameter codes, but doesn’t tell us what they mean. This is a common attribute of databases: you use a common identifier but then have the full information in a lookup file. In this case, the look-up information telling us what the parameter codes mean is in “parameterCdFile” which loads with the dataRetrieval package.\nSo, you could look at that and see what the parameters mean.\nOR We could have R do it and add a column that tells us what the parameters mean. Enter JOINS!\nJoins allow us to combine the data from two different data sets that have a column in common. At its most basic, a join looks for a matching row with the same key in both datasets (for example, a USGS gage number) and then combines the rows. So now you have all the data from both sets, matched on the key.\nBut you have to make some decisions: what if a key value exists in one set but not the other? Do you just drop that observation? Do you add an NA? Let’s look at the different options.\nTake for example the two data sets, FlowTable and SizeTable. The SiteName values are the key values and the MeanFlow and WSsize values are the data.\n\n\n\nJoin Setup\n\n\nNote River1 and River2 match up, but River3 and River5 only exist in one data set or the other.\nThe first way to deal with this is an INNER JOIN: inner_join() In an inner join, you only keep records that match. So the rows for River3 and River5 will be dropped because there is no corresponding data in the other set. See below:\n\n\n\nInner Join\n\n\nBut what if you don’t want to lose the values in one or the other or both?!\nFor instance, let’s say you have a bunch of discharge data for a stream, and then chemistry grab samples. You want to join the chemistry to the discharge based on the dates and times they were taken. But when you do this, you don’t want to delete all the discharge data where there is no chemistry! We need another option. Enter OUTER JOINS\nLEFT JOIN, left_join(): Preserves all values from the LEFT data set, and pastes on the matching ones from the right. This creates NAs where there is a value on the left but not the right. (this is what you’d want to do in the discharge - chemistry example above)\n\n\n\nLeft Join\n\n\nRIGHT JOIN, right_join(): Preserves all values from the RIGHT data set, and pastes on the matching ones from the left. This creates NAs where there is a value on the right but not the left.\n\n\n\nRight Join\n\n\nFULL JOIN, full_join(): KEEP EVERYTHING! The hoarder of the joins. No matching record on the left? create an NA on the right! No matching value on the right? Create an NA on the left! NAs for everyone!\n\n\n\nFull Join\n\n\nWhen you do this in R, you use the functions identified in the descriptions with the following syntax (see example below):\nif the column is named the same in both data sets &gt; xxx_join(left_tibble, right_tibble, by = “key_column”)**\nif the column is named differently in both data sets &gt; xxx_join(left_tibble, right_tibble, by = c(“left_key” = “right_key”)\n\n\n\nLeft Join Differing Col Names\n\n\nNote in both of the above, when you specify which column to use as “by” you have to put it in quotes."
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#join-example",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#join-example",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.4 Join example",
    "text": "6.4 Join example\nSo in the chunk below let’s get add information about the parameters in dataAvailable by joining it with the key file: parameterCdFile. The column with the parameter codes is called parm_cd in dataAvailable and parameter_cd in parameterCdFile\n\ndataAvailable &lt;- left_join(dataAvailable, parameterCdFile, \n                           by = c(\"parm_cd\" = \"parameter_cd\"))\n\ndataAvailable\n\n  agency_cd  site_no               station_nm site_tp_cd dec_lat_va dec_long_va\n1      USGS 03171000 NEW RIVER AT RADFORD, VA         ST   37.14179   -80.56922\n2      USGS 03171000 NEW RIVER AT RADFORD, VA         ST   37.14179   -80.56922\n3      USGS 03171000 NEW RIVER AT RADFORD, VA         ST   37.14179   -80.56922\n  coord_acy_cd dec_coord_datum_cd  alt_va alt_acy_va alt_datum_cd   huc_cd\n1            U              NAD83 1711.99       0.13       NAVD88 05050001\n2            U              NAD83 1711.99       0.13       NAVD88 05050001\n3            U              NAD83 1711.99       0.13       NAVD88 05050001\n  data_type_cd parm_cd stat_cd  ts_id loc_web_ds medium_grp_cd parm_grp_cd\n1           dv   00010   00003 241564         NA           wat        &lt;NA&gt;\n2           dv   00060   00003 145684         NA           wat        &lt;NA&gt;\n3           dv   00095   00003 145685         NA           wat        &lt;NA&gt;\n   srs_id access_cd begin_date   end_date count_nu parameter_group_nm\n1 1645597         0 2006-12-20 2009-03-18      704           Physical\n2 1645423         0 1907-10-01 2024-02-19    33744           Physical\n3 1646694         0 2006-12-20 2008-09-29      534           Physical\n                                                                                parameter_nm\n1                                                        Temperature, water, degrees Celsius\n2                                                           Discharge, cubic feet per second\n3 Specific conductance, water, unfiltered, microsiemens per centimeter at 25 degrees Celsius\n  casrn                  srsname parameter_units\n1             Temperature, water           deg C\n2       Stream flow, mean. daily           ft3/s\n3           Specific conductance      uS/cm @25C\n\n#that made a lot of columns, let's clean it up\ndataAvailClean &lt;- dataAvailable |&gt; select(site_no, \n                                           station_nm,\n                                           parm_cd, \n                                           srsname, \n                                           parameter_units,\n                                           begin_date, \n                                           end_date)\n\ndataAvailClean\n\n   site_no               station_nm parm_cd                  srsname\n1 03171000 NEW RIVER AT RADFORD, VA   00010       Temperature, water\n2 03171000 NEW RIVER AT RADFORD, VA   00060 Stream flow, mean. daily\n3 03171000 NEW RIVER AT RADFORD, VA   00095     Specific conductance\n  parameter_units begin_date   end_date\n1           deg C 2006-12-20 2009-03-18\n2           ft3/s 1907-10-01 2024-02-19\n3      uS/cm @25C 2006-12-20 2008-09-29"
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#finding-ids-to-download-usgs-data",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#finding-ids-to-download-usgs-data",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.5 Finding IDs to download USGS data",
    "text": "6.5 Finding IDs to download USGS data\nYou can find sites via map and just enter the id like we did in the chunks above: https://maps.waterdata.usgs.gov/mapper/index.html\nBelow we will look at two other ways to get sites: using a bounding box of a geographic region, or search terms like State and drainage area\n\n#find sites in a bounding box\n#coords of bottom left, top right\nswva &lt;- c(-81.36, 36.72, -80.27, 37.32)\n\n#get sites in this bounding box that have daily water temperature and discharge\nswva_sites &lt;- whatNWISsites(bBox = swva, \n                            parameterCd = c(\"00060\", \"00010\"), \n                            hasDataTypeCd = \"dv\")\n\nswva_sites\n\n   agency_cd  site_no                                      station_nm\n1       USGS 03473500             M F HOLSTON RIVER AT GROSECLOSE, VA\n2       USGS 03175140         WEST FORK COVE CREEK NEAR BLUEFIELD, VA\n3       USGS 03177710              BLUESTONE RIVER AT FALLS MILLS, VA\n4       USGS 03177700                BLUESTONE RIVER AT BLUEFIELD, VA\n5       USGS 03166000                  CRIPPLE CREEK NEAR IVANHOE, VA\n6       USGS 03164500                      NEW RIVER NEAR GRAYSON, VA\n7       USGS 03165500                        NEW RIVER AT IVANHOE, VA\n8       USGS 03166880  WEST SP AT NAT FISH HAT NEAR GRAHAMS FORGE, VA\n9       USGS 03166800                GLADE CREEK AT GRAHAMS FORGE, VA\n10      USGS 03166900 BOILING SP AT NAT FISH HAT NR GRAHAMS FORGE, VA\n11      USGS 03167000                 REED CREEK AT GRAHAMS FORGE, VA\n12      USGS 03175500                     WOLF CREEK NEAR NARROWS, VA\n13      USGS 03168500                       PEAK CREEK AT PULASKI, VA\n14      USGS 03168000                      NEW RIVER AT ALLISONIA, VA\n15      USGS 03167500        BIG REED ISLAND CREEK NEAR ALLISONIA, VA\n16      USGS 03172500              WALKER CREEK AT STAFFORDSVILLE, VA\n17      USGS 03173000                        WALKER CREEK AT BANE, VA\n18      USGS 03171500                      NEW RIVER AT EGGLESTON, VA\n19      USGS 03171000                        NEW RIVER AT RADFORD, VA\n20      USGS 03170000                 LITTLE RIVER AT GRAYSONTOWN, VA\n21      USGS 03169500             LITTLE RIVER NEAR COPPER VALLEY, VA\n   site_tp_cd dec_lat_va dec_long_va colocated           queryTime\n1          ST   36.88873   -81.34733     FALSE 2024-02-20 13:34:42\n2          ST   37.18428   -81.32982     FALSE 2024-02-20 13:34:42\n3          ST   37.27151   -81.30482     FALSE 2024-02-20 13:34:42\n4          ST   37.25595   -81.28177     FALSE 2024-02-20 13:34:42\n5          ST   36.85984   -80.98036     FALSE 2024-02-20 13:34:42\n6          ST   36.75985   -80.95619     FALSE 2024-02-20 13:34:42\n7          ST   36.83485   -80.95258     FALSE 2024-02-20 13:34:42\n8          SP   36.93429   -80.90313     FALSE 2024-02-20 13:34:42\n9          ST   36.93095   -80.90036     FALSE 2024-02-20 13:34:42\n10         SP   36.93068   -80.89619     FALSE 2024-02-20 13:34:42\n11         ST   36.93901   -80.88730     FALSE 2024-02-20 13:34:42\n12         ST   37.30568   -80.84980     FALSE 2024-02-20 13:34:42\n13         ST   37.04721   -80.78472     FALSE 2024-02-20 13:34:42\n14         ST   36.93762   -80.74563     FALSE 2024-02-20 13:34:42\n15         ST   36.88901   -80.72757     FALSE 2024-02-20 13:34:42\n16         ST   37.24179   -80.71090     FALSE 2024-02-20 13:34:42\n17         ST   37.26818   -80.70951     FALSE 2024-02-20 13:34:42\n18         ST   37.28957   -80.61673     FALSE 2024-02-20 13:34:42\n19         ST   37.14179   -80.56922     FALSE 2024-02-20 13:34:42\n20         ST   37.03763   -80.55672     FALSE 2024-02-20 13:34:42\n21         ST   36.99652   -80.52144     FALSE 2024-02-20 13:34:42\n\n#find sites with other criteria, VA, less than 20 sqmi, other criteria can be used..\n#check out the CRAN documentation\nsmallVA &lt;- readNWISdata(service = \"dv\",\n                           stateCd = \"VA\",\n                           parameterCd = \"00060\",\n                           drainAreaMax = \"20\",\n                           statCd = \"00003\")"
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#ok-lets-download-some-data",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#ok-lets-download-some-data",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.6 OK let’s download some data!",
    "text": "6.6 OK let’s download some data!\nWe are going to use readNWISdv(), which downloads daily values.\nWe will tell it which sites to download, which parameters to download, and then what time period to download.\nsiteNumber gets the sites we want to download, USGS site numbers, as a character. We will use the swva_sites data we generated (yep, you can download multiple sites at once!)\nstartDate and endDate get the…. start and end dates. IMPORTANT: These must be in YYY-MM-DD format, but you don’t have to tell R they are dates before you give them to the function, it’ll do that for you.\nparameterCd is the parameters you want to download. We want water temperature and discharge, which are “00060” and “00010”, respectively.\nOnce we have the data, the column names correspond to the keys that identify them, for example, discharge will be 00060 something something. Fortunately the dataRetrieval package also provides “renameNWISColumns()” which translates these into words, making them more easily understood by humans. We can pipe the results of our download to that function after we get the data to make the column names easier to understand.\n\nstart &lt;- \"2006-10-01\"\nend &lt;- \"2008-09-30\"\nparams &lt;- c(\"00010\", \"00060\")\n\nswva_dat &lt;- readNWISdv(siteNumber = swva_sites$site_no, \n                       parameterCd = params, \n                       startDate = start, \n                       endDate = end) |&gt; \n            renameNWISColumns()\n\nLet’s plot the water temperature data as a line and control the color of the lines with the different sites.\nWhat could be better about this plot?\n\nswva_dat |&gt; ggplot(aes(x = Date, y = Wtemp, color = site_no)) +\n  geom_line()\n\nWarning: Removed 2218 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWe can add site names with….More joins! Our swva_sites data has the names of the sites in human-friendly language. The column in the downloaded data and in the swva_sites data is called “site_no” so we just give that to the “by” argument. Perform a left join to add the names of the sites to the data.\nThen use select to remove some of the unnecessary columns.\nThen make the plot and then snazz it up with labels and a non-junky theme.\n\nswva_dat_clean &lt;- left_join(swva_dat, swva_sites, by = \"site_no\") |&gt;\n  select(station_nm, site_no, Date, Flow, Wtemp, dec_lat_va, dec_long_va)\n\nswva_dat_clean |&gt; ggplot(aes(x = Date, y = Wtemp, color = station_nm)) +\n  geom_line()+\n  ylab(\"Water temperature (deg C)\")+\n  xlab(element_blank())+\n  labs(color = \"Gage Site\")+\n  theme_classic()\n\nWarning: Removed 2218 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#pivoting-wide-and-long-data",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#pivoting-wide-and-long-data",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.7 Pivoting: wide and long data",
    "text": "6.7 Pivoting: wide and long data\nOkay, so with the data above: what would you do if you wanted to subtract the discharge or temperature of one gage from another on the same river: to compute rate of change between the two sites, for instance.\nYou could split them into two objects, then join based on date?\nOr…now hear me out… you could PIVOT them.\nA two-dimensional object can be either long or wide. Each has it’s advantages.\nLONG\nEach observation has it’s own row. In the first image below, the table on the right is long because each measurement has it’s own row. It’s value and name are identified by a other columns, and the values in that column repeat a lot.\nWIDE\nObservations of different things have their own columns. In the first image below, notice in the left hand table there are “Flow”, “Temp”, and “NO3” columns rather than an identifier in a separate column like in the table on the right.\nWhy?\nLong and wide data are more efficient for different things. Think about plotting a data set with 10 stream gages. If they are in a long format, you can just add color = Gage to your ggplot aes(). If they are in a wide format, meaning each gage has it’s own column, you’d have to write a new geom for EACH gage, because they’re all in separate columns.\nNow imagine you want to do some math to create new data: let’s say cases NO3 multiplied by Flow…. How would you even do that using the data on the right? With the wide data on the left it is simply mutate(NO3 * Flow).\nFinally, which table is easier to read in TABLE format (not a plot) in each of the two images below? Wide data is much more fitting for tables.\n\n\n\nPivoting to a longer format\n\n\n\n\n\nPivoting to a wider format\n\n\ndplyr, part of the tidyverse, has functions to convert data between wide and long data. I have to look up the syntax every single time I use them. But they are VERY useful."
  },
  {
    "objectID": "06-Get-Format-Plot-hydrodata_COMPLETE.html#pivot-examples",
    "href": "06-Get-Format-Plot-hydrodata_COMPLETE.html#pivot-examples",
    "title": "6  Joins, Pivots, and USGS dataRetrieval",
    "section": "6.8 Pivot Examples",
    "text": "6.8 Pivot Examples\nBack to our original question: I want to subtract the flow at Ivanhoe from the flow at Radford on the new river to see how much flow increases between the two sites through time.\nTo do this I am going to use pivot_wider() to give Ivanhoe and Radford discharges their own column.\nFirst, we will use select to trim the data to just what we need, then call pivot_wider telling it which data to use for the new column names (names_from = station_nm) and what values we want to pivot into the data under those columns (values_from = Flow).\nThen, subtract the two and make a plot!\n\n#Pivot so we can compute diffs between one river and others\n\nswva_wide &lt;- swva_dat_clean |&gt; select(station_nm, Flow, Date) |&gt;\n  pivot_wider(names_from = station_nm, values_from = Flow)\n\nswva_wide &lt;- swva_wide |&gt; \n  mutate(Radford_Ivanhoe = \n           `NEW RIVER AT RADFORD, VA` - `NEW RIVER AT IVANHOE, VA`)\n\nggplot(swva_wide, aes(x = Date, y = Radford_Ivanhoe))+\n  geom_line()+\n  ggtitle(\"Change in flow from Ivanhoe to Radford\")+\n  theme_classic()\n\n\n\n\nTo further illustrate how to move between long and wide data and when to use them, let’s grab some water quality data. This process will also review some of the other concepts from this topic.\nIn the chunk below we will look to see what sites have data for nitrate and chloride in our swva bounding box from above. We will then filter them to just stream sites (leave out groundwater and springs). And finally we will download the nitrate and chloride data for those sites.\nNote that the readWQPqw() function requires the site number to have a leading “USGS-” so we will add that using the paste0() function in a new column using a mutate. You could also use paste() with sep = ’’. We will also use the resulting column when we join site information with the downloaded data in the next step.\n\n#Nitrate as nitrate and chloride\nparams &lt;- c(\"00940\", \"71851\")\n\n#what sites in our bounding box have cloride and nitrate\nswva_chem_sites &lt;- whatNWISsites(bBox = swva, \n                            parameterCd = params)\n\n#filter to just stream water and\n#prep to send to readWQPqw by adding USGS- to site_no\nswva_chem_sites &lt;- filter(swva_chem_sites, site_tp_cd == \"ST\") |&gt;\n                   mutate(site_no_USGS = paste0(\"USGS-\", site_no))\n\nwqdat &lt;- readWQPqw(siteNumber = swva_chem_sites$site_no_USGS, \n                    parameterCd = params)\n\nNow, let’s clean things up a bit.\nJoin the parameter names from parameterCdFile and then join the site names from swva_chem_site. Then select just the columns we want, and finally filter the remaining data to just look at sites from the New River.\nTo illustrate the functionality of the data in this format, plot Chloride for each site, and then plot Chloride AND Nitrate, using the parameter name in facet_wrap.\n\nwqdat_clean &lt;- wqdat |&gt; \n  left_join(swva_chem_sites, \n            by = c(\"MonitoringLocationIdentifier\" = \"site_no_USGS\")) |&gt;\n  select(MonitoringLocationIdentifier, ActivityStartDate,\n         CharacteristicName, ResultMeasure.MeasureUnitCode, \n         station_nm, ResultMeasureValue) |&gt;\n  filter(str_detect(station_nm, \"NEW RIVER\"))\n  \nwqdat_clean |&gt; filter(CharacteristicName == \"Chloride\") |&gt;\n  ggplot(aes(x = ActivityStartDate, y = ResultMeasureValue, color = station_nm)) +\n  geom_point()+\n  ylab(\"Chloride (mg/L)\")+\n  xlab(element_blank())+\n  labs(color = \"Site\")+\n  theme_classic()\n\n\n\nwqdat_clean |&gt;\n  ggplot(aes(x = ActivityStartDate, y = ResultMeasureValue, color = station_nm)) +\n  geom_point()+\n  facet_wrap(facets = \"CharacteristicName\", nrow = 2)+\n  ylab(\"Concentration (mg/L)\")+\n  xlab(element_blank())+\n  labs(color = \"Site\")+\n  theme_classic()\n\n\n\n\nNow let’s say we want to calculate something with chloride and nitrate. We need to make the data wide so we have a nitrate column and a chloride column. Do that below. What goes into values_from? what goes into names_from?\nNext, plot Chloride and Nitrate added together (Chloride + Nitrate). Could you do this with the data in the previous format?\nFinally, use pivot_longer to transform the data back into a long format. Often you’ll get data in a wide format and need to convert it to long, and we haven’t tried that yet. The only argument you’ll need to pass to pivot_longer() in this case is to tell it what columns to turn into the new DATA column (using the cols = ) parameter.\n\n#make wqdat_clean wide\n\nwqdat_wide &lt;- wqdat_clean |&gt; select(-ResultMeasure.MeasureUnitCode) |&gt;\n  pivot_wider(values_from = ResultMeasureValue, names_from = CharacteristicName)\n\nggplot(wqdat_wide, aes(x = ActivityStartDate, y = Chloride + Nitrate)) +\n  geom_point()+\n  theme_classic()\n\nWarning: Removed 104 rows containing missing values (`geom_point()`).\n\n\n\n\nwqlonger &lt;- wqdat_wide |&gt;\n  pivot_longer(cols = c(\"Chloride\", \"Nitrate\"))"
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#load-the-tidyverse-dataretrieval-and-patchwork-packages.",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#load-the-tidyverse-dataretrieval-and-patchwork-packages.",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.1 Load the tidyverse, dataRetrieval, and patchwork packages.",
    "text": "7.1 Load the tidyverse, dataRetrieval, and patchwork packages.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dataRetrieval)\nlibrary(patchwork)"
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-1",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-1",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.2 Problem 1",
    "text": "7.2 Problem 1\nUsing readWQPqw(), read all the chloride (00940) data for the New River at Radford (03171000, must add USGS- to gage id). Use the head() function to print the beginning of the output from this function."
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-2",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-2",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.3 Problem 2",
    "text": "7.3 Problem 2\nUsing the readNWISdv (daily values) function, download discharge (00060), temperature (00010), and specific conductivity (00095) for the New River at Radford from 2007 to 2009 (regular year). Use renameNWIScolumns() to rename the output of the download. Use head() to show the beginning of the results of your download."
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-3",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-3",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.4 Problem 3",
    "text": "7.4 Problem 3\nJoin the data from Problem 1 and Problem 2 to add the chloride data to the daily discharge, temp, and conductivity data. hint: you will join on the date. Preview your data below the chunk using head()."
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-4",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-4",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.5 Problem 4",
    "text": "7.5 Problem 4\nUsing the joined data, create a line plot of Date (x) and Flow (y). Create a scatter plot of Date (x) and chloride concentration (y). Put the graphs on top of each other using the patchwork library."
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-5",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-5",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.6 Problem 5",
    "text": "7.6 Problem 5\nCreate a scatter plot of Specific Conductivity (y) and Chloride (x). Challenge: what could you do to get rid of the warning this plot generates about NAs."
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-6",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-6",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.7 Problem 6",
    "text": "7.7 Problem 6\nRead in the GG chem subset data and plot Mg_E1 (x) vs Ca_E1 (y) as points."
  },
  {
    "objectID": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-7",
    "href": "07-Activity-Joins-Pivots_dataRetrieval.html#problem-7",
    "title": "7  ACTIVITY: Joins Pivots dataRetrieval",
    "section": "7.8 Problem 7",
    "text": "7.8 Problem 7\nWe want to look at concentrations of each element in the #6 dataset along the stream (Distance), which is difficult in the current format. Pivot the data into a long format, the data from Ca, Mg, and Na _E1 columns should be pivoted. Make line plots of each element where y is the concentration and x is distance. Use facet_wrap() to create a separate plot for each element and use the “scales” argument of facet_wrap to allow each plot to have different y limits."
  },
  {
    "objectID": "08-Summative_Activity.html#problem-1",
    "href": "08-Summative_Activity.html#problem-1",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.1 Problem 1",
    "text": "8.1 Problem 1\nLoad the tidyverse, lubridate, patchwork, and dataRetrieval packages."
  },
  {
    "objectID": "08-Summative_Activity.html#problem-2",
    "href": "08-Summative_Activity.html#problem-2",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.2 Problem 2",
    "text": "8.2 Problem 2\nRead in the McDonald Hollow dataset in the project folder.\n\n8.2.1 What are the data types of the first three columns?\n\n\n8.2.2 How long is the data (number of rows)?\n\n\n8.2.3 What is the name of the last column?"
  },
  {
    "objectID": "08-Summative_Activity.html#problem-3",
    "href": "08-Summative_Activity.html#problem-3",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.3 Problem 3",
    "text": "8.3 Problem 3\nPlot the stage of the stream (Stage_m_pt) on the y axis as a line and the date on the x. These stage data are in meters, convert them to centimeters for the plot.\nFor all plots in this test, label axes properly and use a theme other than the default."
  },
  {
    "objectID": "08-Summative_Activity.html#problem-4",
    "href": "08-Summative_Activity.html#problem-4",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.4 Problem 4",
    "text": "8.4 Problem 4\nWe want to look at the big event that happens from November 11, 2020 to November 27, 2020. Filter the dataset down to this time frame and save it separately. Make a plot with the same setup as in #3 with these newly saved data.\nWe also want to see how specific conductivity changes during the event. Create a second plot of specific conductivity (SpC_mScm) for the same time range.\nStack these two plots on top of each other using the patchwork package."
  },
  {
    "objectID": "08-Summative_Activity.html#problem-5",
    "href": "08-Summative_Activity.html#problem-5",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.5 Problem 5",
    "text": "8.5 Problem 5\nFor the same storm, we are curious about how conductivity changes with the stream level. To do this, make a scatter plot that shows Stage on the x axis and specific conductivity (SpC_mScm) on the y. (units: mScm) Color the points on the plot using the datetime column.\n\n8.5.1 Use the plot to describe how specific conductivity changes with stream stage throughout the storm. (not based on hydrologic processes, just describe the pattern in values on the plot)"
  },
  {
    "objectID": "08-Summative_Activity.html#problem-6",
    "href": "08-Summative_Activity.html#problem-6",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.6 Problem 6",
    "text": "8.6 Problem 6\nContinuing to look at the storm, as an exploratory data analysis, we want to create a plot that shows all the parameters measured. To do this, pivot the STORM EVENT data so there is a column that has the values for all the parameters measured as individual rows, along with another column that identifies the type of measurement. Then use facet_wrap with the “name” column (or whatever you call it) as the facet. Be sure to set the parameters of facet_wrap such that the y axes are all allowed to be different ranges.\nWe need to get rid of a few parameters that aren’t helpful though, so remove “Press_cmH2O_pt”, “PPO2”, and “TSSmgL”\nEX:\nDate Value Name\n10/1/20 12 Stage\n10/1/20 6 Temp\n…."
  },
  {
    "objectID": "08-Summative_Activity.html#problem-7",
    "href": "08-Summative_Activity.html#problem-7",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.7 Problem 7",
    "text": "8.7 Problem 7\nWe want to create a table that clearly shows the differences in water temperature for the three months at the two locations (flow and pool) in the FULL data set (not the storm subset). To do this: Create a new column in the full dataset called “month” and set it equal to the month of the datetime column using the month() function. Then group your dataset by month and summarize temperature at each location by mean. Save these results to a new object and output it so it appears below your chunk when you knit. Be sure the object has descriptive column names.\nYou can do this all in one statement using pipes."
  },
  {
    "objectID": "08-Summative_Activity.html#problem-8",
    "href": "08-Summative_Activity.html#problem-8",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.8 Problem 8",
    "text": "8.8 Problem 8\nPlot the distribution of the flow temperature and show as vertical lines on the plot the mean, median, and IQR. Be careful about how you show IQR. Look at the definition and then think about how you would put it on the plot. Describe in the text above the chunk what color is what statistic in the plot.\n\n8.8.1 Using the shape of the distribution and the measures you plotted, explain why you think the distribution is normal or not. What statistical test could you perform to see if it is normal?"
  },
  {
    "objectID": "08-Summative_Activity.html#problem-9",
    "href": "08-Summative_Activity.html#problem-9",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.9 Problem 9",
    "text": "8.9 Problem 9\nIn this question we will get and format data for three USGS gages.\nGages: 03177710, 03173000, 03177480\nDischarge in cubic feet per second (cfs) code: 00060\n\nRead and save the gage information for the three gages using readNWISsite().\nUse the readNWISdv() function to read and save the daily discharge values for the following three gages for the 2020 water year (10-01-2019 to 9-30-2020). And then use the renameNWIScolumns() function to make the names human-friendly.\nJoin the gage site information from (a) to the data from (b) so you can reference the gages by their names.\n\nOutput a preview of your data below the code chunk using head()"
  },
  {
    "objectID": "08-Summative_Activity.html#problem-10",
    "href": "08-Summative_Activity.html#problem-10",
    "title": "8  ACTIVITY Summative 1",
    "section": "8.10 Problem 10",
    "text": "8.10 Problem 10\nUsing the data from #9, Plot flow on the y axis and date on the x axis, showing the data as a line, and coloring by gage name."
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#get-data",
    "href": "09-Flow_Duration_Curves.html#get-data",
    "title": "9  Flow Duration Curves",
    "section": "9.1 Get data",
    "text": "9.1 Get data\nTo start, let’s grab the USGS discharge data for the gage in Linville NC from 1960 to 2020.\nWe will download the data using USGS dataRetrieval and look at a line plot.\n\nsiteno &lt;- \"02138500\" #Linville NC\nstartDate &lt;- \"1960-01-01\"\nendDate &lt;- \"2020-01-01\"\nparameter &lt;- \"00060\"\n\nQdat &lt;- readNWISdv(siteno, parameter, startDate, endDate) |&gt; \n  renameNWISColumns()\n\n#Look at the data\nQdat |&gt; ggplot(aes(x = Date, y = Flow))+\n  geom_line()"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#review-describe-the-distribution",
    "href": "09-Flow_Duration_Curves.html#review-describe-the-distribution",
    "title": "9  Flow Duration Curves",
    "section": "9.2 Review: describe the distribution",
    "text": "9.2 Review: describe the distribution\nMake a plot to view the distribution of the discharge data.\n\nWhat is the median flow value?\nWhat does this tell us about flow at that river?\nHow often is the river at or below that value?\nCould you pick that number off the plot?\nWhat about the flow the river is at or above only 5% of the time?\n\n\nQdat |&gt; ggplot(aes(Flow))+\n  stat_density()+\n  scale_x_log10()+\n  geom_vline(xintercept = median(Qdat$Flow), color = \"red\")"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#ecdfs",
    "href": "09-Flow_Duration_Curves.html#ecdfs",
    "title": "9  Flow Duration Curves",
    "section": "9.3 ECDFs",
    "text": "9.3 ECDFs\nLet’s look at an Empirical Cumulative Density Function (ECDF) of the data.\nLook at this carefully, what does it show? How is it different from the pdf of the data?\nPlot the median again. Without the line on the plot, how would you tell where the median is?\nGiven your answer to the question above, can you determine the flow the river is at or above only 25% of the time? Think carefully about what the y axis of the ECDF means.\n\nQdat |&gt; ggplot(aes(Flow))+\n  stat_ecdf()+\n  scale_x_log10()+\n  geom_vline(xintercept = median(Qdat$Flow), color = \"red\")+\n  geom_vline(xintercept = quantile(Qdat$Flow)[4], color = \"blue\")"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#calculate-flow-exceedence-probabilities",
    "href": "09-Flow_Duration_Curves.html#calculate-flow-exceedence-probabilities",
    "title": "9  Flow Duration Curves",
    "section": "9.4 Calculate flow exceedence probabilities",
    "text": "9.4 Calculate flow exceedence probabilities\nIn hydrology, it is common to look at a similar representation of flow distributions, but with flow on the Y axis and “% time flow is equaled or exceeded” on the X axis. There are a number of ways we could make this plot: for example we could transform the axes of the plot above or we could use the function that results from the ECDF function in R to calculate exceedence probabilities at flow throughout our range of flows. But for our purposes, we are just going to calculate it manually.\nWe are going to calculate our own exceedence probabilities because knowing how to do this will hopefully help us understand what a flow duration curve is AND we will need to do similar things in our high and low flow analyses.\nThe formula for exceedence probability (P) is below. What do we need to calculate this?\nExceedence probability (P), Probability a flow is equaled or exceeded\n\\(P = 100 * [M / (n + 1)]\\)\nM = Ranked position of the flow n = total number of observations in data record\nHere’s a description of what we will do: &gt; Pass our Qdat data to mutate and create a new column that is equal to the ranks of the discharge column. &gt; Then pass that result to mutate again and create another column equal exceedence probability (P) * 100, which will give us %.\n\n#Flow is negative in rank() to make \n#high flows ranked low (#1)\nQdat &lt;- Qdat |&gt;\n  mutate(rank = rank(-Flow)) |&gt;\n  mutate(P = 100 * (rank / (length(Flow) + 1)))"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#plot-a-flow-duration-curve-using-the-probabilities",
    "href": "09-Flow_Duration_Curves.html#plot-a-flow-duration-curve-using-the-probabilities",
    "title": "9  Flow Duration Curves",
    "section": "9.5 Plot a Flow Duration Curve using the probabilities",
    "text": "9.5 Plot a Flow Duration Curve using the probabilities\nNow construct the following plot: A line with P on the x axis and flow on the y axis. Name the x axis “% Time flow equaled or exceeded” and log the y axis.\nThat’s a flow duration curve!\nQuestions about the flow duration curve: * How often is a flow of 100 cfs exceeded at this gage? * Is flow more variable for flows exceeded 0-25% or of the time or 75-100% * of the time? * How can you tell? * These data are daily observations. Given that, what is a more accurate name for the x axis? * What would the X axis be called if we were using maximum yearly data?\n\nQdat |&gt; ggplot(aes(x = P, y = Flow))+\n  geom_line()+\n  scale_y_log10()+\n  xlab(\"% Time flow equalled or exceeded\")+\n  ylab(\"Q (cfs)\")"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#make-an-almost-fdc-with-stat_ecdf",
    "href": "09-Flow_Duration_Curves.html#make-an-almost-fdc-with-stat_ecdf",
    "title": "9  Flow Duration Curves",
    "section": "9.6 Make an almost FDC with stat_ecdf",
    "text": "9.6 Make an almost FDC with stat_ecdf\nBelow is an example of making a very similar plot with the stat_ecdf() geometry in ggplot. Notice how similar the result is to the one we calculated manually.\nTo make the plot similar, we will reverse the y axis of the ecdf plot with scale_y_reverse and flip the axes (change the x to y and the y to x) with coord_flip()\n\nQdat |&gt; ggplot(aes(Flow))+\n  stat_ecdf()+\n  scale_x_log10()+\n  scale_y_reverse()+\n  coord_flip()+\n  xlab(\"Q (cfs)\")+\n  ylab(\"Probability flow is not exceeded\")"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#example-use-of-an-fdc",
    "href": "09-Flow_Duration_Curves.html#example-use-of-an-fdc",
    "title": "9  Flow Duration Curves",
    "section": "9.7 Example use of an FDC",
    "text": "9.7 Example use of an FDC\nLet’s explore one potential use of flow duration curves: examining the differences between two sets of flow data.\nFrom the line plot of the discharge, it looked like the flow regime may have shifted a bit in the data between the early years and newer data. Let’s use flow duration curves to examine potential differences. We can come up with groups and then use group_by to run the analysis by groups instead of the whole dataset.\nWe are introducing a new function here called case_when(). This allows you to assign values to a new column based on values in another column. In our case, we are going to name different time periods in our data.\nWe will then group the data by these periods and calculate exceedence probabilities for each. The procedure works the same, except we add a group_by statement to group by our time period column before we create the rank and P columns. Then, when we plot, we can just tell ggplot to create different colored lines based on the time period names and it will plot a separate flow duration curve for each. Tidyverse FOR THE WIN!\nDescribe the differences in flow regime you see between the three periods of 1960-1980, 1980-2000, and 2000-2020.\n\nQdat &lt;- Qdat |&gt;\n  mutate(year = year(Date)) |&gt;\n  mutate(period = case_when( year &lt;= 1980 ~ \"1960-1980\",\n                             year &gt; 1980 & year &lt;= 2000 ~ \"1980-2000\",\n                             year &gt; 2000 ~ \"2000-2020\"))\n\nQdat &lt;- Qdat |&gt;\n  group_by(period) |&gt;\n  mutate(rank = rank(-Flow)) |&gt; \n  mutate(P = 100 * (rank / (length(Flow) + 1)))\n\nQdat |&gt; ggplot(aes(x = P, y = Flow, color = period))+\n  geom_line()+\n  scale_y_log10()+\n  xlab(\"% Time flow equalled or exceeded\")+\n  ylab(\"Q (cfs)\")"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#compare-to-a-boxplot-of-the-same-data",
    "href": "09-Flow_Duration_Curves.html#compare-to-a-boxplot-of-the-same-data",
    "title": "9  Flow Duration Curves",
    "section": "9.8 Compare to a boxplot of the same data",
    "text": "9.8 Compare to a boxplot of the same data\nWe are really just looking at the data distribution here. Remember another good way to compare distributions is a boxplot. Let’s create a boxplot showing flows from these time periods. (we will also mess with the dimensions of the plot so the boxes aren’t so wide using fig.width and fig.height in the ``` header above the code chunk)\nWhat are the advantages/disadvantages of the flow duration curves vs. boxplots?\n\nQdat |&gt; ggplot(aes(x = period, y = Flow)) +\n  geom_boxplot()+\n  scale_y_log10()"
  },
  {
    "objectID": "09-Flow_Duration_Curves.html#challenge-examining-flow-regime-change-at-the-grand-canyon",
    "href": "09-Flow_Duration_Curves.html#challenge-examining-flow-regime-change-at-the-grand-canyon",
    "title": "9  Flow Duration Curves",
    "section": "9.9 Challenge: Examining flow regime change at the Grand Canyon",
    "text": "9.9 Challenge: Examining flow regime change at the Grand Canyon\nThe USGS Gage “Colorado River at Yuma, AZ” is below the Hoover dam. The Hoover Dam closed in 1936, changing the flow of the Colorado River below. Load average daily discharge data from 10-01-1905 to 10-01-1965 from the Yuma gage. Use a line plot of discharge and flow duration curves to examine the differences in discharge for the periods: 1905 - 1936, 1937 - 1965.\nHow does the FDC show the differences you observed in the line plot?\n\nsiteid &lt;- \"09521000\"\nstartDate &lt;- \"1905-10-01\"\nendDate &lt;- \"1965-10-01\"\nparameter &lt;- \"00060\"\n\nWS &lt;- readNWISdv(siteid, parameter, startDate, endDate) |&gt; \n  renameNWISColumns() |&gt;\n  mutate(year = year(Date)) |&gt;\n  mutate(period = case_when( year &lt;= 1936 ~ \"Pre Dam\",\n                             year &gt; 1936  ~ \"Post Dam\")) |&gt;\n  group_by(period) |&gt;\n  mutate(rank = rank(-Flow)) |&gt; \n  mutate(P = 100 * (rank / (length(Flow) + 1)))\n\nflow &lt;- ggplot(WS, aes(Date, Flow, color = period))+\n  geom_line()+\n  ylab(\"Q (cfs)\")\n\nfdc &lt;- WS |&gt; ggplot(aes(x = P, y = Flow, color = period))+\n  geom_line()+\n  #scale_y_log10()+\n  xlab(\"% Time flow equalled or exceeded\")+\n  ylab(\"Q (cfs)\")\n\nflow / (fdc + plot_spacer())\n\n\n\n\nThat’s it! Next we will apply some of these principles to look at low-flow statistics."
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#what-are-low-flow-statistics",
    "href": "10-LowFlowStats-FDC.html#what-are-low-flow-statistics",
    "title": "10  Low Flow Analysis",
    "section": "10.1 What are low flow statistics?",
    "text": "10.1 What are low flow statistics?\nLow flow design flows can be specified based on hydrological or biological data. Biological methods look more at water quality standards relevant to biota. The hydrologic method just looks at the statistical distribution of low flows over a period of time.\n\nJust from this simple definition, can you think of a management situation where it would make sense to use the biological method? the hydrologic method? What are the advantages to each?\n\nWe will focus on hydrologic methods. What a surprise! You will most frequently see low flow stats in the format of xQy. So for example 7Q10 or 1Q10 are common design flows. Let’s look at the EPA definition of these and then break them down.\n“The 1Q10 and 7Q10 are both hydrologically based design flows. The 1Q10 is the lowest 1-day average flow that occurs (on average) once every 10 years. The 7Q10 is the lowest 7-day average flow that occurs (on average) once every 10 years.” -EPA https://www.epa.gov/ceam/definition-and-characteristics-low-flows#1Q10\nSo the first number, the 7 in 7Q10 is how many days we will average flow over to calculate the statistic. Why does does this matter? Why not always use a 1 day flow record?\nThen the second number is the return-interval of the flow, or the probability that a flow of that magnitude or lower will occur any given year. The 10 in 7Q10 means there is a 10 percent chance that the associated 7-day average flow or below will occur in any given year. Another way of saying this is that a flow of that magnitude or below occurs on average once every 10 years. However expressing it this way can be dangerous, especially with the opposite type of extreme flows: Floods. Why do you think it could be dangerous to say a flow of this magnitude or below will happen on average once every 10 years?\nSo, to calculate a 7Q10 we need: * 7-day mean-daily flows * The minimum value per year of those 7-day mean-daily flows * The return intervals of those minimum yearly flows\nBecause a 7Q10 flow means * There is a 10% chance (return interval = 10) that a river will have a average weekly flow of that level or below in a given year."
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#get-data",
    "href": "10-LowFlowStats-FDC.html#get-data",
    "title": "10  Low Flow Analysis",
    "section": "10.2 Get data",
    "text": "10.2 Get data\nLet’s get started on an example. We will calculate the 7Q10 low flow statistic for the Linville NC usgs gage (02138500) using daily discharge data from 1922-1984. (parameter = 00060)\n\nsiteno &lt;- \"02138500\"\nstartDate &lt;- \"1922-01-01\"\nendDate &lt;- \"1984-01-01\"\nparameter &lt;- \"00060\"\n\nQdat &lt;- readNWISdv(siteno, parameter, startDate, endDate) |&gt; \n  renameNWISColumns()"
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#create-the-x-days-average-flow-record",
    "href": "10-LowFlowStats-FDC.html#create-the-x-days-average-flow-record",
    "title": "10  Low Flow Analysis",
    "section": "10.3 Create the X days average flow record",
    "text": "10.3 Create the X days average flow record\nRemember the 7 in 7Q10 means we are looking at the 7-day average flow. We just have daily values from the USGS gage, so we need to create this data record.\nTo do this we will calculate a rolling average, also called a moving-window average. This just means you grab the first 7 days, average them, then move the window of the days you are averaging forward a day, and average again… all the way through the record.\nFor your rolling mean you can have the window look forward, backward, or forward and backward. For example, a forward window takes the average of X number of records and places the value at the beginning. Backward places that value at the end, and both would put it in the middle. In the function we will use to do this, forward is a left align, backward is right align, and both is centered.\nFor example\n\ndata window = 1, 2, 3, 4, 5 (lots of values before and after this)\n\n\nmean = 3\n\n\nforward window/left align: 3, NA, NA, NA, NA\n\n\nbackward window/right align: NA, NA, NA, NA, 3\n\n\nboth/center align: NA, NA, 3, NA, NA\n\nWe could certainly set up some code to calculate this, but there is a nice and fast function in the zoo package for calculating rolling means.\nAs we write the code to do this analysis, we are going to keep in mind that we may want to calculate a different type of low flow, like a 1Q10, so we are going to store the x and y of the xQy low flow statistic as objects rather than including them several places in the code. That way we can just change them in one place and run the analysis to compute a different statistic.\n\n#set x and y for xQy design flow\nXday &lt;- 7\nYrecInt &lt;- 10\n\n#X day rolling mean, don't fill the ends of the timeseries,\n#don't ignore NAs, use a backward-looking window (right align)\nQdat &lt;- Qdat |&gt; mutate(xdaymean = rollmean(Flow, \n                                            Xday, \n                                            fill = NA, \n                                            na.rm = F, \n                                            align = \"right\"))"
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#look-at-what-a-rolling-mean-does.",
    "href": "10-LowFlowStats-FDC.html#look-at-what-a-rolling-mean-does.",
    "title": "10  Low Flow Analysis",
    "section": "10.4 Look at what a rolling mean does.",
    "text": "10.4 Look at what a rolling mean does.\nWe just added a new column with the rolling mean, so let’s plot it and see what it did to the discharge record.\nLet’s look at June-August 1960. You can’t see too well what is going on in the full record.\n\nQdat |&gt; \n  filter(Date &gt; mdy(\"06-01-1960\") & Date &lt; mdy(\"08-01-1960\")) |&gt;\n  ggplot(aes(Date, Flow, color = \"daily\"))+\n    geom_line()+\n    geom_line(aes(x = Date, y = xdaymean, color = \"rolling mean\"))"
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#calculate-yearly-minimums",
    "href": "10-LowFlowStats-FDC.html#calculate-yearly-minimums",
    "title": "10  Low Flow Analysis",
    "section": "10.5 Calculate yearly minimums",
    "text": "10.5 Calculate yearly minimums\nOkay, we have our X-day rolling mean. Now we need to calculate the probability that a given magnitude flow or below will happen in a given year. Because we are concerned with a given year we need the lowest flow per year.\nWe will calculate minimum flow per year by creating a Year column, grouping by that column, and using the summarize function to calculate the minimum flow per year. The code we are going to write will also drop any years that are missing too much data by dropping years missing 10% or more days.\n\n#missing less than 10% of each year and 10% or fewer NAs\nQyearlyMins &lt;- Qdat |&gt; mutate(year = year(Date)) |&gt;\n                        group_by(year) |&gt;\n                        summarize(minQ = min(xdaymean, na.rm = T), \n                                  lenDat = length(Flow),\n                                  lenNAs = sum(is.na(xdaymean))) |&gt;\n                        filter(lenDat &gt; 328 & lenNAs / lenDat &lt; 0.1)"
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#calculate-return-interval",
    "href": "10-LowFlowStats-FDC.html#calculate-return-interval",
    "title": "10  Low Flow Analysis",
    "section": "10.6 Calculate return interval",
    "text": "10.6 Calculate return interval\nNow that we have an object that contains our yearly minimum flows, we can calculate the return interval as\n\\(Return Interval = (n + 1) / rank\\)\nWhere n is the number of records in the data (number of years) and rank is the rank of each year’s low flow (lowest flow = rank 1 and so on). We can calculate the rank with the rank() function in base R. In the rank function we will specify that in the case of a tie, the first value gets the lower rank using ties.method = “first”.\nWe can then transform that to an exceedence probability as\n\\(Exceedence Probability = 1 / Return Interval\\)\nOnce we calculate the return interval and exceedence probability we will plot the return interval against the minimum discharge.\n\n# add rank column and return interval column\nQyearlyMins &lt;- QyearlyMins |&gt; \n                mutate(rank = rank(minQ, ties.method = \"first\")) |&gt;\n                mutate(ReturnInterval = (length(rank) + 1)/rank) |&gt;\n                mutate(ExceedProb = 1 / ReturnInterval)\n      \nggplot(QyearlyMins, aes(x = ReturnInterval, y = minQ))+\n  geom_point()\n\n\n\n\nChallenge question How is this similar to a flow duration curve? Could you make a “flow duration curve” from these data? What would it tell you?\n\nggplot(QyearlyMins, aes(x = ExceedProb, y = minQ))+\n  geom_point()"
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#fit-to-pearson-type-ii-distribution",
    "href": "10-LowFlowStats-FDC.html#fit-to-pearson-type-ii-distribution",
    "title": "10  Low Flow Analysis",
    "section": "10.7 Fit to Pearson Type II distribution",
    "text": "10.7 Fit to Pearson Type II distribution\nSource for these calculations: https://water.usgs.gov/osw/bulletin17b/dl_flow.pdf\nWe now have everything we need to calculate what the 10-year return interval flow is (the 0.1 probability flow). To do this, we have to fit a distribution to our data and then use that fitted distribution to predict the value of the 10-year return interval flow.\nThis may sound a little complex, but let’s think about it this way:\n\nYou have some data, let’s say: heights of students at Virginia Tech\nYou did some tests on it and know it is a normal distribution\nIf you measure the mean and standard deviation of that distribution, you could create a “fitted” representation of your distrubtion by generating a normal distribution with the same mean and standard deviation with the rnorm() function.\nNow you could plot that fitted, synthetic distribution as an ECDF and read the plot to determine, say, 10% of students (0.1 probability) are at or above what height?\n\nAssume the average height from your data was 65 inches and the standard deviation was 6 inches (this is 100% made up), let’s look at it.\n\nfitteddistribution &lt;- rnorm(1000, mean = 65, sd = 6) |&gt;\n  as_tibble()\n\nggplot(fitteddistribution, aes(x = value))+\n  stat_ecdf()+\n  xlab(\"height (in)\")+\n  ylab(\"probability\")\n\n\n\n\nTo get our 10 year return period (0.1 exceedence probability) we are going to do the same thing, except we know the distribution of the data isn’t normal, so we have to use a different distribution.\nThere are a bunch of “extreme value” distributions used in these type of analyses. When we talk about floods we will use the Gumbel distribution, for example. For this type of analysis, it is common to use the Pearson Type III distribution.\nWhen we used the normal distribution example, we let R produce the distribution that fit our data. In this case we will use the equation that describes the Pearson Type III distribution. To predict flow at a given recurrence interval we will need the mean of the logged discharges (Xbar), the frequency factor (K), the standard deviation of the log discharges (S), skewness (g), and the standard normal variate (z). We will first compute this for all of the values in our data set to see how the fitted values fit our calculated values.\nPearson Type III\n\\(Flow = exp(Xbar + KS)\\)\nwhere:\nXbar = mean of the log discharge you are investigating\nK = frequency factor\nS = standard deviation of log discharges\nFrequency Factor\n\\(K = (2 / g) * ((1 +( g * z)) / 6 - ((g ^ 2) / 36)) ^ 3 - 1)\\)\nSkewness\ng = skewness() from moments package\nStandard normal variate\n\\(z = 4.91 * ((1 / y) ^ {0.14} - (1 - (1 / y)) ^ {0.14})\\)\ny = recurrence interval\n\n#Measures of the distribution\nXbar &lt;- mean(log(QyearlyMins$minQ))\nS    &lt;- sd(log(QyearlyMins$minQ))\ng    &lt;- skewness(log(QyearlyMins$minQ))\n \n#calculate z, K, to plot the fitted Pearson Type III\nQyearlyMins &lt;- QyearlyMins |&gt; \n  mutate(z = 4.91 * ((1 / ReturnInterval) ^ 0.14 - (1 - 1 / ReturnInterval) ^ 0.14)) |&gt;\n  mutate(K = (2 / g) * (((1 + (g * z) / 6 - (g ^ 2) / 36) ^ 3) - 1) ) |&gt;\n  mutate(Qfit = exp(Xbar + K * S))\n\nLet’s look our results and see how they fit. Plot the return interval on the x axis and flow on the y. Plot minQ, the minimum Q data, and Qfit, the data from the the model fit.\n\nQyearlyMins |&gt; \n  ggplot(aes(x = ReturnInterval, y = minQ, color = \"Estimated\"))+\n  geom_point()+\n  geom_line(aes(x = ReturnInterval, y = Qfit, color = \"Fitted\"))+\n  theme_classic()+\n  scale_x_log10()+\n  ylab(\"X day yearly minimum\")+\n  xlab(\"Return Interval\")\n\n\n\n\nAbove we calculated z, K and the flow for each return interval in our data record to see how the distribution fit our data. We can see it fits quite well.\nWe can use the same calculations as we used on the entire record to calculate a specific return period of interest. In our case, the 10 year return period for the 7Q10.\nWe will set y equal to YrecInt, which we set above. This way we can just change it at the top of the code to run whatever xQy metric we want.\n\n#xQy ei: 7Q10\ny &lt;- YrecInt\n\n#Find these values based on established relationships\nz    &lt;- 4.91 * ((1 / y) ^ 0.14 - (1 - 1 / y) ^ 0.14)\nK    &lt;- (2 / g) * (((1 + (g * z) / 6 - (g ^ 2) / 36) ^ 3) - 1) \n\nPearsonxQy &lt;- exp(Xbar + K * S)\n\nSo, our 7Q10 flow in cfs for this gage is….\n\n#Low flow stat (7Q10 in exercise)\nPearsonxQy\n\n[1] 16.70488"
  },
  {
    "objectID": "10-LowFlowStats-FDC.html#distribution-free-method",
    "href": "10-LowFlowStats-FDC.html#distribution-free-method",
    "title": "10  Low Flow Analysis",
    "section": "10.8 Distribution-free method",
    "text": "10.8 Distribution-free method\nWe won’t go over this in the same detail, but the xQy flow can also be calculated using a formula that does not assume a specific distribution. The expression, and code to perform it, is below.\nThe expression for xQy is:\n\\(xQy = (1-e) X(m1) + eX(m2)\\)\nwhere: [ ] indicates the value is truncated\n\\(X(m)\\) = the m-th lowest annual low flow of record\n\\(m1 = [(n+1)/y]\\)\n\\(m2 = [(n+l)/y] + 1\\)\n\\([z]\\) = the largest integer less than or equal to z\n\\(e = (n+l)/y - [(n+l)/y]\\)\nThis method is only appropriate when the desired return period is less than n/5 years\n\nx &lt;- Xday\ny &lt;- YrecInt\nn &lt;- length(QyearlyMins$minQ)\n\nm1 &lt;- trunc((n + 1)/y)\nm2 &lt;- trunc(((n + 1)/y) + 1)\n\ne &lt;- ((n + 1)/y) - m1\n\nXm1 &lt;- QyearlyMins$minQ[QyearlyMins$rank == m1]\nXm2 &lt;- QyearlyMins$minQ[QyearlyMins$rank == m2]\n\nDFxQy &lt;- (1-e) * Xm1 + e * Xm2\n\nDFxQy\n\n[1] 15"
  },
  {
    "objectID": "11-Flood_Frequency.html#template-repository",
    "href": "11-Flood_Frequency.html#template-repository",
    "title": "11  Flood Frequency Analysis and Creating Functions",
    "section": "11.1 Template Repository",
    "text": "11.1 Template Repository\nThe following activity is available as a template github repository at the following link:"
  },
  {
    "objectID": "11-Flood_Frequency.html#intro",
    "href": "11-Flood_Frequency.html#intro",
    "title": "11  Flood Frequency Analysis and Creating Functions",
    "section": "11.2 Intro",
    "text": "11.2 Intro\nThis methods for this chapter are adapted from the following activity: https://serc.carleton.edu/hydromodules/steps/166250.html\nAfter working with Flow Duration Curves (FDCs) and performing a low flow analysis, we now understand all the concepts necessary to perform a flood frequency analysis. In this chapter we will perform a flood frequency analysis using a Gumbel extreme value distribution and then write our own function that will return the magnitude of whatever probability flood we want!\nFirst we will load the tidyverse and dataRetrieval packages and the set the theme for our plots.\n\nlibrary(tidyverse)\nlibrary(dataRetrieval)\nlibrary(extRemes)\nlibrary(modelr)\n\ntheme_set(theme_classic())\n\nNext, download the yearly peakflow data from USGS dataRetrieval using the readNWISpeak() function. We don’t have to create our own yearly values like we did in the low flow analysis. This function just returns the highest flow for each year.\nDownload the data for the New River at Radford.\nThen make a plot of the peak flow for each year.\n\nradford &lt;- \"03171000\"\n\npeakflows &lt;- readNWISpeak(radford)\n\nggplot(peakflows, aes(peak_dt, peak_va))+\n  geom_point()\n\n\n\n\nAs with the couple previous chapters, the next step is to create a column that contains the ranks of each flow in the record. Create a column that has the rank of each flow, with the highest flow ranked #1. Use select() to trim your dataset to just the peak data, peak values, and ranks columns.\nMake the plot from the last code chunk again but color the points by rank to check that this worked. Also, look at the data through the environment tab in rstudio or using head() to double check.\n\n#create rank column (minus flips the ranking)\n#then clean it up, pull out only peak value, date, rank\npeakflows &lt;- peakflows |&gt; \n  mutate(ranks = rank(-peak_va)) |&gt;\n  select(peak_dt, peak_va, ranks)\n\n#look at it\nggplot(peakflows, \n       aes(peak_dt, peak_va, color = ranks))+\n  geom_point()\n\n\n\nhead(peakflows)\n\n     peak_dt peak_va ranks\n1 1878-09-15  217000   2.0\n2 1896-04-01   52400  40.0\n3 1897-02-22   67200  24.0\n4 1898-09-23   37200  70.0\n5 1899-03-05   50600  41.5\n6 1900-03-01   34000  78.5\n\n\nNow we need to calculate the exceedance probability and return period for each value in our data. For flood frequency analysis, it is common to use the Weibull plotting formula:\n\\(Tp = (N + 1) / m\\)\n\\(Tp\\) = Return Period\n\\(N\\) = Number of observations in your record\n\\(m\\) = Rank of specific observation, m = 1 is the largest, m = N is the smallest.\n\nAnd the return period is the inverse of the exceedance probability so:\nExceedance probability = \\(p = 1 / Tp\\)\nNon-exceedance probability = \\(pne = 1 - p\\)\n\nIn the chunk below, create a column in your dataset and calculate each: exceedance probability, non-exceedance probability, and return period.\nThen make a plot with peak flow on the Y axis and Return Period on the X.\n\nN &lt;- length(peakflows$peak_dt)\n\n#calculate return period, exceedence/non-exceedence with Weibull\npeakflows &lt;- peakflows |&gt;\n  mutate(Tp = (N + 1)/ranks,\n         pe = 1 / Tp,\n         pne = 1 - pe)\n\n#Plot peak flows on y and est return period on the x\npeakflows |&gt; \n  ggplot(aes(x = Tp, y = peak_va)) +\n  geom_point()\n\n\n\n\nNow we need to fit these data to a distribution in order to make a relationship we can use to predict the discharge of specific return intervals.\nThere are many distributions that can be used in this situation, but a common one for flood frequency analyses is the Gumbel extreme value distribution:\n\n\n\nGumbel Distribution\n\n\nx is observed discharge data, u and x are parameters that shape the distribution.\nWe can calculate u and x in order to create a distribution that best fits our data with the following equations. Notice x bar is mean and sx2 is variance. We will need to find sx, which is the square root of the variance, also known as the standard deviation.\n\n\n\nGumbel parameters\n\n\nIn the chunk below, calculate u and alpha by first calculating xbar (mean) and sx (standard deviation) and then using them in the above equations for u and x.\n\nxbar &lt;- mean(peakflows$peak_va)\n\nsx &lt;- sd(peakflows$peak_va)\n\nalpha &lt;- (sqrt(6)*sx) / pi\n\nu &lt;- xbar - (0.5772 * alpha)\n\nNow that we have the parameters that best represent our data as a Gumbel Distribution, we can use the formula to create the theoretical values for the return interval according to that distribution.\n\n\n\nGumbel Distribution\n\n\nIn the chunk below:\nFirst calculate non-exceedance probability with the equation above.\nThen calculate Tp theoretical (the return period) as T was calculated above Tp = 1 / (1-p)\nFinally create a plot of return period on the x axis and peak values on the y. Include return periods calculated from your data and those calculated from the Gumbel distribution on your plot as points of different colors.\n\npeakflows &lt;- peakflows |&gt; \n  mutate(\n    pne_gumbel = exp(-exp(-((peak_va - u) / alpha)))) |&gt;\n  mutate(Tp_gumbel = (1 / (1-pne_gumbel)))\n\n\npeakflows |&gt; ggplot() +\n  geom_point(aes(x = Tp_gumbel, y = peak_va, color = \"Gumbel\"))+\n  geom_point(aes(x = Tp, y = peak_va, color = \"Estimated\"))+\n  ylab(\"Annual Peak Flows\")+\n  xlab(\"Return Period\")+\n  theme_classic() \n\n\n\n\nLet’s look at these data a slightly different way to make it easier to see what is going on and how we can pull of flows for different return periods.\nMake the same plot as above but show the Gumbel distribution values as a line, with the estimated values as points, and log the x axis with limits set to 1 - 100. Save this plot, we will add to it later.\nWith this plot you could look up the return period for any flood or the discharge level for any return period.\n\npeakplot &lt;- peakflows |&gt; \n  ggplot() +\n  geom_point(aes(x = Tp, y = peak_va, color = \"Estimated\"))+\n  geom_line(aes(x = Tp_gumbel, y = peak_va, color = \"Gumbel\"))+\n  ylab(\"Annual Peak Flows\")+\n  xlab(\"Return Period\")+\n  scale_x_log10(breaks = c(1,5,10,20,50,100,200))+\n  theme_classic()\n\npeakplot\n\n\n\n\nThis plot is showing a representation of the fitted distribution by calculating the return period for each point in our dataset. But we can also use it to calculated the specific flow that corresponds to any return period by using the established relationship.\nIn the chunk below, calculate the magnitude of a 1 in 100 chance, or 100 year flood using the following two formulas where p = non-exceedance probability and Tp = return period. These are just the equations used to calculate return period rearranged to calculate peak flow.\np = 1 - (1 / Tp)\npeakflow = u - (alpha * ln(-ln(p)))\n(log() in r is the natural log, log10() is base 10 log)\nAccording to this flow, what is the 1 in 100 chance flood at this location? Do you see any issues with reporting this as the 1 in 100 chance flood? What are they?\n\nTp = 100\n\npne = 1 - (1/Tp)\n\npeak_va = u - (alpha * log(-log(pne)))\n\nThe gumbel distribution fit for the New River at Radford isn’t great. Fortunately there are ways to find an appropriate fit.\nThe generalized extreme value distribution will fit a much wider range of extreme value distributions. The distribution has three parameters:\nlocation, scale, and shape\nWhen the shape parameter is 0, the GEV is the same as the Gumbel Distribution. If it is greater than 0, the GEV is the the same as the Type II extreme value distribution. If it is less than 0, the GEV is the the same as the Type III extreme value distribution\nFitting data to this distribution is much more complex than what we did above with the Gumbel, but there are stats packages available that will do it for us.\nHere we will use the fevd() function from the extRemes package to fit a generalized extreme value distribution to our data.\nBelow, do the following - Pass the peak flow values to fevd() and save the result as “GEVD”. This will save all the outputs from fevd() so we can use them in a few different ways.\n\nPass the “GEVD” object to summary() and then plot(). You can see the GEVD object includes the three parameters: location, scale, and shape. plot() also shows you how well the fit turned out using several plots.\nSave the location, scale, and shape parameters. You get them out of the GEVD object using the following syntax: GEVD$results$par[#] where # is 1: Location, 2: Scale, 3: Shape. (if you are looking at the syntax in an un-knitted rmd, ignore the backslashes)\n\nBased on the parameters from the fit, what type of extreme value distribution best fits the New River data?\n\nGEVD &lt;- fevd(peakflows$peak_va)\n\nsummary(GEVD)\n\n\nfevd(x = peakflows$peak_va)\n\n[1] \"Estimation Method used: MLE\"\n\n\n Negative Log-Likelihood Value:  1478.643 \n\n\n Estimated parameters:\n    location        scale        shape \n3.243469e+04 1.679966e+04 2.824642e-01 \n\n Standard Error Estimates:\n    location        scale        shape \n1.688961e+03 1.419770e+03 7.949888e-02 \n\n Estimated parameter covariance matrix.\n              location         scale         shape\nlocation 2852588.84028  1.501585e+06 -40.722126890\nscale    1501585.06140  2.015746e+06  -4.189323634\nshape        -40.72213 -4.189324e+00   0.006320071\n\n AIC = 2963.286 \n\n BIC = 2971.865 \n\nplot(GEVD)\n\n\n\nlocation &lt;- GEVD$results$par[1]\nscale &lt;- GEVD$results$par[2]\nshape &lt;- GEVD$results$par[3]\n\nWe can now calculate flow magnitudes for the non-exceedance probabilities in our peakflows data using the function qevd(). Use a mutate to add a column of flows calculated from qevd, using the parameters from your distribution fit (location, scale, shape) and the non-exceedance probabilities calculated earlier (the pne column in the peakflows data).\n\npeakflows &lt;- peakflows |&gt;\n  mutate(\n    GEVDflows = qevd(\n     p = pne, \n     loc = location, \n     scale = scale, \n     shape = shape))\n\nNow add a line showing flows you just calculated to the plot we made above showing the Gumbel fit. Does this approach work better in this case?\n\n#add a line to the plot above that shows the Gumbel fit\npeakplot + \n  geom_line(data = peakflows, aes(x = Tp, y = GEVDflows, color = \"GEVD\"))\n\n\n\n\nFinally, we can calculate the magnitude of a flow with a specific return interval using the same qevd() function. If we want the 1 in 100 chance flood. Our return interval would be 100 and the non-exceedance probability would be 0.99 (pne above). Pass qevd this probability value and the location, scale, and shape parameters you got above to get the 1 in 100 chance flood magnitude.\nHow is this different from what we calculated above? Do you think it is more accurate? Why?\n\n#p from above is the \nqevd(p = .99, \n     loc = location, \n     scale = scale, \n     shape = shape)\n\nlocation \n191056.2 \n\n\nThis is a good opportunity to illustrate the usefulness of writing your own functions. When you install packages in R, you get a bunch of functions you can use. But you can also create these on your own to simplify your analyses!\nYou do this with the following syntax &gt;MyNewFunction &lt;- function(param1, param2){ &gt;code &gt;}\nWhatever the last line of the “code” portion of the function spits out, get’s returned from the function. So if you said X &lt;- mynewfunction(param1, parm2) X would now have it in whatever your function returned. See a simple example below: a function that adds 1 to any number we pass to it.\n\nadd1 &lt;- function(number){\n          number + 1\n          }\n\nadd1(4)\n\n[1] 5\n\n\nLet’s create a function that returns the return period for a flood of any magnitude for the gage we are investigating. Creating functions is a great way to streamline your workflow. You can write a function that performs an operation you need to perform a bunch of times, then just use the function rather than re-writing/copying the code.\nOur function will be called “ReturnPeriod” and we will pass it the flow we want the return period for, and the u and alpha of the distribution.\nWe will test the function by having it calculate the return period for the 100 year flood we calculated earlier (120027). If it works, it should spit out 100.\n\nReturnPeriod &lt;- function(flow, u, alpha){\n  \n  pTheoretical = exp(-exp(-((flow - u) / alpha)))\n  TpTheoretical = (1 / (1 - pTheoretical))\n  \n  TpTheoretical\n}\n\nReturnPeriod(158349, u, alpha)\n\n[1] 103.3297"
  },
  {
    "objectID": "11-Flood_Frequency.html#challenge-create-a-function",
    "href": "11-Flood_Frequency.html#challenge-create-a-function",
    "title": "11  Flood Frequency Analysis and Creating Functions",
    "section": "11.3 Challenge: Create a function",
    "text": "11.3 Challenge: Create a function\nCreate a function that returns the 100 year flood when given a USGS gage id. Use the generalized extreme value distribution fit function to do this."
  },
  {
    "objectID": "12-Intro-Geospatial-R.html#goals",
    "href": "12-Intro-Geospatial-R.html#goals",
    "title": "12  Geospatial data in R - Vector",
    "section": "12.1 Goals",
    "text": "12.1 Goals\nOur goals for this chapter are just to see some of the ways we can wrangle and plot vector spatial data using R. This is by no means the only way and is not an exhaustive demonstration of the packages loaded, but it’ll get us started.\nFirst, we need to define raster and vector spatial data.\nCheck out the images below for two examples of the same data represented as raster data or vector data.\nVector: Points, lines, polygons, boundaries are crisp regardless of scale Raster: Grid of same sized cells, vales in cells, cell size = resolution (smaller cells, higher resolution)\n\n\n\nRaster vs. Vector 1\n\n\n\n\n\nRaster vs. Vector 2\n\n\nQuestions from these two images:\nWhat are the advantages/disadvantages of raster/vector for each? Which is best to show on a map for each?  *For elevation, which would be better for calculating slope? \nSo, today we are sticking to vector data, but then we will be deal primarily with raster elevation data."
  },
  {
    "objectID": "12-Intro-Geospatial-R.html#intro-to-tmap",
    "href": "12-Intro-Geospatial-R.html#intro-to-tmap",
    "title": "12  Geospatial data in R - Vector",
    "section": "12.2 Intro to tmap",
    "text": "12.2 Intro to tmap\nWe are going to make maps mostly with tmap. But there are several other options (ggplot, leaflet, etc).\nLet’s look at how tmap works. It uses the same syntax as ggplot: the grammar of graphics.\nFirst we want to set tmap to static map mode. This is what we would want if we were making maps for a manuscript or slides. You can also make interactive maps with tmap, which we will show later. We will also set the check.and.fix option in tmap_options to TRUE, we need to do this for the data we are using, but it isn’t always necessary.\nSecond, we will read in our data. We’ll read in the “smallerws” shapefile from the CAMELS dataset and another shapefile of the outline of US states. To read in the shapefiles we will use st_read() from the sf package.\nNote that each of these shapefiles is in a separate folder and contains several files. You must have all of those files for the shapefile to work. *yes: “A shapefile” is actually several files. Silly? Maybe. The cause of much confusion when emailing someone “a shapefile”? Definitely.\nFinally we will read in a csv called gauge information that has some extra info we will join to the watershed shapefile later.\nOnce that is all done, we will look at the watershed data to see what is available in the shapefile attribute table.\nWhat extra information does the data have beyond a regular R object? Play around with it, can you reference columns in the table the same way you would with a regular object?\n\n#make sure tmap is in static map mode\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n#the CAMELS shapefile throws an error about having \n#invalid polygons, this line allows it to plot\ntmap_options(check.and.fix = TRUE)\n\n#Read shapefiles\nwatersheds &lt;- st_read(\"small_ws/smallerws.shp\")\n\nReading layer `smallerws' from data source \n  `/Users/jpgannon/My Drive (jpgannon@vt.edu)/CLASSES/SPRING Hydroinformatics/Quarto_Book/small_ws/smallerws.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 671 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.4377 ymin: 26.96621 xmax: -67.69059 ymax: 48.99931\nGeodetic CRS:  GCS_unknown\n\nstates &lt;- st_read(\"cb_2018_us_state_20m/cb_2018_us_state_20m.shp\")\n\nReading layer `cb_2018_us_state_20m' from data source \n  `/Users/jpgannon/My Drive (jpgannon@vt.edu)/CLASSES/SPRING Hydroinformatics/Quarto_Book/cb_2018_us_state_20m/cb_2018_us_state_20m.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 52 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1743 ymin: 17.91377 xmax: 179.7739 ymax: 71.35256\nGeodetic CRS:  NAD83\n\ngageinfo &lt;- read_csv(\"gauge information.csv\")\n\nRows: 672 Columns: 7\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): HUC_02, GAGE_ID, GAGE_NAME\ndbl (4): LAT, LONG, DRAINAGE_AREA_KM2, Elevation_m\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#look at the watersheds shapefile data\nhead(watersheds)\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -71.34071 ymin: 44.59213 xmax: -67.69059 ymax: 47.31557\nGeodetic CRS:  GCS_unknown\n   hru_id ann_P   lon_cen  lat_cen       AREA elev_mean ave_T july_T Perimeter\n1 1013500     0 -68.56551 47.01169 2303988087  277.4935    NA     NA    647993\n2 1022500     0 -68.07313 44.79691  620387273  103.6042    NA     NA    312624\n3 1030500     0 -68.14985 45.80527 3676154745  174.4339    NA     NA    662248\n4 1031500     0 -69.58119 45.23568  766544703  304.7435    NA     NA    309614\n5 1047000     0 -70.16213 44.98744  904956194  379.7800    NA     NA    310157\n6 1052500     0 -71.17197 44.96168  396110305  646.0736    NA     NA    172588\n                        geometry\n1 MULTIPOLYGON (((-68.06259 4...\n2 MULTIPOLYGON (((-67.97836 4...\n3 MULTIPOLYGON (((-68.09162 4...\n4 MULTIPOLYGON (((-69.31629 4...\n5 MULTIPOLYGON (((-70.10847 4...\n6 MULTIPOLYGON (((-71.10862 4...\n\n\nLet’s make a map showing the watersheds data. Each watershed has coordinates to draw it in the dataset, and tmap knows how to deal with that. It uses the same format as ggplot, but instead of ggplot() you will use tm_shape(). Then the geoms are prefixed tm_, so we will use tm_fill to show a map of the watersheds filled in with a color.\n\n# Pass the watershed data to tmap and fill the polygons \ntm_shape(watersheds) +\n  tm_fill()\n\nWarning: The shape watersheds is invalid. See sf::st_is_valid\n\n\n\n\n\nIf we use tm_borders instead, it will just outline the watersheds.\n\n# Add border layer to shape\ntm_shape(watersheds) +\n  tm_borders()\n\nWarning: The shape watersheds is invalid. See sf::st_is_valid\n\n\n\n\n\nThat’s fun, and maybe you can guess what country we are in based on the distribution of watersheds, but it would be better to show some geopolitical boundaries to get a sense of where we are.\nTo do this, we will use a second tm_shape and show the states data we read in on the plot as well. Just like in ggplot, you can use multiple tm_ functions to show multiple datsets on the same map.\n\n# Add border layer to shape\ntm_shape(watersheds) +\n  tm_fill() +\n  tm_shape(states)+\n  tm_borders()\n\nWarning: The shape watersheds is invalid. See sf::st_is_valid\n\n\n\n\n\nYou can also show the SAME data with multiple geoms. Let’s add tm_borders under the watershed portion of the map and before the states portion so we get fill and borders on out watersheds.\n\n# Add fill and border layers to shape\ntm_shape(watersheds) +\n  tm_fill() +\n  tm_borders()+\n  tm_shape(states)+\n  tm_borders()\n\nWarning: The shape watersheds is invalid. See sf::st_is_valid\n\n\n\n\n\nOkay, this is starting to look like a map! But we need to add elements to make it better.\nWe could do this all in one statement, but you can also save your existing map as a kind of “basemap” and then add to it later, just like with a ggplot object. We will save the above map as usa.\nThen we can use several built in geometries in tmap to add a compass, scale, and title. Note the syntax for specifying the position of the objects. Again, you could do this all in one statement too if you wanted.\n\n#Save basic map object as \"usa\"\nusa &lt;- tm_shape(watersheds) +\n  tm_fill(col = \"lightblue\")+\n  tm_shape(states)+\n  tm_borders()\n   \nusa + \n  tm_compass(type = \"8star\", position = c(\"right\", \"bottom\")) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))+\n  tm_layout(title = \"CAMELS Watersheds\", title.position = c(\"right\", \"TOP\"))\n\nWarning: The shape watersheds is invalid. See sf::st_is_valid\n\n\n\n\n\nBelow is an example of how to edit the “symbology” of the map. In other words, we want to color each of the polygons depending on a variable. Here we make the watersheds darker blue if they have a higher elevation.\nThe syntax below is basically (pseudo code):\n\nRepresent watersheds as shapes +\ncolor the shapes based on elev_mean, use 10 colors, use the Blues palette\nadd a legend in the bottom right, add some space for the title, define the title, position the title\nadd a compass at the bottom left\nadd a scale bar at the bottom left\n\n\n\ntm_shape(watersheds) + \n  tm_fill(col = \"elev_mean\", n = 10, palette = \"Blues\")+\n  tm_borders(lwd = 0.2)+\n  tm_shape(states)+\n  tm_borders()+\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            inner.margins = 0.1,\n            title = \"Mean Elevation\", \n            title.position = c(\"center\", \"TOP\"))+\n  tm_compass(type = \"8star\", position = c(\"left\", \"bottom\")) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))\n\nWarning: The shape watersheds is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "12-Intro-Geospatial-R.html#data-wrangling-with-tidyverse-principles",
    "href": "12-Intro-Geospatial-R.html#data-wrangling-with-tidyverse-principles",
    "title": "12  Geospatial data in R - Vector",
    "section": "12.3 Data wrangling with tidyverse principles",
    "text": "12.3 Data wrangling with tidyverse principles\nYou can use the same techniques as with other data to change or filter the spatial data. Below we filter to show just watershed number 3164000, which will be in quotes because the column is a character datatype. Note when we looked at the watersheds object above there is a column called hru_id for the watershed ids.\n\nwatersheds |&gt; filter(hru_id == \"3164000\") |&gt;\n  tm_shape() +\n  tm_fill() +\n  tm_borders()"
  },
  {
    "objectID": "12-Intro-Geospatial-R.html#add-non-spatial-data-to-spatial-data-with-a-join",
    "href": "12-Intro-Geospatial-R.html#add-non-spatial-data-to-spatial-data-with-a-join",
    "title": "12  Geospatial data in R - Vector",
    "section": "12.4 Add non-spatial data to spatial data with a join",
    "text": "12.4 Add non-spatial data to spatial data with a join\nWe have been using the watersheds shapefile. The other dataset we read in “gageinfo” has more data, but it is just a regular tibble, not a geospatial file.\nWe need to attach data from gageinfo object to the watersheds geospatial object based on watershed ID. How in the world will we do that?\nA JOIN!!\nIn the watersheds shapefile the watershed ids are in a column called hru_id and in gageinfo tibble they are in a column called GAGE_ID. So when we do the join, we need to tell R that these columns are the same and we want to use them to match the values. We will do a left join to accomplish this.\nBUT. TWIST!\nLet’s look at the first value in hru_id in watersheds: “1013500” Now the same one in GAGE_ID in gageinfo: “01013500”\nThey’re both the same IDs, but one has a leading zero. And because they are character format, “1013500” does not equal “01013500”. We can address this a couple of ways, but the best is probably to keep them as characters, but add a leading 0 to the hru_id column.\nWe will do this by creating a new column with a mutate and using the paste0() function to add a leading 0. To make the join easier, we will also give this new column the same name as in the gageinfo tibble: “GAGE_ID”.\nTo review joins, check out chapter @ref(getdata)\n\n#add leading 0 to hru_id column and save it as GAGE_ID\nwatersheds &lt;- watersheds |&gt;\n  mutate(GAGE_ID = paste0(\"0\", hru_id))\n\n#join watersheds and gageinfo using the GAGE_ID column as the key\nwatersheds_info &lt;- watersheds |&gt;\n  left_join(gageinfo, by = \"GAGE_ID\")\n\nhead(watersheds_info)\n\nSimple feature collection with 6 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -71.34071 ymin: 44.59213 xmax: -67.69059 ymax: 47.31557\nGeodetic CRS:  GCS_unknown\n   hru_id ann_P   lon_cen  lat_cen       AREA elev_mean ave_T july_T Perimeter\n1 1013500     0 -68.56551 47.01169 2303988087  277.4935    NA     NA    647993\n2 1022500     0 -68.07313 44.79691  620387273  103.6042    NA     NA    312624\n3 1030500     0 -68.14985 45.80527 3676154745  174.4339    NA     NA    662248\n4 1031500     0 -69.58119 45.23568  766544703  304.7435    NA     NA    309614\n5 1047000     0 -70.16213 44.98744  904956194  379.7800    NA     NA    310157\n6 1052500     0 -71.17197 44.96168  396110305  646.0736    NA     NA    172588\n   GAGE_ID HUC_02                                    GAGE_NAME      LAT\n1 01013500     01             Fish River near Fort Kent, Maine 47.23739\n2 01022500     01      Narraguagus River at Cherryfield, Maine 44.60797\n3 01030500     01  Mattawamkeag River near Mattawamkeag, Maine 45.50097\n4 01031500     01 Piscataquis River near Dover-Foxcroft, Maine 45.17501\n5 01047000     01   Carrabassett River near North Anson, Maine 44.86920\n6 01052500     01    Diamond River near Wentworth Location, NH 44.87739\n       LONG DRAINAGE_AREA_KM2 Elevation_m                       geometry\n1 -68.58264           2252.70      250.31 MULTIPOLYGON (((-68.06259 4...\n2 -67.93524            573.60       92.68 MULTIPOLYGON (((-67.97836 4...\n3 -68.30596           3676.17      143.80 MULTIPOLYGON (((-68.09162 4...\n4 -69.31470            769.05      247.80 MULTIPOLYGON (((-69.31629 4...\n5 -69.95510            909.10      310.38 MULTIPOLYGON (((-70.10847 4...\n6 -71.05749            383.82      615.70 MULTIPOLYGON (((-71.10862 4...\n\n\nAnd now we can plot this formerly non-spatial data on our map.\nIn this case, we can now add the name of the watershed to the map rather than the number.\n\n#now the gageinfo columns are available for us to use when mapping\nwatersheds_info |&gt; filter(hru_id == \"3164000\") |&gt;\n  tm_shape() +\n  tm_fill() +\n  tm_borders() +\n  tm_text(\"GAGE_NAME\", size = 1) \n\n\n\n\nWe can also subset vector data to create new datasets or plot. Below we will use filter statements to grab all the polygons in Virginia from the watersheds shapefile and then the Virginia state outline from the states shapefile.\nThe method we will use for getting the Virginia watersheds is a little new. We are going to use the grepl() function to grab any gages that include the text “, VA” since there isn’t a state column, but the watershed names include the state.\nWhy are we doing “, VA” and not just “VA”? Good question!\n“, VA” will most likely only show up in gage names that are in Virginia because they’ll be in the format “Gage name, VA”. If we just say “VA” we might get some gages that have “VA” as part of their name but are not in Virginia.\nOnce we successfully filter to Virginia, we will then make a nice map of the CAMELS watersheds in Virginia!\n\nva_watersheds &lt;- filter(watersheds_info, grepl(\", VA\", GAGE_NAME))\nva_outline &lt;- filter(states, NAME == \"Virginia\")\n\nva_outline |&gt;\n  tm_shape() +\n  tm_borders() + \n  tm_shape(va_watersheds) +\n  tm_fill() +\n  tm_borders() +\n  tm_layout(inner.margins = 0.1,\n            title = \"VA CAMELS Watersheds\", \n            title.position = c(\"left\", \"TOP\"))+\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"left\", \"top\"))\n\n\n\n\nIn addition to filtering, we can use the data in the attribute table to calculate additional parameters, just like with a normal object. Below we calculate the ratio of Area to Perimeter by changing the perimeter datatype to numeric and then dividing area by perimeter.\n\nva_watersheds &lt;- va_watersheds |&gt; \n  mutate(PerimeterNum = as.numeric(Perimeter),\n         Area_Perimeter = AREA / PerimeterNum) \n\nhead(va_watersheds)\n\nSimple feature collection with 6 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -79.32293 ymin: 38.31301 xmax: -77.55629 ymax: 39.2801\nGeodetic CRS:  GCS_unknown\n   hru_id ann_P   lon_cen  lat_cen      AREA elev_mean ave_T july_T Perimeter\n1 1620500     0 -79.27321 38.37592  61339485  913.1406    NA     NA     69535\n2 1632000     0 -78.96078 38.70073 547286614  605.6940    NA     NA    175812\n3 1632900     0 -78.72713 38.54450 249206633  410.4016    NA     NA    134796\n4 1634500     0 -78.44939 39.06919 264494917  441.7808    NA     NA    151377\n5 1638480     0 -77.67379 39.20060 233835164  175.1224    NA     NA    112488\n6 1644000     0 -77.80391 38.99276 859599217  187.0871    NA     NA    237588\n   GAGE_ID HUC_02                                GAGE_NAME      LAT      LONG\n1 01620500     02         NORTH RIVER NEAR STOKESVILLE, VA 38.33763 -79.24004\n2 01632000     02 N F SHENANDOAH RIVER AT COOTES STORE, VA 38.63706 -78.85280\n3 01632900     02          SMITH CREEK NEAR NEW MARKET, VA 38.69345 -78.64279\n4 01634500     02          CEDAR CREEK NEAR WINCHESTER, VA 39.08122 -78.32945\n5 01638480     02        CATOCTIN CREEK AT TAYLORSTOWN, VA 39.25455 -77.57638\n6 01644000     02            GOOSE CREEK NEAR LEESBURG, VA 39.01955 -77.57749\n  DRAINAGE_AREA_KM2 Elevation_m                       geometry PerimeterNum\n1             44.74      786.35 MULTIPOLYGON (((-79.24689 3...        69535\n2            543.36      543.76 MULTIPOLYGON (((-78.98026 3...       175812\n3            245.05      417.29 MULTIPOLYGON (((-78.85841 3...       134796\n4            263.98      344.44 MULTIPOLYGON (((-78.37821 3...       151377\n5            232.00      155.98 MULTIPOLYGON (((-77.62354 3...       112488\n6            859.17      187.25 MULTIPOLYGON (((-77.62354 3...       237588\n  Area_Perimeter\n1       882.1383\n2      3112.9082\n3      1848.7688\n4      1747.2596\n5      2078.7565\n6      3618.0245\n\n\nNow we can plot our newly calculated data by controlling color with that new column name.\n\nva_outline |&gt;\n  tm_shape() +\n  tm_borders() + \n  tm_shape(va_watersheds) +\n  tm_fill(col = \"Area_Perimeter\", n = 10, palette = \"Reds\") +\n  tm_borders()+\n  tm_layout(inner.margins = 0.12,\n            title = \"VA CAMELS Watersheds\", \n            title.position = c(\"left\", \"TOP\"))+\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"))"
  },
  {
    "objectID": "12-Intro-Geospatial-R.html#plot-maps-side-by-side",
    "href": "12-Intro-Geospatial-R.html#plot-maps-side-by-side",
    "title": "12  Geospatial data in R - Vector",
    "section": "12.5 Plot maps side by side",
    "text": "12.5 Plot maps side by side\nJust like we can use facets in ggplot, we can use facets to show multiple maps. Below we color our map by AREA and elev_mean and put them next to each other using tm_facets.\n\nfacets = c(\"AREA\", \"elev_mean\")\n\ntm_shape(va_watersheds) + \n  tm_polygons(facets) + \n  tm_facets(ncol = 2, sync = FALSE)"
  },
  {
    "objectID": "12-Intro-Geospatial-R.html#built-in-styles-like-themes-in-ggplot",
    "href": "12-Intro-Geospatial-R.html#built-in-styles-like-themes-in-ggplot",
    "title": "12  Geospatial data in R - Vector",
    "section": "12.6 Built in styles, like themes in ggplot",
    "text": "12.6 Built in styles, like themes in ggplot\nTmap also has built in styles, which are like themes in ggplot. We can use these styles with tm_style. Try “classic”, “cobalt”, or “col_blind” below.\n\nva_outline |&gt;\n  tm_shape() +\n  tm_borders() + \n  tm_shape(va_watersheds) +\n  tm_fill(col = \"Area_Perimeter\", n = 10, palette = \"Reds\") +\n  tm_borders()+\n  tm_layout(inner.margins = 0.12,\n            title = \"VA CAMELS Watersheds\", \n            title.position = c(\"left\", \"TOP\"))+\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"))+\n  tm_style(\"classic\") #try cobalt, bw, col_blind\n\nNote that tm_style(\"classic\") resets all options set with tm_layout, tm_view, tm_format, or tm_legend. It is therefore recommended to place the tm_style element prior to the other tm_layout/tm_view/tm_format/tm_legend elements."
  },
  {
    "objectID": "12-Intro-Geospatial-R.html#interactive-maps",
    "href": "12-Intro-Geospatial-R.html#interactive-maps",
    "title": "12  Geospatial data in R - Vector",
    "section": "12.7 Interactive Maps",
    "text": "12.7 Interactive Maps\n\n12.7.1 tmap\nYou can also generate maps that you can interact with, as opposed to static maps, that we have been using before. If you are generating a map for an app or webpage, this may be a good choice. But for a pdf report, the static maps are more appropriate.\nIn tmap all you have to do is run tmap_mode(“view”) and it will create an interactive map with the exact same syntax! To switch back to a static map, run tmap_mode(“plot”)\nAlso in this chunk we see how to add a basemap to a tmap object, using tm_basemap.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nusa &lt;- tm_shape(watersheds_info) +\n  tm_fill(col = \"lightblue\", alpha = 0.3)+\n  tm_borders()+\n  tm_shape(states)+\n  tm_borders() +\n  tm_layout(title = \"CAMELS Watersheds\", title.position = c(\"right\", \"TOP\"))\n\nusa + tm_basemap(server = \"OpenTopoMap\")\n\nWarning: The shape watersheds_info is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\n\n\n12.7.2 Leaflet\nLeaflet is another way to make interactive maps. It’s syntax is very different, as you can see below. But depending on what functionality you need, it could be a better choice.\n\nleaflet(watersheds_info) |&gt; \n  addTiles() |&gt;\n  addPolygons(color = \"#444444\", weight = 1, smoothFactor = 0.5,\n    opacity = 1.0, fillOpacity = 0.5,\n    fillColor = ~colorQuantile(\"YlOrRd\", elev_mean)(elev_mean))\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'"
  },
  {
    "objectID": "13-Summative-Assessment-2.html#info-for-assessment",
    "href": "13-Summative-Assessment-2.html#info-for-assessment",
    "title": "13  Summative Assessment 2",
    "section": "13.1 Info for assessment",
    "text": "13.1 Info for assessment\nTo complete this assessment, go to the repository linked below and either copy it to your github account or download the repository, just as you do for other assignments and activities in class.\nGithub repo: https://github.com/VT-Hydroinformatics/14-Summative2"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#introduction",
    "href": "14-Geospatial-Raster-Hydro.html#introduction",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nThe following activity is available as a template github repository at the following link: https://github.com/VT-Hydroinformatics/13-Geospatial-Raster-Hydro.git\nFor more: https://geocompr.robinlovelace.net/spatial-class.html#raster-data\nTo read in detail about any of the WhiteboxTools used in this activity, check out the user manual: https://jblindsay.github.io/wbt_book/intro.html\nIn this activity we are going to explore how to work with raster data in R while computing several hydrologically-relevant landscape metrics using the R package whitebox tools. Whitebox is very powerful and has an extensive set of tools, but it is not on CRAN. You must install it with the commented-out line at the top of the next code chunk.\nInstall/Load necessary packages and data:\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(sf)\nlibrary(whitebox)\nlibrary(tmap)\n\n#If installing/using whitebox for the first time\n#install_whitebox()\n\nwhitebox::wbt_init()\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#read-in-dem",
    "href": "14-Geospatial-Raster-Hydro.html#read-in-dem",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.2 Read in DEM",
    "text": "14.2 Read in DEM\nFirst, we will set tmap to either map or view depending on how we want to see our maps. I’ll often set to map unless I specifically need to view the maps interactively because if they are all set to view it makes scrolling through the document kind of a pain: every time you hit a map the scroll zooms in or out on the map rather than scrolling the document.\nFor this activity we are going to use a 5-meter DEM of a portion of a Brush Mountain outside Blacksburg, VA.\nWhat does DEM stand for? What does it show?\nWhat does it mean that the DEM is “5-meter”?\nWe will use rast() to load the DEM. We let R know that coordinate system is WGS84 by setting the crs equal to ‘+init=EPSG:4326’ using the crs() funciton, where 4326 is the EPSG number for WGS84.\nNext, an artifact of outputting the DEM for this analysis is that there are a bunch of errant cells around the border that don’t belong in the DEM. If we make a map with them, it really throws off the scale. So we are going to set any elevation values below 1500 ft to NA. Note how this is done as if the dem was just a normal vector. COOL!\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ndem &lt;- rast(\"McDonaldHollowDEM/brushDEMsm_5m.tif\")\n\ncrs(dem) &lt;- '+init=EPSG:4326'\n\nwriteRaster(dem, \"McDonaldHollowDEM/brushDEMsm_5m_crs.tif\", overwrite = TRUE) \n\ndem[dem &lt; 1500] &lt;- NA"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#plot-dem",
    "href": "14-Geospatial-Raster-Hydro.html#plot-dem",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.3 Plot DEM",
    "text": "14.3 Plot DEM\nNow let’s plot the DEM. We will use the same syntax as we did in the previous lecture about vector data. Give tm_shape the raster, then visualize it with tm_raster. We will tell tmap that the scale on the raster is continuous, which color palette to use, whether or not to show the legend, and then add a scale bar.\n\ntm_shape(dem)+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE)+\n  tm_scale_bar()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#generate-a-hillshade",
    "href": "14-Geospatial-Raster-Hydro.html#generate-a-hillshade",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.4 Generate a hillshade",
    "text": "14.4 Generate a hillshade\nSince the DEM is just elevation values, it is tough to see much about the landscape. We can see the stream network and general featuers, but not much more. BUT those features are there! We just need other ways to illuminate them. One common way to visualize these kind of data is a hillshade. This basically “lights” the landscape with a synthetic sun, casting shadows and illuminating other features. You can control the angle of the sun and from what direction it is shining to control the look of the image. We will position the sun in the south-south east so it illuminates the south side of Brush Mountain well.\nWe will use the whitebox tools function wbt_hillshade() to produce a hillshade.\n\n14.4.1 How whitebox tools functions work\nThe whitebox tools functions work can be a little tricky to work with at first. You might want to pass R objects to them and get R objects back, but that’s not how they are set up.\nBasically for your input, you well the wbt function the name of the file that has the input data.\nFor output, you tell it what to name the output.\nThe wbt function then outputs the calculated data to your working directory, or whatever directory you give it in the output argument.\nThis means if you want to do something with the output of the wbt function, you have to read it in separately.\nIn the chunk below we will read in the brush mountain 5m DEM and output a hillshade with wbt_hillshad().\nWe will then read the output hillshade in with raster() and make a map with tmap. We will use the “Greys” palette in revers by adding a negative sign in front of it. This is just to make the hillshade look nice.\nNotice how much more you can see in the landscape!\n\nwbt_hillshade(dem = \"McDonaldHollowDEM/brushDEMsm_5m_crs.tif\",\n              output = \"McDonaldHollowDEM/brush_hillshade.tif\",\n              azimuth = 115)\n\nhillshade &lt;- rast(\"McDonaldHollowDEM/brush_hillshade.tif\")\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\", palette = \"-Greys\", legend.show = FALSE)+\n  tm_scale_bar()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#prepare-dem-for-hydrology-analyses",
    "href": "14-Geospatial-Raster-Hydro.html#prepare-dem-for-hydrology-analyses",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.5 Prepare DEM for Hydrology Analyses",
    "text": "14.5 Prepare DEM for Hydrology Analyses\nAlright, we are cooking now.\nBut we have to cool our jets for a second. If we are going to do hydrologic analyses, we have to prep our DEM a bit.\nOur hydrologic tools often work on the premise of following water down the hillslope based on the elevation of the cells in the DEM. If, along a flowpath, there is no cell lower than a location, the algorithm we are using will stop there. This is called a sink, pit, or depression. See the figure below.\n\n\n\nSink\n\n\nWe can deal with these features two ways. The first is to “fill” them. Which means the dead end cells will have their elevations raised until the pit is filled and water can flow downhill. See below.\n\n\n\nFilled\n\n\nThe second way we can deal with these features is to “breach” them. This means the side of the feature that is blocking flow will be lowered to allow water to flow downhill. See below.\n\n\n\nBreached\n\n\nWe are going to do both to prep our DEM. We will first breach depressions using wbt_breach_depressions_least_cost(), which will lower the elevation of the cells damming depressions.\nTo use this function we will also give it a maximum distance to search for a place to breach the depression (dist) and tell with whether to fill any depressions leftover after it does its thing (fill).\nThen, because this algorithm can leave some remaining depressions, we wil use wbt_fill_depressions_wang_and_liu() to clean up any remaining issues.\nBe careful to give wbt_fill_depressions_wang_and_liu() the result of the breach depressions function, not the original DEM!\n\nwbt_breach_depressions_least_cost(\n  dem = \"McDonaldHollowDEM/brushDEMsm_5m_crs.tif\",\n  output = \"McDonaldHollowDEM/bmstationdem_breached.tif\",\n  dist = 5,\n  fill = TRUE)\n\nwbt_fill_depressions_wang_and_liu(\n  dem = \"McDonaldHollowDEM/bmstationdem_breached.tif\",\n  output = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\"\n)"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#visualize-filled-sinks-and-breached-depressions",
    "href": "14-Geospatial-Raster-Hydro.html#visualize-filled-sinks-and-breached-depressions",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.6 Visualize filled sinks and breached depressions",
    "text": "14.6 Visualize filled sinks and breached depressions\nNow let’s look at what this did. This is a great example of how easily you can use multiple rasters together.\nIf we want to see how the fill and breach operations changed the DEM, we can just subtract the filled and breached DEM from the original. Then, in areas where nothing changed, the values will be zero, areas that were filled will be positive, and areas that were decreased in elevation to “breach” a depression will be negative.\nTo more easily see where stuff happened, we will set all the cells that equal zero to NA. Then we will plot them on the hillshade.\nWhere where changes made?\nWhat do you think these pit/depression features represent in real life?\n\nfilled_breached &lt;- rast(\"McDonaldHollowDEM/bmstationdem_filled_breached.tif\")\n\n## What did this do?\ndifference &lt;- dem - filled_breached\n\ndifference[difference == 0] &lt;- NA\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\n  tm_scale_bar()+\ntm_shape(difference)+\n  tm_raster(style = \"cont\",legend.show = TRUE)+\n  tm_scale_bar()\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#d8-flow-accumulation",
    "href": "14-Geospatial-Raster-Hydro.html#d8-flow-accumulation",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.7 D8 Flow Accumulation",
    "text": "14.7 D8 Flow Accumulation\nThe first hydrological analysis we will perform is the D8 flow accumulation algorithm. In whitebox this is wbt_d8_flow_accumulation(). We give the function the DEM, and for each cell, it determines the direction water fill flow from that cell. To do this it looks at the elevation of the surrounding cells relative to the current cell. In the D8 algorithm, the flow direction can be one of 8 directions, show in the figure below. All flow from the current cell is moved to the cell to which the flow direction points.\nUsing another function, we can just output the flow direction for each cell. This will be important when we delineate a watershed, but for visualization and many analysis purposes, we just want to look at the flow accumulation.\nThis tool outputs a raster that tells us how many cells drain to each cell. In other words, for a given cell in the raster, its value corresponds to the number of cells that drain to it. As a result, this highlights streams quite well.\n\n\n\nD8 Flow Direction\n\n\nThe wbt_d8_flow_accumulation() function takes an input of a DEM or a flow direction (pointer) file. We will pass it out filled, breached DEM. The default output is the number of cells draining to each cell, but you can also choose specific contributing area or contributing area.\nWe will visualize our output buy plotting the log of the D8 flow accumulation grid over the hillshae with an opacity of 0.5 using the alpha parameter in tm_raster. Mapping with the hillshade helps us see the flow accumulation in the context of the landscape. Ploting the log values helps us see differences in flow accumulation, because the high values are so much higher than the low values in a flow accumulation grid.\nWhere are the highest values? Where are the lowest values?\n\nwbt_d8_flow_accumulation(input = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n                         output = \"McDonaldHollowDEM/D8FA.tif\")\n\nd8 &lt;- rast(\"McDonaldHollowDEM/D8FA.tif\")\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\ntm_shape(log(d8))+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE, alpha = .5)+\n  tm_scale_bar()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#d-infinity-flow-accumulation",
    "href": "14-Geospatial-Raster-Hydro.html#d-infinity-flow-accumulation",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.8 D infinity flow accumulation",
    "text": "14.8 D infinity flow accumulation\nAnother method of calculating flow accumulation is the D infinity algorithm. This operates similarly to the D8 algorithm but with some important differences.\nWith D infinity, the flow direction for each cell can be any angle. Endless possibilities!\nIf the angle does not point squarely at one of the neighboring cells, the flow from the focus cell can be SPLIT between neighboring cells. We will see the resulting difference this makes when we look at the output.\n\n\n\nD inf Flow Direction\n\n\nThe function for d infinity is wbt_d_inf_flow_accumulation() and like the D8 function it will take a flow direction (pointer) file or a DEM. We will give it out filled and breached DEM.\nAs with the D8 data, we will plot the logged accumulation values over a hillshade with 50% opacity.\nHow does this look different from the D8 results?\nWhich do you think represents reality better? Why?\n\nwbt_d_inf_flow_accumulation(\"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n                         \"McDonaldHollowDEM/DinfFA.tif\")\n\ndinf &lt;- rast(\"McDonaldHollowDEM/DinfFA.tif\")\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\ntm_shape(log(dinf))+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE, alpha = 0.5)+\n  tm_scale_bar()\n\nVariable(s) \"NA\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette."
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#topographic-wetness-index",
    "href": "14-Geospatial-Raster-Hydro.html#topographic-wetness-index",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.9 Topographic Wetness Index",
    "text": "14.9 Topographic Wetness Index\nThe topographic wetness index (TWI) combines flow accumulation with the slope of each cell to calculate a index that basically corresponds to how likely an area is to be wet. If we think about a landscape, an area with a lot of contributing area and a flat slope is more likely than an area with a lot of contributing area and a steep slope to be wet. That’s what TWI measures.\nThe formula for TWI is\n\\(TWI = Ln(As / tan(Slope))\\)\nWhere As is the specific contributing area and Slope is the slope at the cell. Specific contributing area is contributing area per unit contour length.\nThis is the first function we will use that doesn’t just take a DEM as input. The wbt_wetness_index() function takes a raster of specific contributing area and another of slope. We will need to create each of these first and then pass them to the function (since we didn’t calculate specific contributing area when we used the flow accumulation algorithms above)\nWe will then plot TWI with the same approach we used for the flow accumulation data.\nWe get some funkiness around the edges of the DEM, so we will also filter the twi output so the visualization looks better.\n\nwbt_d_inf_flow_accumulation(input = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n                            output = \"McDonaldHollowDEM/DinfFAsca.tif\",\n                            out_type = \"Specific Contributing Area\")\n\nwbt_slope(dem = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n          output = \"McDonaldHollowDEM/demslope.tif\",\n          units = \"degrees\")\n\nwbt_wetness_index(sca = \"McDonaldHollowDEM/DinfFAsca.tif\",\n                  slope = \"McDonaldHollowDEM/demslope.tif\",\n                  output = \"McDonaldHollowDEM/TWI.tif\")\n\ntwi &lt;- rast(\"McDonaldHollowDEM/TWI.tif\")\n\ntwi[twi &gt; 0] &lt;- NA\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\ntm_shape(twi)+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE, alpha = 0.5)+\n  tm_scale_bar()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#downslope-twi",
    "href": "14-Geospatial-Raster-Hydro.html#downslope-twi",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.10 Downslope TWI",
    "text": "14.10 Downslope TWI\nAnother index of interest is the downslope index. The downslope index is the measure of slope between a cell and then another cell a specified distance downslope. We can replace the regular slope number in the TWI calculation with downslope index to calculate TWId or Downslope TWI.\n\\(TWI = Ln(As / tan(DownslopeIndex))\\)\nAs is specific contributing area, and the tan of the downslope index can be output from the whitebox downslope index tool. We have to do this calculation with the rasters becasue there isn’t a specific whitebox tool that calculateds TWId.\nEssentially, this index captures the fact that you should expect a 50 meter long bench on a hillslope to be wetter than a 1 meter long bench, because the 1 meter one would drain a lot faster.\nSimilar to TWI we get some edge issues so we will filter the result to make the visualization look better.\n\nwbt_downslope_index(dem = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n                    output = \"McDonaldHollowDEM/downslope_index.tif\",\n                    out_type = \"tangent\")\n\ndownslope_index &lt;- rast(\"McDonaldHollowDEM/downslope_index.tif\")\n\ndinfFA &lt;- rast(\"McDonaldHollowDEM/DinfFAsca.tif\")\n\ntwid &lt;- log(dinfFA / downslope_index)\n\ntwid[twid &gt; 40] &lt;- NA\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\ntm_shape(twid)+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE, alpha = 0.5)+\n  tm_scale_bar()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#map-stream-network",
    "href": "14-Geospatial-Raster-Hydro.html#map-stream-network",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.11 Map Stream Network",
    "text": "14.11 Map Stream Network\nOne neat and often very useful thing we can do with the flow accumulation grids we calculated, is map the stream network in a watershed. If we look at either flow accumulation grid we can see the highest values are in the streams. Therefore if we determine the flow accumulation value at the highest place on the streamnetwork with consistent flow, we can set all cells with a flow accumulation lower than that to NO and we will only have cells that are in the stream.\nOften, we actually want out stream network to be represented as lines, so we then have to convert that raster to a vector format.\nWhitebox Tools has two handy functions to let us do this: wbt_extract_streams() makes a raster of the stream network by using a threshold flow accumulation you give it. It takes a D8 flow accumulation grid as input.\nThen wbt_raster_streams_to_vector() will take the outut from wbt_extract_streams() and a D8 pointer file and output a shapefile of your stream network.\nBelow we extrac the streams, generate a D8 pointer file, and then convert the raster streams to vector. We will then plot the streams on the hillshade.\n\nwbt_extract_streams(flow_accum = \"McDonaldHollowDEM/D8FA.tif\",\n                    output = \"McDonaldHollowDEM/raster_streams.tif\",\n                    threshold = 6000)\n\nwbt_d8_pointer(dem = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n               output = \"McDonaldHollowDEM/D8pointer.tif\")\n\nwbt_raster_streams_to_vector(streams = \"McDonaldHollowDEM/raster_streams.tif\",\n                             d8_pntr = \"McDonaldHollowDEM/D8pointer.tif\",\n                             output = \"McDonaldHollowDEM/streams.shp\")\n\nstreams &lt;- read_sf(\"McDonaldHollowDEM/streams.shp\")\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\ntm_shape(streams)+\n  tm_lines(col = \"blue\")+\n  tm_scale_bar()\n\nWarning: Currect projection of shape streams unknown. Long-lat (WGS84) is\nassumed."
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#extract-raster-values-to-point-locations",
    "href": "14-Geospatial-Raster-Hydro.html#extract-raster-values-to-point-locations",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.12 Extract raster values to point locations",
    "text": "14.12 Extract raster values to point locations\nNow let’s say you have some sample or monitoring sites in this study area. You may want to know what the values of the rasters we just made are for your sites. Below we will read in the coordinates of a few sites and extract data from a single raster and then multiple at once.\n\n14.12.1 Import and plot points\nFirst, we will read in a csv of point locations.\nThen we have to convert it to a spatial datatype. We will use st_as_sf() to do this. We need to give it our points, and then cell it what projection and format our data is in. We are using geographic coordinates (lon, lat) and the WGS84 datum (most gps’ use this and it is EPSG 4326).\nThen we will plot the points just to make sure they show up where we expect.\n\npoints &lt;- read_csv(\"McDonaldHollowDEM/points.csv\")\n\nRows: 3 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): lon, lat\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npointsSP &lt;- st_as_sf(points, coords = c(\"lon\", \"lat\"), crs = 4326)\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(twid)+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE)+\n  tm_scale_bar()+\ntm_shape(pointsSP)+\n  tm_dots(col = \"red\")\n\n\n\n\n\n\n14.12.2 Extract values from a single raster\nWe can extract values from any raster using the extract() function in the raster package. We give this funciton the raster we want to pull data from (x) and the points where we want data (y). Then we specify how we want it to grab data from the raster. “Simple” just grabs the value at the locaitons specified. If we specify “bilinear” it will return an interpolated value based on the four nearest raster cells.\nThis function just spits out a vector of values disconnected from our points, so next we will add the column to our existing points object. NOT the geospatial one, just the datafram/tibble.\n\ntwidvals &lt;-  extract(x = twid, #raster \n                 y = pointsSP, #points\n                 method = \"simple\")\n\npoints$twid &lt;- twidvals$DinfFAsca\n\npoints\n\n# A tibble: 3 × 3\n    lon   lat  twid\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -80.5  37.2 -19.9\n2 -80.5  37.2 -19.6\n3 -80.5  37.2 -18.3\n\n\n\n\n14.12.3 Extract values from multiple rasters at once\nMaybe you want a bunch of topographic information for all your sample sites. You could repeat the process above for each one, or we can stack the rasters of interest and then pull out values for each using the same method as below. WHOA!\nSo: we give c() each raster we want data from, using the rasters we read in earlier in the activity\nThen: use the same exact syntax as when we extracted values from a single raster, but give the extract funciton the stacked raster.\nWe can then use cbind() to slap the extracted data onto our existing dataframe (gain, NOT the spatial one, just the original regular one)\n\nslope &lt;- rast(\"McDonaldHollowDEM/demslope.tif\")\n\nraster_stack &lt;- c(twi, twid, slope, dem)\n\nraster_values &lt;-extract(x = raster_stack, #raster \n                 y = pointsSP, #points\n                 method = \"simple\")\n\npoints &lt;- cbind(points, raster_values)\n\npoints\n\n        lon      lat      twid ID       TWI DinfFAsca demslope brushDEMsm_5m\n1 -80.48355 37.24087 -19.90985  1 -8.450521 -19.90985 43.42010      2130.564\n2 -80.48705 37.24570 -19.64899  2 -8.174915 -19.64899 57.76965      2301.624\n3 -80.48477 37.24697 -18.29477  3 -6.870579 -18.29477 30.68143      2430.992"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#view-raster-data-as-a-pdf-or-histogram",
    "href": "14-Geospatial-Raster-Hydro.html#view-raster-data-as-a-pdf-or-histogram",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.13 View raster data as a PDF or histogram",
    "text": "14.13 View raster data as a PDF or histogram\nOften if can be useful to look at a summary of different topographic characteristics in an area or watershed outside of a map. One way we can do this is to look at a histogram or pdf of the values in our map by converting the raster values to a dataframe and plotting with ggplot.\nWe will use as.data.frame to convert our slope raster to a data frame and then plot the result in ggplot with stat_density.\nIt would be very difficult to accurately describe the differences in slope between two areas by looking at a map of the values, but this way we can do it quite effectively.\n\nslopedf &lt;- as.data.frame(slope)\n\nggplot(slopedf, aes(demslope)) +\n  stat_density()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#subsetting-a-raster-for-visualization",
    "href": "14-Geospatial-Raster-Hydro.html#subsetting-a-raster-for-visualization",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.14 Subsetting a raster for visualization",
    "text": "14.14 Subsetting a raster for visualization\nSimilar to how we set junky values to NA earlier in this activity, we can also use this as a visualization tool. We will create a new raster with areas with a slope of less than 60 percent set to NA. When we plot this, it will just show slope where slope is greater than 60.\nAt it’s heart this is a reclassification. You could use the same strategy to classify slopes into bins. For example, make slopes from 0 - 60 percent equal to 1, for low slope, and then &gt; 60 equal to 2 for high slope… or any number of bins. This can be really useful for finding locations that satisfy a several criteria. Think: where might we find the right habitat for a certain tree species, or bird, or sasquatches?\n\nslope2 &lt;- slope\nslope2[slope2 &lt; 60] &lt;- NA\n\ntm_shape(slope2)+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE)+\n  tm_scale_bar()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#raster-math",
    "href": "14-Geospatial-Raster-Hydro.html#raster-math",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.15 Raster Math",
    "text": "14.15 Raster Math\nSuper quick: you can also do math with your rasters!\nIf you’ve made model that predicts the likelihood of the location or something, you can just plug your rasters in like a normal equation and it’ll do the math and you can map it! SO. COOL.\nHere’s a super simple example to just illustate that you can do this: we will just divide slope by the elevation (dem)\n\ndemXslope &lt;- slope / dem\n\ntm_shape(demXslope)+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE)+\n  tm_scale_bar()"
  },
  {
    "objectID": "14-Geospatial-Raster-Hydro.html#extra-plot-topo-characteristics-against-one-another",
    "href": "14-Geospatial-Raster-Hydro.html#extra-plot-topo-characteristics-against-one-another",
    "title": "14  Geospatial R Raster - Hydro Analyses",
    "section": "14.16 Extra: plot topo characteristics against one another",
    "text": "14.16 Extra: plot topo characteristics against one another\nHere we convert slope and elevation each to a dataframe, and then plot a 2 dimensional density plot.\n\nrasters_df &lt;- as.data.frame(raster_stack)\n\nggplot(rasters_df, aes(x = demslope, y = brushDEMsm_5m))+\n  geom_density_2d_filled()+\n  ylab(\"Elevation (ft)\")+\n  xlab(\"Slope (%)\")\n\nWarning: Removed 19206 rows containing non-finite values\n(`stat_density2d_filled()`)."
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#introduction",
    "href": "15-Watershed-Delineation-and-Analysis.html#introduction",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nThe following activity is available as a template github repository at the following link: https://github.com/VT-Hydroinformatics/15-Watershed-Delineation.git\nFor more: https://geocompr.robinlovelace.net/spatial-class.html#raster-data\nTo read in detail about any of the WhiteboxTools used in this activity, check out the user manual: https://jblindsay.github.io/wbt_book/intro.html\nThis activity is adapted from/inspired by: https://matthewrvross.com/active.html and other code from Nate Jones.\nGoals for this activity:\nUse R to delineate the watershed for several pour points on a DEM\nExtract topographic information from those watersheds\nCheck out the list of packages for this exercise. If you have trouble getting rayshader or rgl going, don’t fret too much, these are for the 3d watershed visualization at the end that is basically just something fun to check out.\n\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(sf)\nlibrary(whitebox)\nlibrary(tmap)\nlibrary(stars)\nlibrary(rayshader)\nlibrary(rgl)\n\nwhitebox::wbt_init()\n\nknitr::knit_hooks$set(webgl = hook_webgl)\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#the-watershed-delineation-toolprocess",
    "href": "15-Watershed-Delineation-and-Analysis.html#the-watershed-delineation-toolprocess",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.2 The watershed delineation tool/process",
    "text": "15.2 The watershed delineation tool/process\nWith watershed delineation, it is helpful to take a step back and think about how the whole process works before we dive in. The way things are presented below is pretty linear, but there is no way you would arrive at it by just trying to fire up the watershed delineation function in whitebox. It took some planning to figure out this process! So let’s work through it…\nWe will use the wbt_watershed() function to delineate our watersheds. This function looks upslope from a given point or points (the pour point(s)) and figures out what area drains to that point. To run this function you need a D8 pointer grid and a pour point, or pour points. Sounds simple. But we have to do a few things to generate those two inputs.\nGenerating the D8 pointer grid: this is a grid where each cell specifies what direction water will flow out of that cell. So if our DEM has any pits or depressions without outlets in it, there will be flow dead-ends. This will result in an incorrect watershed delineation. (This was covered in the last chapter) So we have to prepare our DEM to get rid of pits/depressions.\nWe will do this in two stages. First we will fill single cell pits and then breach larger depressions. It is important to remember that for this to work you have to fill the single cell depressions, then pass the resulting DEM from that process to the breach depression function. Once you have done this, you can run the D8 pointer function to generate your pointer grid.\nMaking pour points: The key here is that your pour points MUST…. MUST be on one of the cells in the “stream”, according to your DEM. If it is even one cell over from the high flow accumulation cells that denote the stream on your DEM, you will just get a weird, small sliver of a watershed.\nThe first step is to get the coordinates of your pour point(s). You could grab them from google earth, use a GPS in the field, or maybe you have coordinates for a known location/structure that defines your “outlet”. This could be a flume, weir, gaging station, etc. AHEM EVEN IF your coordinates came from the super fanciest GPS-iest post-processed differential nanometer accuracy wonder GPS, you STILL need to use the process described below to be sure they are on your flow network. Your points might be in the right place, but that doesn’t mean that’s where the stream is according to your DEM!\nYou can read your points in as a csv or just define them in your code if there aren’t many (that’s what we will do) and then turn them into a spatial object using SpatialPoints(), and then export them as a shapefile. If you are reading in a shapefile with your points, you don’t need to convert/export them.\nWith our points in hand, we then need to make sure they are on our stream network. To do that we will use Jensen snap pour points function in wbt. This tool takes your points, searches within a defined area for the stream network, then moves the points to the stream network. This function needs your pour points and a raster stream file. So…. we need to make the raster stream file! We do this using the extract streams wbt function. This function takes a D8 flow accumulation grid, so we need to create that first. Then we can run the stream network function and the snap pour points function.\nA note on units: be sure you know what the distance units are in your data. In our case everything is in decimal degrees, so we need to specify how far the pour point function will search in degrees. If we were using UTM data, that number would be in meters, and if we were in state plane (in VA) that number would be in feet. You can 100% crash R and send wbt into a death spiral if you mix your units up and send the snap pour points function off searching for the biggest stream within 10,000 miles.\nSO! Our process will look like this:\nRead in DEM\nFill single cell sinks then breach breach larger sinks\nCreate D8 flow accumulation and D8 pointer grids\nRead in pour points\nCreate stream raster\nSnap pour points to stream raster\n*Run watershed function\n\nThere will be some extra steps in there just to aid in visualization, but if you were just writing code to perform the analysis, the above is the ticket"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#read-in-dem",
    "href": "15-Watershed-Delineation-and-Analysis.html#read-in-dem",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.3 Read in DEM",
    "text": "15.3 Read in DEM\nThe first several steps are review from the previous activity.\nFirst we will read in the raster, set its CRS (not always needed), set values below 1500 to NA since they are artefacts around the edges, and plot the DEM to be sure everything went ok.\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ndem &lt;- rast(\"McDonaldHollowDEM/brushDEMsm_5m.tif\")\n\ncrs(dem) &lt;- '+init=EPSG:4326'\n\nwriteRaster(dem, \"McDonaldHollowDEM/brushDEMsm_5m_crs.tif\", overwrite = TRUE)\n\ndem[dem &lt; 1500] &lt;- NA\n\ntm_shape(dem)+\n  tm_raster(style = \"cont\", palette = \"PuOr\", legend.show = TRUE)+\n  tm_scale_bar()"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#generate-a-hillshade",
    "href": "15-Watershed-Delineation-and-Analysis.html#generate-a-hillshade",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.4 Generate a hillshade",
    "text": "15.4 Generate a hillshade\nNext, we will generate a hillshade to aid in visualization and then plot it to be sure it turned out ok.\n\nwbt_hillshade(dem = \"McDonaldHollowDEM/brushDEMsm_5m_crs.tif\",\n              output = \"McDonaldHollowDEM/brush_hillshade.tif\",\n              azimuth = 115)\n\nhillshade &lt;- rast(\"McDonaldHollowDEM/brush_hillshade.tif\")\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\n  tm_scale_bar()"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#prepare-dem-for-hydrology-analyses",
    "href": "15-Watershed-Delineation-and-Analysis.html#prepare-dem-for-hydrology-analyses",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.5 Prepare DEM for Hydrology Analyses",
    "text": "15.5 Prepare DEM for Hydrology Analyses\nThis step is review from last time as well, but it is important to point out that it is crucial for this analysis. Basically we are looking upslope for all DEM cells that drain to a specific spot, so if there are any dead-ends, we will not get an accurate watershed.\nIn order to be sure all of our terrain drains downlope, we will breach depressions using the wbt_breach_depressions_least_cost() tool with a distance of 5 and then fill any remaining depressions with wbt_fill_depressions_wang_and_liu()\nThere is a much more in-depth discussion of why we are doing this in the previous chapter.\nFrom now on in the analysis be careful to use the filled and breached DEM.\n\nwbt_breach_depressions_least_cost(\n  dem = \"McDonaldHollowDEM/brushDEMsm_5m_crs.tif\",\n  output = \"McDonaldHollowDEM/bmstationdem_breached.tif\",\n  dist = 5,\n  fill = TRUE)\n\nwbt_fill_depressions_wang_and_liu(\n  dem = \"McDonaldHollowDEM/bmstationdem_breached.tif\",\n  output = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\"\n)"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#create-flow-accumulation-and-pointer-grids",
    "href": "15-Watershed-Delineation-and-Analysis.html#create-flow-accumulation-and-pointer-grids",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.6 Create flow accumulation and pointer grids",
    "text": "15.6 Create flow accumulation and pointer grids\nThe watershed delineation process requires a D8 flow accumulation grid and a D8 pointer file. There were both discussed last chapter. The flow accumulation grid is a raster where each cell is the area that drains to that cell, and the pointer file is a raster where each cell has a value that specifies which direction water would flow downhill away from that cell.\nBelow, create these two rasters using the filled and breached DEM.\n\nwbt_d8_flow_accumulation(input = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n                         output = \"McDonaldHollowDEM/D8FA.tif\")\n\nwbt_d8_pointer(dem = \"McDonaldHollowDEM/bmstationdem_filled_breached.tif\",\n               output = \"McDonaldHollowDEM/D8pointer.tif\")"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#setting-pour-points",
    "href": "15-Watershed-Delineation-and-Analysis.html#setting-pour-points",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.7 Setting pour points",
    "text": "15.7 Setting pour points\nThe last thing we need is our pour points. These are the point locations for which we will delineate our watersheds. It is crucial that these points are on the stream network in each watershed. If the points are even one cell off to the side, you will not get a valid watershed. Instead you will end up with a tiny sliver that shows the area that drains to that one spot on the landscape.\nEven with highly accurate GPS locations, we still need to check to be sure our pour points are on the stream network, because the DEM might not line up perfectly with the points.\nFortunately, there is a wbt function that will make sure our points are on the stream network. wbt_jenson_snap_pour_points() looks over a defined distance from the points you pass it for closest stream and then moves the points to those locations. So to use this function we also need to create a stream network.\nWe will follow the following process to get our pour points set up:\n\nCreate dataframe with pour points\nConvert data frame to shapefile\nWrite the shapefile to our data directory\nMove points with snap pour points function\n\nPerform the first two operations above in this chunk, the pour points are given. I just grabbed them from google earth.\n\nppoints &lt;- tribble(\n          ~Lon, ~Lat,\n          -80.482778, 37.240504,\n          -80.474464, 37.242990,\n          -80.471506, 37.244512\n          )\n\nppointsSP &lt;- st_as_sf(ppoints, coords = c(\"Lon\", \"Lat\"), crs = 4326)\n\nwrite_sf(ppointsSP, \"McDonaldHollowDEM/pourpoints.shp\", overwrite = TRUE)\n\nNow, following the process from last chapter, we will create a raster stream grid using a threshold flow accumulation of 6000 using the D8 flow accumulation grid.\nThen finally, we will use the Jenson snap pour points function to move the pour points to their correct location.\nThe parameter snap_dist tells the function what distance in which to look for a stream. The units of the files we are using are decimal degrees, so we have to be careful here! Use a value of 0.0005, which is about 50 meters. If you were to put 50, it would search over 50 degrees of lat and lon!!! (I did this when making this activity and there was a lot of crashing)\nAfter you get the streams and snapped pour points, read them into your R environment and plot them to be sure the pour points are on the streams.\n\nwbt_extract_streams(flow_accum = \"McDonaldHollowDEM/D8FA.tif\",\n                    output = \"McDonaldHollowDEM/raster_streams.tif\",\n                    threshold = 6000)\n\nwbt_jenson_snap_pour_points(pour_pts = \"McDonaldHollowDEM/pourpoints.shp\",\n                            streams = \"McDonaldHollowDEM/raster_streams.tif\",\n                            output = \"McDonaldHollowDEM/snappedpp.shp\",\n                            snap_dist = 0.0005) #careful with this! Know the units of your data\n\npp &lt;- read_sf(\"McDonaldHollowDEM/snappedpp.shp\")\nstreams &lt;- rast(\"McDonaldHollowDEM/raster_streams.tif\")\n\ntm_shape(streams)+\n  tm_raster(legend.show = TRUE, palette = \"Blues\")+\ntm_shape(pp)+\n  tm_dots(col = \"red\")"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#delineate-watersheds",
    "href": "15-Watershed-Delineation-and-Analysis.html#delineate-watersheds",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.8 Delineate watersheds",
    "text": "15.8 Delineate watersheds\nNow we are all set to delineate our watersheds!\nUse wbt_watershed(), which takes as input a D8 pointer file (d8_pntr) and our snapped pour points (pour_pts). It will output a raster where each watershed is populated with a unique value and all other cells are NA.\nRead the results of this function back in and plot them over the hillshade with alpha set to 0.5 to see what it did.\n\nwbt_watershed(d8_pntr = \"McDonaldHollowDEM/D8pointer.tif\",\n              pour_pts = \"McDonaldHollowDEM/snappedpp.shp\",\n              output = \"McDonaldHollowDEM/brush_watersheds.tif\")\n\nws &lt;- rast(\"McDonaldHollowDEM/brush_watersheds.tif\")\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\ntm_shape(ws)+\n  tm_raster(legend.show = TRUE, alpha = 0.5, style = \"cat\")+\ntm_shape(pp)+\n  tm_dots(col = \"red\")"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#convert-watersheds-to-shapefiles",
    "href": "15-Watershed-Delineation-and-Analysis.html#convert-watersheds-to-shapefiles",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.9 Convert watersheds to shapefiles",
    "text": "15.9 Convert watersheds to shapefiles\nFor mapping or vector analysis it can be very useful to have your watersheds as polygons. To do this we will use the stars package. st_as_stars() converts our watershed raster into an object that the stars package can work with, and then st_as_sf() converts the raster stars object to a vector sf object. We also need to set merge to TRUE, which tells st_as_sf to treat each clump of cells with the same value (our watersheds) as its own feature.\nNow we can plot the vector versions of our watersheds, and also use filter() to just show one at a time, or some combination, rather than all three.\n\nwsshape &lt;- st_as_stars(ws) %&gt;% st_as_sf(merge = T)\n\nws1shp &lt;- wsshape %&gt;% filter(brush_watersheds.tif == \"1\")\n\ntm_shape(hillshade)+\n  tm_raster(style = \"cont\",palette = \"-Greys\", legend.show = FALSE)+\ntm_shape(ws1shp)+\n  tm_borders(col = \"red\")"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#extract-data-based-on-watershed-outline",
    "href": "15-Watershed-Delineation-and-Analysis.html#extract-data-based-on-watershed-outline",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.10 Extract data based on watershed outline",
    "text": "15.10 Extract data based on watershed outline\nNow, just like we looked at the distribution of different landscape data over an entire DEM in the last chapter, we can look at landscape data for each watershed. To do this we will use the extract() function to extract elevation data for just the watershed shapes (vector version). Then we will grab the data for each watershed, since the output here is a list, and plot them in separate geoms in ggplot.\nJust like in last chapter you could do this for any of the topographic measures we calculated, including extracting multiple datasets and comparing them to one another. Cool!\n\nwsElevs &lt;- extract(dem, wsshape)\n\nWS1 &lt;- wsElevs$brushDEMsm_5m[wsElevs$ID == 1] |&gt; as_tibble()\nWS2 &lt;- wsElevs$brushDEMsm_5m[wsElevs$ID == 2] |&gt; as_tibble()\nWS3 &lt;- wsElevs$brushDEMsm_5m[wsElevs$ID == 3] |&gt; as_tibble()\n\nggplot() +\n  geom_density(WS1, mapping = aes(value, color = \"WS1\"))+\n  geom_density(WS2, mapping = aes(value, color = \"WS2\"))+\n  geom_density(WS3, mapping = aes(value, color = \"WS3\"))+\n  xlab(\"Elevation (ft)\")"
  },
  {
    "objectID": "15-Watershed-Delineation-and-Analysis.html#bonus-make-a-3d-map-of-your-watershed-with-rayshader",
    "href": "15-Watershed-Delineation-and-Analysis.html#bonus-make-a-3d-map-of-your-watershed-with-rayshader",
    "title": "15  Geospatial R Raster - Watershed Delineation",
    "section": "15.11 BONUS: Make a 3d map of your watershed with rayshader",
    "text": "15.11 BONUS: Make a 3d map of your watershed with rayshader\nThe following code is here just because it is cool. We clip the DEM to the watershed we want, convert it to a matrix, create a hillshade using rayshader (a visualization tool for 3d stuff), and then plot the output.\n\nws1_bound &lt;- filter(wsshape, brush_watersheds.tif == \"1\")\n\n#crop\nwsmask &lt;- dem %&gt;%\n  crop(., ws1_bound) %&gt;%\n  mask(., ws1_bound)\n\n#convert to matrix\nwsmat &lt;- matrix(\n  data = wsmask,\n  nrow = ncol(wsmask),\n  ncol = nrow(wsmask))\n\n#create hillshade\nraymat &lt;- ray_shade(wsmat, sunable = 115)\n\n#render\nwsmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  plot_3d(wsmat, zscale = 10, fov = 0, theta = 135, zoom = 0.75, phi = 45,\n          windowsize = c(750,750))\n\n#render as html\nrglwidget()"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#introduction",
    "href": "16-Modeling-Getting-Started-HBV.html#introduction",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nThe following activity is available as a template github repository at the following link:\nBefore we start with the code below, we will briefly discuss modeling in general, using the following reading as a guide: https://cfpub.epa.gov/si/si_public_record_report.cfm?dirEntryId=339328&Lab=NERL\nPlease be prepared to discuss the reading at the start of class.\nGoals for this activity:\n\nBecome familiar with the process of running a simple hydrologic model\nTry parameterizing the model manually\nIntroduction to methods of assessing model success\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(patchwork)\nlibrary(plotly)\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#creating-the-hbv-model-function",
    "href": "16-Modeling-Getting-Started-HBV.html#creating-the-hbv-model-function",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.2 Creating the HBV model function",
    "text": "16.2 Creating the HBV model function\nThe code for the HBV model is in the HBV folder, it is called HBV.R. We will open that and look at the code a bit to discuss how HBV works.\nThe code in HBV.R creates a function that runs the HBV model. We can run that code by using the source() function in. When we give the location of the HBV.R file to source(), it runs the code and creates the HBV function. Run the line of code below, then type HBV( and you’ll see the input parameters pop up just like any other function.\nFrom this we see that HBV takes as input pars, P, Temp, PET, and routing.\npars is a vector of all the parameters that HBV uses to run (discussed below) P is a vector of precipitation values Temp is a vector of temperature values PET is a vector of PET values And routing is a parameter that controls how water is routes through the channel of larger watersheds. We will not use this because we are running the model on a small watershed.\nHBV is a lumped conceptual model. We will use the diagram below and the HBV.R code to discuss how the model works and get an idea of what the parameters do.\n This schematic was reproduced from: Durga Lal Shrestha & Dimitri P. Solomatine (2008) Data‐driven approaches for estimating uncertainty in rainfall‐runoff modelling, International Journal of River Basin Management, 6:2, 109-122, DOI: 10.1080/15715124.2008.9635341\n\nsource('HBV/HBV.R')"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#read-in-precip-and-temp",
    "href": "16-Modeling-Getting-Started-HBV.html#read-in-precip-and-temp",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.3 Read in Precip and Temp",
    "text": "16.3 Read in Precip and Temp\nOur objective today is basically to get the model running with data from watershed 3 at the Hubbard Brook experimental forest in NH, USA.\nWatershed 3 is a hydrologic reference watershed at HBEF. This means it hasn’t been experimentally manipulated, but discharge, temperature, and precipitation have been recorded there for a long time. This makes it a good candidate for some modeling! You can see more about watershed 3 here: https://hubbardbrook.org/watersheds/watershed-3\nAs we saw in the chunk above, we need P, Temp, PET, and a set of parameters to run the model.\nWe will talk parameters later, but for now we need the data that drives the model: P, Temp, and PET.\nWe have P and Temp. The data is in Pwd2009-2012.csv and Tdm2009-2012.csv.\nSo how do we get PET?\nWe calculate it! But before we do that let’s bring in the precip and temp data and format them how the model wants them.\nOur model function just wants a vector of values. No dates attached. So we must be careful that each set of input data is for the same amount of time with the same number of values. To help with that, we will start below by defining and start and end date for the model run.\nFinally, we will read in the data, select the gage or station we want to represent the watershed, change the format of the date column, and then filter to our start and end date.\nThen we will pull out just the data, so it can be passed to the model.\n\nstart &lt;- mdy(\"01-01-2009\")\nend &lt;- mdy(\"12-31-2012\")\n\n#Precip\nP1 &lt;- read_csv(\"HBV/Pwd2009-2012.csv\") %&gt;% \n         select(DATE, WS_3) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nRows: 1461 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): DATE, WS_1, WS_2, WS_3, WS_4, WS_5, WS_6, WS_7, WS_8, WS_9\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nP &lt;- P1$WS_3\n\n#Temp in deg C\nTemp1 &lt;- read_csv(\"HBV/Tdm2009-2012.csv\")%&gt;%\n         select(DATE, STA_1) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nRows: 1461 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): DATE, STA_1, STA_6, STA_14, STA_INT, STA_HQ, STA_23, STA_17, STA_24\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTemp &lt;- Temp1$STA_1"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#calculate-pet",
    "href": "16-Modeling-Getting-Started-HBV.html#calculate-pet",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.4 Calculate PET",
    "text": "16.4 Calculate PET\nNext, we need to calculate PET for the same time reange as our temperature and precipitation record.\nWe will use the Hamon method to calculate PET for each day of the record.\nTo do that we will take the latitude of the site in radians…\n\nlat &lt;- 43 + 57/60 #43 degrees and 57 minutes\nlatrad &lt;- (lat/360) * 2 * pi #convert to radians\n\nPET1 &lt;- select(Temp1, DATE) %&gt;%\n         mutate(DOY = yday(DATE)) %&gt;% #DOY for dates\n         mutate(tempvar = (2 * pi / 365) * DOY) %&gt;%\n         #declination of the sun above the celestial equator in \n         #radians on day JulDay of the year\n         mutate(delta_h = 0.4093 * sin(tempvar - 1.405)) %&gt;% \n         #day length in h\n         mutate(daylen = (2 * acos(-tan(delta_h) * tan(latrad)) / 0.2618)) %&gt;% \n         mutate(\n           PET = 29.8 * daylen * 0.611 * exp(17.3 * Temp / \n                  (Temp + 237.3)) / (Temp + 273.2))  #PET Hamon method\n\nPET &lt;- PET1$PET"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#hbv-parameters",
    "href": "16-Modeling-Getting-Started-HBV.html#hbv-parameters",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.5 HBV Parameters",
    "text": "16.5 HBV Parameters\nOkay, so now we have our data set up. Next we need to talk parameters.\nThe table below shows the parameters of the HBV model, the minimum and maximum for those values, and a description of their function in the model.\n\n\n\n\n\n\n\n\n\nObject Name\nMin\nMax\nDescription\n\n\n\n\nFC\n40\n400\nMax soil moisture storage, field capacity\n\n\nbeta\n1\n6\nShape coefficient governing fate of water input to soil moisture storage\n\n\nLP\n.3\n1\nThreshold for reduction of evaporation\n\n\nSFCF\n0.4\n1.2\nSnowfall correction factor\n\n\nTT\n-1.5\n1.2\nThreshold temperature\n\n\nCFMAX\n1\n8\nDegree-day factor\n\n\nk0\n0.05\n0.5\nRecession constant (upper storage, near surface)\n\n\nk1\n0.01\n0.3\nRecession constant (upper storage)\n\n\nk2\n0.001\n0.15\nRecession constant (lower storage)\n\n\nUZL\n0\n70\nThreshold for shallow storage\n\n\nPERC\n0\n4\nPercolation, max flow from upper to lower storage\n\n\nMAXBAS\n1\n3\nbase of the triangular routing function, days”\n\n\n\nTo pass a set of parameters to the model, we just put them into a single vector in the order they are in the table above. In the chunk below, I’ve structured this to make it easy to see a description of each parameter, but you could also just do it in one line without all the comments. Just be sure to get the order right!\nThe code below sets the routing to 0, which is what we want for a small catchment.\nThen I just set each parameter to it’s minimum value. We will use this parameter set to run the model and see what happens!\n\n#when this term = 1, then triangular routing is invoked, or for no routing, routing = 0\n#if routing = 0 then MAXBAS doesn't do anything\nrouting &lt;- 0      \n\n#hard code parameters \nparams &lt;- c(40,    #FCM ax soil moisture storage, field capacity\n            1,     #beta Shape coefficient governing fate of water input to soil moisture storage\n            0.3,   #LP Threshold for reduction of evap\n            0.4,   #SFCF Snowfall correction factor\n            -1.5,  #TT Threshold temperature\n            1,     #CFMAX Degree-day factor\n            0.05,  #k0 Recession constant (upper storage, near surface)\n            0.01,  #k1 Recession constant (upper storage)\n            0.001, #k2 Recession constant (lower storage)\n            0,     #UZLThreshold for shallow storage\n            0,     #PERC Percolation, max flow from upper to lower storage\n            1      #MAXBAS base of the triangular routing function, days\n            )"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#first-model-run",
    "href": "16-Modeling-Getting-Started-HBV.html#first-model-run",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.6 First model run",
    "text": "16.6 First model run\nLet’s run the model! Remember the function is HBV(parameters, Precip, Temp, PET, routing)\nWe will set the output of the model equal to ModelOutput and then have a look at what it outputs.\nHow can we tell how this did at modeling flow in watershed 3?\n\nModelOutput &lt;- HBV(params, P, Temp, PET, routing)\n\nhead(ModelOutput)\n\n# A tibble: 6 × 12\n      q    qs    qi    qb Storage   SWE   AET    SF    S1    S2  soil     w\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0     0     0       0    0      0  0        0     0     0     0\n2     0     0     0     0      40  152.     0  0        0     0    40     0\n3     0     0     0     0      40  152.     0  0        0     0    40     0\n4     0     0     0     0      40  152.     0  0        0     0    40     0\n5     0     0     0     0      40  154.     0  1.16     0     0    40     0\n6     0     0     0     0      40  154.     0  0        0     0    40     0"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#import-observed-streamflow-data",
    "href": "16-Modeling-Getting-Started-HBV.html#import-observed-streamflow-data",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.7 Import observed streamflow data",
    "text": "16.7 Import observed streamflow data\nq in the model output is discharge, and we also have a record of measured discharge from watershed 3 in the file called SWD2009-2012.csv\nIn order to compare the modeled discharge to the measured discharge, we will read in the data and process it the same way we did with the model inputs. Then we can attach it to the model output so we can compare the two.\nNOTE: These data are all nicely processed. They have the same temporal resolution (daily), there are no NAs, and they are in the same units. These are all things to check if you are preparing data from another site.\n\n#Streamflow mm/d\nQobs1 &lt;- read_csv(\"HBV/SWD2009-2012.csv\") %&gt;%\n         select(DATE, WS_3) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nRows: 1461 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): DATE, WS_1, WS_2, WS_3, WS_4, WS_5, WS_6, WS_7, WS_8, WS_9\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nQobs &lt;- Qobs1$WS_3\n\nModelOutput &lt;- bind_cols(ModelOutput, Qobs1)"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#compare-observed-and-modeled-discharge-graphically",
    "href": "16-Modeling-Getting-Started-HBV.html#compare-observed-and-modeled-discharge-graphically",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.8 Compare observed and modeled discharge graphically",
    "text": "16.8 Compare observed and modeled discharge graphically\nNow that we have the observed values glued on to our model output, we can plot the two and see how they compare. Below we will plot the modeled and observed discharge as two lines on a ggplot.\nBecause the model starts out with its storages at zero, it takes some time for it to actually approximate real conditions. For this reason, you will typically set aside part of the modelled period as a “warm up” period. The duration of this period depends on the system, but for our purposes, we are doing to drop the first half of the model run and only look at the second half: 2011 - 2013. So we will filter the data to just look at that time.\nHow’d we do? Did the model do a good job of capturing runoff dynamics in watershed 3?\nDescribe what it looks like the model did well and what it did poorly?\nWhen you are looking at results like this, it is often helpful to look at the full timeframe but to also zoom in and look at specific event dynamics to see what hydrologic processes the model is or is not capturing well. One helpful tool at doing that is plotly. Plotly allows you to create interactive plots. The code chunk after the ggplot creates a plotly graph. From here on out we will use plotly to look at our modeling results so we can investigate them more thoroughly.\n\nOutputTrim &lt;- filter(ModelOutput, DATE &gt;= mdy(\"01-01-2011\"))\n\nggplot(OutputTrim, aes(x = DATE, y = WS_3, color = \"Measured\"))+\n  geom_line()+\n  geom_line(aes(y = q, color = \"Modelled\"))+\n  ylab(\"Discharge (mm/d)\")"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#compare-observed-and-modelled-discharge-with-interactive-graph",
    "href": "16-Modeling-Getting-Started-HBV.html#compare-observed-and-modelled-discharge-with-interactive-graph",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.9 Compare observed and modelled discharge with interactive graph",
    "text": "16.9 Compare observed and modelled discharge with interactive graph\nThe code below creates a plot analogous to the ggplot above. Note the differences in syntax. All the same information is there, it is just a different recipe, and you use pipes instead of +.\nYou can click and drag a box over any time you want to look at to zoom, and if you hover over one of the lines, it will show you the values in a dialogue box. There is a lot of other functionality as well, which you can explore in the menu in the upper right.\nYou can create a similar graph by saving the one above and passing it to ggplotly()!\n\nOutputTrim %&gt;% plot_ly(x = ~DATE) %&gt;% \n        add_trace(y = ~q, name = 'Modeled',  type = 'scatter', mode = 'lines') %&gt;% \n        add_trace(y = ~WS_3, name = 'Measured',  type = 'scatter', mode = 'lines')"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#measure-how-well-the-model-fits-with-nse",
    "href": "16-Modeling-Getting-Started-HBV.html#measure-how-well-the-model-fits-with-nse",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.10 Measure how well the model fits with NSE",
    "text": "16.10 Measure how well the model fits with NSE\nBefore we can start trying to tune our model to look more like the observed discharge record, it would be helpful to have some sort of quantified metric for how well our modeled data fits the measured data.\nThere are many different ways to do this, but discussion of the pros and cons of those approaches is beyond this quick introduction to modeling. So we are just going to look at one method, the Nash-Sutcliffe Efficiency (NSE).\nBasically, the NSE looks at how much better your model run did that if you had just used the mean discharge for the data record as your “modeled results”. It does this by comparing how far off the observed values were from the mean discharge to how far off the modeled values were from the observed discharge.\nMathematically NSE is the sum of the squared differences between the modeled and observed discharge divided by the sum of the squared differences between the observed and mean discharge, subtracted by 1.\n\\[\nNSE = 1 - \\frac{\\sum_{t = 1}^{T}{(Q_m^t - Q_o^t)^2}}{\\sum_{t = 1}^{T}{(Q_o^t - \\bar{Q_o})^2}}\n\\] Where \\(Q_m^t\\) is modeled discharge at time t, \\(Q_o^t\\) is observed discharge at time t, and \\(\\bar{Q_o}\\) is mean observed discharge.\nBelow, we calculate NSE for the model run above. We will continue to exclude the warm up period.\nAn NSE over 0 means the model did better than the mean discharge at predicting discharge. An NSE of 1 would be a perfect model fit. How’d we do? Does this make sense with the timeseries we looked at above?\n\n#Calculate NSE\nNSE &lt;- 1 - ((sum((OutputTrim$q - OutputTrim$WS_3) ^ 2)) / \n                 sum((OutputTrim$WS_3 - mean(OutputTrim$WS_3)) ^ 2))\n\nNSE\n\n[1] 0.2135378"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#assess-model-fit-with-a-different-measure-snow",
    "href": "16-Modeling-Getting-Started-HBV.html#assess-model-fit-with-a-different-measure-snow",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.11 Assess model fit with a different measure: Snow",
    "text": "16.11 Assess model fit with a different measure: Snow\nBefore we jump into trying to parameterize this thing, I wanted to note that there are other things you can calibrate your model to other than discharge. In the case of watershed 3, for example, we also have a snow record. Snowmelt is a very important input driving our model so we might want to make sure that the model is doing a good job of capturing that as well!\nLet’s pull in snow, calculate NSE between the modeled and measure snow, and look at a plot of the data.\nSnow is not measured daily, so we will plot the measured values as points. Likewise, we will have filter our data to only times when we have a modeled snow amount AND a recorded snow amount when we calculate NSE.\n\n#Read and prep snow data\nsnow &lt;- read_csv(\"HBV/sno2009-2012.csv\") %&gt;%\n         select(DATE, STA2) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nNew names:\nRows: 76 Columns: 25\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): DATE dbl (21): STA1, STA2, STA3, STA4, STA5, STA6, STA7, STA8, STA9,\nSTA10, STA11... lgl (3): ...23, ...24, ...25\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `DATE = ymd(DATE)`.\nCaused by warning:\n!  8 failed to parse.\n\n#Join measured snow to model output\nOutputTrimSno &lt;- left_join(OutputTrim, snow, by = \"DATE\")\n\n#Plot modeled and measured snow\nOutputTrimSno %&gt;% plot_ly(x = ~DATE) %&gt;% \n        add_trace(y = ~SWE, name = 'Modeled', type = 'scatter', mode = 'lines') %&gt;% \n        add_trace(y = ~STA2, name = 'Measured', type = 'scatter', mode = 'markers')\n\nWarning: Ignoring 698 observations\n\n\n\n\n\n#Drop rows where there isn't a measured snow amount\nCompareSnow &lt;- drop_na(OutputTrimSno, STA2)\n\n#Calculate NSE for snow, SWE is modeled, STA2 is measured\nNSEsno &lt;- 1 - ((sum((CompareSnow$SWE - CompareSnow$STA2) ^ 2)) / \n                 sum((CompareSnow$STA2 - mean(CompareSnow$STA2)) ^ 2))\n\nNSEsno\n\n[1] 0.1339054"
  },
  {
    "objectID": "16-Modeling-Getting-Started-HBV.html#calibrate-hbv-manually",
    "href": "16-Modeling-Getting-Started-HBV.html#calibrate-hbv-manually",
    "title": "16  Intro to Modeling - Getting Started with HBV",
    "section": "16.12 Calibrate HBV manually",
    "text": "16.12 Calibrate HBV manually\nWoohoo! We can now run our model and assess how well it is working!\nNow, let’s see how well we can get it to work. The code below runs the model, produces a plot, and calculates the NSE based on discharge.\nBy changing the parameters, see how well you can get the model to fit. Take note of how the discharge changes when you change certain parameters. Let’s see who can get the highest NSE!\n\n#when this term = 1, then triangular routing is invoked, or for no routing, routing = 0\n#if routing = 0 then MAXBAS doesn't do anything\nrouting &lt;- 0      \n\n#hard code parameters \nparams &lt;- c(40,     #FCM ax soil moisture storage, field capacity\n            1,      #beta Shape coefficient governing fate of water input to soil moisture storage\n            0.3,    #LP Threshold for reduction of evap\n            0.4,    #SFCF Snowfall correction factor\n            -1.5,   #TT Threshold temperature\n            1,      #CFMAX Degree-day factor\n            0.05,   #k0 Recession constant (upper storage, near surface)\n            0.01,   #k1 Recession constant (upper storage)\n            0.001,  #k2 Recession constant (lower storage)\n            0,      #UZL Threshold for shallow storage\n            0,      #PERC Percolation, max flow from upper to lower storage\n            1       #MAXBAS base of the triangular routing function, days\n            )\n\n#Run the model\nOut &lt;- HBV(params, P, Temp, PET, routing)\n\n#Add observed output\nOut &lt;- bind_cols(Out, Qobs1)\n\n#Trim out the warm up period\nOutTrim &lt;- filter(Out, DATE &gt;= mdy(\"01-01-2011\"))\n\n#Calculate NSE\nNSE &lt;- 1 - ((sum((OutTrim$q - OutTrim$WS_3) ^ 2)) / \n                 sum((OutTrim$WS_3 - mean(OutTrim$WS_3)) ^ 2))\n\n#Create plot with NSE in title\nOutTrim %&gt;% plot_ly(x = ~DATE) %&gt;% \n        add_trace(y = ~q, name = 'Modeled',  type = 'scatter', mode = 'lines') %&gt;% \n        add_trace(y = ~WS_3, name = 'Measured', type = 'scatter', mode = 'lines') %&gt;% \n        layout(title=paste(\"NSE: \", round(NSE,2)))"
  },
  {
    "objectID": "17-Calibrate-HBV.html#introduction",
    "href": "17-Calibrate-HBV.html#introduction",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nThe following activity is available as a template github repository at the following link: https://github.com/VT-Hydroinformatics/17-calibrate-model-for-loops\nGoals for this activity:\n\nLearn how for loops work\nUse for loops to calibrate the HBV model using the Monte Carlo technique\nAssess the results of the calibration, discuss equifinality\n\nWe saw in the last activity that coming up with the parameters that create a model that fits our watershed discharge best is hard. For this reason, there are several ways people have devised for automating the process. We are going to explore one: a Monte Carlo calibration.\nTo use this approach, we will create a ton of random parameter sets, run the model for each, and then see which one fits best. The idea is that by creating a bunch of random sets of parameters, we will end up with at least one that works well. As you might guess, this can take a while. We will just do this for 100 or 1000 runs here in this activity, but we will also explore some output from a 100,000 run Monte Carlo that I ran and saved.\nThere is one new package this time, GGally, just for creating a parallel coordinate plot to look at the results of the 100k run Monte Carlo at the end.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(plotly)\nlibrary(GGally) #for making parallel coordinate plot\n\ntheme_set(theme_classic())\n\nIn order to do what we are going to do, we need “for” loops.\nA for loop is a way to run a chunk of code several times while changing a value in that chunk of code each time.\nSo using an example from class: you could run some code that downloads data from a USGS gage, and each time through the loop it changes which gage you download.\nIn R, the syntax for this is\n\nfor(x in sequence){\nx\n}\n\n\nWhere x will change every time you go through the loop. The values x will have each time through the loop are those that are in “sequence” above. If you changed this to x in 1:3, x would have values of 1, 2, and 3 each time through. If you made it x in c(2,3,5) x would change to 2, then 3, and then 5 each time through the loop. You can make the sequence anything and x can have any name so “letter in c(a, b, c)” would have letter change to a, then b, then c each time through the loop.\nThe image below shows a short example of how a for loop works.\n Let’s try some examples\n\nval &lt;- 0\n\nfor(x in 1:10){\n  val[x] &lt;- x\n}\n\nval\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ndays &lt;- c(\"mon\",\"tues\",\"wednes\")\n\nfor(x in 1:length(days)){\n  days[x] &lt;- paste0(days[x], \"day\")\n}\n\ndays\n\n[1] \"monday\"    \"tuesday\"   \"wednesday\"\n\ngages &lt;- c(\"123\",\"ABC\",\"001\")\n\nfor(g in gages){\n  print(g)\n}\n\n[1] \"123\"\n[1] \"ABC\"\n[1] \"001\""
  },
  {
    "objectID": "17-Calibrate-HBV.html#challenge-write-a-for-loop",
    "href": "17-Calibrate-HBV.html#challenge-write-a-for-loop",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.2 Challenge: Write a for loop",
    "text": "17.2 Challenge: Write a for loop\nDefine a vector with values 5, 6, 7, 8, 9\nCreate a for loop that steps through this vector and adds 1 to each value.\nYes, you can do this by just typing the name of the vector + 1, but that’s not the point!"
  },
  {
    "objectID": "17-Calibrate-HBV.html#prep-data-for-hbv",
    "href": "17-Calibrate-HBV.html#prep-data-for-hbv",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.3 Prep data for HBV",
    "text": "17.3 Prep data for HBV\nSince we did this in the last chapter, we are just going to quickly run the code the preps our data to go into the model. Below we load the HBV model and make vectors of P, Qobs, and Temp.\n\n#Create the HBV function with the HBV R file\nsource('HBV/HBV.R')\n\n#set start and end dates for model to filter data\nstart &lt;- mdy(\"01-01-2009\")\nend &lt;- mdy(\"12-31-2012\")\n\n#Precip\nP1 &lt;- read_csv(\"HBV/Pwd2009-2012.csv\") %&gt;% \n         select(DATE, WS_3) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nRows: 1461 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): DATE, WS_1, WS_2, WS_3, WS_4, WS_5, WS_6, WS_7, WS_8, WS_9\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nP &lt;- P1$WS_3\n\n#Streamflow mm/d\nQobs1 &lt;- read_csv(\"HBV/SWD2009-2012.csv\") %&gt;%\n         select(DATE, WS_3) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nRows: 1461 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): DATE, WS_1, WS_2, WS_3, WS_4, WS_5, WS_6, WS_7, WS_8, WS_9\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nQobs &lt;- Qobs1$WS_3\n\n#Temp in deg C\nTemp1 &lt;- read_csv(\"HBV/Tdm2009-2012.csv\")%&gt;%\n         select(DATE, STA_1) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nRows: 1461 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): DATE, STA_1, STA_6, STA_14, STA_INT, STA_HQ, STA_23, STA_17, STA_24\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTemp &lt;- Temp1$STA_1\n\n#Snow in SWE, mm\nsnow &lt;- read_csv(\"HBV/sno2009-2012.csv\") %&gt;%\n         select(DATE, STA2) %&gt;%\n         mutate(DATE = ymd(DATE)) %&gt;%\n         filter(DATE &gt;= start & DATE &lt;= end)\n\nNew names:\nRows: 76 Columns: 25\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): DATE dbl (21): STA1, STA2, STA3, STA4, STA5, STA6, STA7, STA8, STA9,\nSTA10, STA11... lgl (3): ...23, ...24, ...25\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `DATE = ymd(DATE)`.\nCaused by warning:\n!  8 failed to parse."
  },
  {
    "objectID": "17-Calibrate-HBV.html#calculate-pet",
    "href": "17-Calibrate-HBV.html#calculate-pet",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.4 Calculate PET",
    "text": "17.4 Calculate PET\nAgain, because we did this last time, we will just re-run this code to create our PET values.\n\nlat &lt;- 43 + 57/60 #43 degrees and 57 minutes\nlatrad &lt;- (lat/360) * 2 * pi #convert to radians\n\nPET1 &lt;- select(Temp1, DATE) %&gt;%\n         mutate(DOY = yday(DATE)) %&gt;% #DOY for dates\n         mutate(tempvar = (2 * pi / 365) * DOY) %&gt;%\n         #declination of the sun above the celestial equator in \n         #radians on day JulDay of the year\n         mutate(delta_h = 0.4093 * sin(tempvar - 1.405)) %&gt;% \n         #day length in h\n         mutate(daylen = (2 * acos(-tan(delta_h) * tan(latrad)) / 0.2618)) %&gt;% \n         mutate(\n           PET = 29.8 * daylen * 0.611 * exp(17.3 * Temp / \n                  (Temp + 237.3)) / (Temp + 273.2))  #PET Hamon method\n\nPET &lt;- PET1$PET"
  },
  {
    "objectID": "17-Calibrate-HBV.html#monte-carlo-step-1-generate-random-parameter-sets",
    "href": "17-Calibrate-HBV.html#monte-carlo-step-1-generate-random-parameter-sets",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.5 Monte Carlo step 1: generate random parameter sets",
    "text": "17.5 Monte Carlo step 1: generate random parameter sets\nBelow is a table of the parameters for the HBV model and their suggested min and max values for our application. What we basically want to do is mimic you sitting down at the computer and randomly changing all the parameters, running the model, and checking to see how well it did. Over, and over, and over.\nYou can do this by generating the set of parameters each time you run the model, but we are going to do it separately so we can see the Nashe-Sutcliff efficiency (NSE) for each set of parameters.\n\n\n\n\n\n\n\n\n\nObject Name\nMin\nMax\nDescription\n\n\n\n\nFC\n40\n400\nMax soil moisture storage, field capacity\n\n\nbeta\n1\n6\nShape coefficient governing fate of water input to soil moisture storage\n\n\nLP\n.3\n1\nThreshold for reduction of evaporation\n\n\nSFCF\n0.4\n1.2\nSnowfall correction factor\n\n\nTT\n-1.5\n1.2\nThreshold temperature\n\n\nCFMAX\n1\n8\nDegree-day factor\n\n\nk0\n0.05\n0.5\nRecession constant (upper storage, near surface)\n\n\nk1\n0.01\n0.3\nRecession constant (upper storage)\n\n\nk2\n0.001\n0.15\nRecession constant (lower storage)\n\n\nUZL\n0\n70\nThreshold for shallow storage\n\n\nPERC\n0\n4\nPercolation, max flow from upper to lower storage\n\n\nMAXBAS\n1\n3\nbase of the triangular routing function, days”\n\n\n\nTo make our parameter sets, we first have to decide how many model runs we want to try. Let’s try 100 so we don’t burn down anyone’s computer.\nThen we will use the runif() function to generate the same number of random values as the number of model runs we want to perform for each parameter, within the bounds of the min and max values above.\nRemember, runif() creates a set of random numbers in a uniform distribution with a specified length with a specified minimum and maximum value. A uniform distribution is one where each number has the same probability of occurrence as the next, so it’s basically a set of random numbers between your min and max values.\nWe will also create a blank vector of the same length which we will populate with the NSE values for each model run, and then slap them all together with cbind() (column bind).\n\n#number of runs\nN &lt;- 100\n\n# PARAMETERS RANGE and generate set\nFC    &lt;- runif(N, min = 40   , max = 400)  #Max soil moisture storage, field capacity\nbeta  &lt;- runif(N, min = 1    , max = 6)    #Shape coefficient governing fate of water input to soil moisture storage\nLP    &lt;- runif(N, min = 0.3   , max = 1)    #Threshold for reduction of evap\nSFCF  &lt;- runif(N, min = 0.4  , max = 1.2)  #Snowfall correction factor\nTT    &lt;- runif(N, min = -1.5 , max = 1.2)  #Threshold temperature\nCFMAX &lt;- runif(N, min = 1    , max = 8)    #Degree-day factor\nk0    &lt;- runif(N, min = 0.05 , max = 0.5)  #Recession constant (upper storage, near surface)\nk1    &lt;- runif(N, min = 0.01 , max = 0.3)  #Recession constant (upper storage)\nk2    &lt;- runif(N, min = 0.001, max = 0.15) #Recession constant (lower storage)\nUZL   &lt;- runif(N, min = 0    , max = 70)   #Threshold for shallow storage\nPERC  &lt;- runif(N, min = 0    , max = 4)    #Percolation, max flow from upper to lower storage\nMAXBAS&lt;- rep(1, N)   #base of the triangular routing function, days\n#MAXBAS is just 1's because routing will be set to zero, so the parameter isn't used\n\nNSE &lt;- rep(NA, N) #create NSE column, to be filled in for loop\n\nrouting &lt;- 0\n\npars &lt;- cbind(FC, beta, LP, SFCF, \n               TT, CFMAX, k0, k1, \n               k2, UZL, PERC, MAXBAS, NSE)"
  },
  {
    "objectID": "17-Calibrate-HBV.html#run-the-model-for-each-parameter-set",
    "href": "17-Calibrate-HBV.html#run-the-model-for-each-parameter-set",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.6 Run the model for each parameter set",
    "text": "17.6 Run the model for each parameter set\nAlright, let’s get at this. We now have our random sets of parameters. So we will run the model for each and see how it does.\nHere’s how we will do that:\nFirst: Define the period for which we want to calculate NSE. Remember we are going to drop the first 40% of the model run to let the model “warm up”. Since we are starting with nothing in the storages we need to let the boxes “fill up” and settle in.\nThen we run a FOR LOOP!\nOur for loop will step through a sequence of numbers from 1 to the total number of parameter sets we have. So in this case: 100. We will use i as our variable. So i will be 1, 2, 3…. all the way to 100.\nIn the code portion of the loop, it will grab the parameters from row i of the parameter set we created. And pass that to the HBV function with the Precip, Temp, and PET data. Remember routing is just set to zero.\nAfter we run the model (still in the for loop) we will stick our observed values onto the results and then trip them to the period we want to evaluate. (drop the first 40%)\nFinally, we calculate NSE for that individual model run, and then add it into the NSE column we made before. So each run will just tack on an NSE value to the end of the parameter set. We don’t need to save the model results, because we can just rerun the model with the same parameters and it’ll give us the same output.\nThe figure below illustrates how the for loop will work:\n\n\n\nmonte carlo for loop\n\n\n\n#trim the first 40% (warm up) of Qobs off for NSE calculation \nEvalStart &lt;- floor(length(Qobs) * 0.4)\nEvalEnd &lt;- length(Qobs)\n\n\nfor (i in 1:N){\n   \n   #call model with i parameter set generated above\n   results &lt;- HBV(pars[i,1:12], P, Temp, PET, routing)\n   \n   #add the Qobs to results\n   results &lt;- cbind(results, Qobs)\n   \n   #trim the first 40% of the record so it isn't included in the NSE calculation\n   results &lt;- results[EvalStart:EvalEnd,]\n    \n   #Calculate NSE and add to parameter set\n   pars[i,13]  &lt;- 1 - ((sum((results$Qobs - results$q) ^ 2)) / sum((results$Qobs - mean(results$Qobs)) ^ 2))\n   \n}"
  },
  {
    "objectID": "17-Calibrate-HBV.html#find-the-best-parameter-set",
    "href": "17-Calibrate-HBV.html#find-the-best-parameter-set",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.7 Find the best parameter set",
    "text": "17.7 Find the best parameter set\nAlright, now we have a NSE value that tells us how well each parameter set did at estimating runoff. So we need to find the best one!\nTo do this, we will convert the parameter sets to a tibble, use filter to grab the row with the maximum NSE, then use slice to pull just that row out of the tibble. Finally, we will run as.numeric on the data to be sure everything is numbers.\nNow we have our best parameter set, let’s see how it did!\nWe will do what we did in the previous lesson: run the model with the parameter set, slap on the observed data, and filter it to the time period we want to look at. (after 08-07-2010 in this case, that’s the 40% cutoff in this case)\n\n#find best parameters\npars &lt;- as_tibble(pars)\n\nbestparams &lt;- pars %&gt;% filter(NSE == max(NSE)) %&gt;%\n                        slice(1) %&gt;%\n                        as.numeric()\n\n#run with best parameters\nmodeloutput &lt;-  HBV(bestparams, P, Temp, PET, routing)\n\n#add observations for plotting\nmodeloutput &lt;- bind_cols(modeloutput, Qobs1)\n\n#trim out warm up period for plotting\nOutputTrim &lt;- filter(modeloutput, DATE &gt;= mdy(\"08-07-2010\"))\n\nBelow we will look at the model run using plotly. First, we calculate the NSE again, just to be sure, and then plot the same way we did in the last lesson.\nHow did the model do? Did the 100 run Monte Carlo do better or worse than your manual attempts from last lesson?\n\n#Calculate NSE\nNSE &lt;- 1 - ((sum((OutputTrim$q - OutputTrim$WS_3) ^ 2)) / \n                 sum((OutputTrim$WS_3 - mean(OutputTrim$WS_3)) ^ 2))\n\n#Create plot with NSE in title\nOutputTrim %&gt;% plot_ly(x = ~DATE) %&gt;% \n        add_trace(y = ~q, name = 'Modeled',  type = 'scatter', mode = 'lines') %&gt;% \n        add_trace(y = ~WS_3, name = 'Measured', type = 'scatter', mode = 'lines') %&gt;% \n        layout(title=paste(\"NSE: \", round(NSE,2)))"
  },
  {
    "objectID": "17-Calibrate-HBV.html#investigating-a-much-bigger-monte-carlo",
    "href": "17-Calibrate-HBV.html#investigating-a-much-bigger-monte-carlo",
    "title": "17  Intro to Modeling - Calibrate HBV",
    "section": "17.8 Investigating a much bigger Monte Carlo",
    "text": "17.8 Investigating a much bigger Monte Carlo\nThe next couple of code chunks make plots from a Monte Carlo I ran with 100,000 parameter sets.\nWe will look at the two plots and discuss.\nHow much difference was there in NSE in the top 100 runs of the 100,000 run set? How much would you expect the parameters to vary between these runs? What do you think it means if they did or did not vary much?\n\n#the [,-1] drops the first column, which is just row numbers\npars100 &lt;- read_csv(\"HBV/parsFrom100kRun.csv\")[,-1]\n\nNew names:\nRows: 100 Columns: 15\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(15): ...1, FC, beta, LP, SFCF, TT, CFMAX, k0, k1, k2, UZL, PERC, MAXBAS...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nggplot(pars100, aes(NSE))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe chunk below creates a parallel coordinate plot. Each parameter is shown, but normalized, where 1 is it’s maximum value from our random sets, and 0 is the minimum value.\nEach connected line is a set of parameters. The green line is the set that had the highest NSE.\nLook at this plot for a bit and discuss.\nAre there any parameters that were always in the same range for these runs? What does that tell you about them? Are there any parameters that look like they spanned almost the whole range for these runs? What does that tell you about them? What does this plot tell you about the model’s representation of the real world?\n\n#filter the 100,000 run to the top ten\nrankedpars &lt;- filter(pars100, ranks &lt;= 10)\n\n#set the min and max values for each parameter for the plot\nmins &lt;- c(40, 1, 0.3 , 0.4, -1.5, 1, 0.05, 0.01, 0.001, 0, 0, 1, 0, 0)\nmaxs &lt;- c(400, 6, 1, 1.2, 1.2, 8, 0.5, 0.3, 0.15, 70 , 4, 3, 0, 0)\n\n#add maxes and mins to the parameter sets\nrankedpars &lt;- rbind(rankedpars, mins, maxs)\n\n#Make NSE a character so the legend will work properly\nrankedpars &lt;- rankedpars %&gt;% mutate(NSE = as.character(round(NSE, 2)))\n\n#Make the NSE equal \"Min/Max\" for appropriate legend labels\nrankedpars$NSE[rankedpars$NSE == \"0\"] &lt;- \"Min/Max\"\n\n#Make the ranks a factor datatype\nrankedpars &lt;- rankedpars %&gt;% mutate(ranks = factor(ranks))\n\n#Create parallel coordinate plot\nrankedpars %&gt;% ggparcoord(columns = 1:11, \n              groupColumn = 13,\n              showPoints = TRUE,\n              scale = \"uniminmax\") +\n              theme_minimal() +\n              ggtitle(\"Top 10 parameter sets from 100,000 run Monte Carlo\")+\n              ylab(\"normalized parameter values\")+\n              xlab(\"parameters\")"
  }
]